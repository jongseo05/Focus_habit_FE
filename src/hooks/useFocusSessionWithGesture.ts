import { useCallback, useEffect, useRef, useState } from 'react'
import { useWebSocket } from './useWebSocket'
import { useMediaStream } from './useMediaStream'
import { FrameStreamer } from '@/lib/websocket/utils'
import { useDashboardStore } from '@/stores/dashboardStore'
import type { WebcamFrameAnalysisResult, FocusAnalysisFeatures } from '@/types/websocket'
import { useFocusSessionErrorHandler } from '@/hooks/useFocusSessionErrorHandler'
import { FocusSessionErrorType, FocusSessionStatus } from '@/types/focusSession'
import { determineFocusStatus, type FocusStatus } from '@/lib/focusScoreEngine'
import { supabaseBrowser } from '@/lib/supabase/client'
import type { GestureFeatures } from '@/types/focusSession'

interface FocusSessionWithGestureOptions {
  frameRate?: number
  enableGestureRecognition?: boolean
  gestureJpegQuality?: number
}

interface FocusSessionWithGestureReturn {
  // MediaStream ê¸°ëŠ¥ ì „ì²´ë¥¼ í¬í•¨
  stream: MediaStream | null
  isLoading: boolean
  error: string | null
  isPermissionGranted: boolean
  isPermissionDenied: boolean
  requestPermission: () => Promise<boolean>
  startStream: () => Promise<boolean>
  stopStream: () => void
  resetError: () => void
  retryPermission: () => Promise<boolean>
  
  // ì œìŠ¤ì²˜ ì¸ì‹ ì¶”ê°€ ê¸°ëŠ¥
  isGestureRecognitionActive: boolean
  gestureWebSocketConnected: boolean
  gestureFramesSent: number
  
  // ì§‘ì¤‘ ì„¸ì…˜ ì—ëŸ¬ ì²˜ë¦¬
  sessionStatus: FocusSessionStatus
  sessionErrors: any[]
  lastSessionError: any | null
  canRecoverFromError: boolean
  retrySessionRecovery: () => Promise<boolean>
}

export function useFocusSessionWithGesture(
  isRunning: boolean,
  sessionId: string | null | undefined,
  options: FocusSessionWithGestureOptions = {}
) {
  
  const {
    frameRate = 10,
    enableGestureRecognition = true,
    gestureJpegQuality = 0.8
  } = options
  
  // ê¸°ì¡´ ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ í›… ì‚¬ìš©
  const mediaStream = useMediaStream()
  
  // ì œìŠ¤ì²˜ ì¸ì‹ ìƒíƒœ
  const [gestureFramesSent, setGestureFramesSent] = useState(0)
  const [isGestureActive, setIsGestureActive] = useState(false)

  // 1. ìƒíƒœ ì¶”ê°€
  const [isSpeechRecognitionActive, setIsSpeechRecognitionActive] = useState(false)

  // ì§‘ì¤‘ ì„¸ì…˜ í†µí•© ì—ëŸ¬ í•¸ë“¤ëŸ¬
  const { 
    state: errorHandlerState, 
    handleError, 
    classifyError, 
    retryRecovery,
    clearErrors 
  } = useFocusSessionErrorHandler({
    config: {
      autoRecovery: true,
      maxRecoveryAttempts: 3,
      gracefulDegradation: true,
      fallbackMode: true
    },
    onError: (error) => {
      
      // íŠ¹ì • ì—ëŸ¬ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
      switch (error.type) {
        case FocusSessionErrorType.CAMERA_DISCONNECTED:
          setIsGestureActive(false)
          break
        case FocusSessionErrorType.WEBSOCKET_FAILED:
        case FocusSessionErrorType.GESTURE_SERVER_ERROR:
          setIsGestureActive(false)
          break
      }
    },
    onRecoveryStart: (errorType) => {
    },
    onRecoverySuccess: (errorType) => {
      // ë³µêµ¬ ì„±ê³µ ì‹œ ì œìŠ¤ì²˜ ì¸ì‹ ì¬ì‹œì‘
      if (isRunning && mediaStream.stream && mediaStream.isPermissionGranted) {
        setTimeout(() => {
          startGestureRecognition()
        }, 1000)
      }
    },
    onRecoveryFailed: (error) => {
      console.error('[FOCUS_SESSION] ì§‘ì¤‘ ì„¸ì…˜ ë³µêµ¬ ì‹¤íŒ¨:', error)
      setIsGestureActive(false)
    },
    onSessionInterrupted: () => {
      console.warn('[FOCUS_SESSION] ì§‘ì¤‘ ì„¸ì…˜ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤')
      setIsGestureActive(false)
    },
    onFallbackMode: () => {
      // ì¹´ë©”ë¼ ì—†ì´ ê¸°ë³¸ ì§‘ì¤‘ ì„¸ì…˜ ìœ ì§€
    }
  })
  
    // í”„ë ˆì„ ìŠ¤íŠ¸ë¦¬ë¨¸ì™€ ìˆ¨ê²¨ì§„ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ì°¸ì¡°
  const frameStreamerRef = useRef<FrameStreamer | null>(null)
  const hiddenVideoRef = useRef<HTMLVideoElement | null>(null)
  
  // ì œìŠ¤ì²˜ í”¼ì³ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
  const saveGestureFeatures = useCallback(async (features: GestureFeatures) => {
    let currentSessionId = sessionId
    
    // If sessionId is not provided, try to fetch active session directly
    if (!currentSessionId) {
      try {
        const supabase = supabaseBrowser()
        const { data: { user }, error: authError } = await supabase.auth.getUser()
        
        if (!authError && user) {
          const { data: activeSession, error: sessionError } = await supabase
            .from('focus_session')
            .select('session_id')
            .eq('user_id', user.id)
            .is('ended_at', null)
            .order('started_at', { ascending: false })
            .limit(1)
            .single()
          
          if (!sessionError && activeSession) {
            currentSessionId = activeSession.session_id
          }
        }
      } catch (error) {
        console.error('Failed to fetch active session:', error)
      }
    }
    
    if (!currentSessionId) {
      console.error('No session ID available')
      return
    }
    
    try {
      const supabase = supabaseBrowser()
      
      // ML í”¼ì³ ë°ì´í„° ì €ì¥ ì œê±° (í”¼ì³ ì €ì¥ ë¶ˆí•„ìš”)

      // 2. focus_sample í…Œì´ë¸”ì— ê¸°ë³¸ ì§‘ì¤‘ë„ ì ìˆ˜ë§Œ ì €ì¥
      const { error: sampleError } = await supabase
        .from('focus_sample')
        .insert({
          session_id: currentSessionId,
          ts: new Date().toISOString(),
          score: features.focusScore,
          score_conf: features.focusConfidence,
          topic_tag: 'webcam_analysis'
        })
      
      if (sampleError) {
        console.error('Focus sample save failed:', sampleError)
      }

      // 3. ì§‘ì¤‘ ìƒíƒœ ë³€í™”ë¥¼ focus_event í…Œì´ë¸”ì— ì €ì¥ (í”¼ì³ ì œê±°)
      const { error: eventError } = await supabase
        .from('focus_event')
        .insert({
          session_id: currentSessionId,
          ts: new Date().toISOString(),
          event_type: 'focus',
          payload: {
            focus_score: features.focusScore,
            focus_confidence: features.focusConfidence,
            analysis_method: 'webcam_analysis'
          }
        })
      
      if (eventError) {
        console.error('Focus event save failed:', eventError)
      }

    } catch (error) {
      console.error('Gesture features save error:', error)
    }
  }, [sessionId])

  // ìƒˆë¡œìš´ ì›¹ìº  í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ ìƒíƒœ
  const [webcamAnalysisResult, setWebcamAnalysisResult] = useState<WebcamFrameAnalysisResult | null>(null)
  const [focusFeatures, setFocusFeatures] = useState<FocusAnalysisFeatures | null>(null)
  const [lastFocusScore, setLastFocusScore] = useState<number>(85)
  
  // ì œìŠ¤ì²˜ ì¸ì‹ ìƒíƒœ ì¶”ê°€
  const [currentGesture, setCurrentGesture] = useState<string>('neutral')
  const [lastGestureTime, setLastGestureTime] = useState<string>('')
  const [gestureHistory, setGestureHistory] = useState<Array<{ gesture: string; timestamp: string }>>([])
  
  // ëŒ€ì‹œë³´ë“œ ìŠ¤í† ì–´ì—ì„œ ì§‘ì¤‘ë„ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
  const { updateFocusScore } = useDashboardStore()

  // í—¤ë“œ í¬ì¦ˆë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œìŠ¤ì²˜ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
  const determineGestureFromHeadPose = (headPose: { pitch: number; yaw: number; roll: number }): string => {
    const { pitch, yaw, roll } = headPose
    
    // ê³ ê°œ ìˆ™ì„ (pitchê°€ ì–‘ìˆ˜)
    if (pitch > 15) return 'head_down'
    // ê³ ê°œ ë“¤ê¸° (pitchê°€ ìŒìˆ˜)
    if (pitch < -15) return 'head_up'
    // ê³ ê°œ ì¢Œìš° íšŒì „ (yaw ì ˆëŒ“ê°’ì´ í¼)
    if (Math.abs(yaw) > 30) return 'head_turn'
    // ê³ ê°œ ê¸°ìš¸ê¸° (roll ì ˆëŒ“ê°’ì´ í¼)
    if (Math.abs(roll) > 20) return 'head_tilt'
    
    return 'neutral'
  }

  // ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë””ë°”ìš´ì‹±ì„ ìœ„í•œ ref
  const saveTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const lastSavedScoreRef = useRef<number | null>(null)

  // ì§‘ì¤‘ë„ ì ìˆ˜ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (2ì´ˆ ë””ë°”ìš´ì‹±)
  const saveFocusScoreToDatabase = useCallback(async (
    sessionId: string, 
    focusScore: number, 
    confidence: number, 
    timestamp: number
  ) => {
    // ì´ì „ íƒ€ì´ë¨¸ê°€ ìˆìœ¼ë©´ ì·¨ì†Œ
    if (saveTimeoutRef.current) {
      clearTimeout(saveTimeoutRef.current)
    }

    // ë§ˆì§€ë§‰ ì €ì¥ëœ ì ìˆ˜ì™€ ê°™ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
    if (lastSavedScoreRef.current === focusScore) {
      return
    }

    // 2ì´ˆ í›„ì— ì €ì¥
    saveTimeoutRef.current = setTimeout(async () => {
      try {
        const response = await fetch('/api/focus-score', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
          },
                      body: JSON.stringify({
              sessionId,
              focusScore,
              timestamp: new Date(timestamp).toISOString(),
              confidence,
              analysisMethod: 'webcam_analysis'
            })
        })

        if (response.ok) {
          console.log('âœ… ì›¹ìº  ì§‘ì¤‘ë„ ì ìˆ˜ ì €ì¥ ì„±ê³µ:', focusScore)
          lastSavedScoreRef.current = focusScore
        } else {
          console.warn('âš ï¸ ì›¹ìº  ì§‘ì¤‘ë„ ì ìˆ˜ ì €ì¥ ì‹¤íŒ¨:', response.status)
        }
      } catch (error) {
        console.error('âŒ ì›¹ìº  ì§‘ì¤‘ë„ ì ìˆ˜ ì €ì¥ ì˜¤ë¥˜:', error)
      }
    }, 2000) // 2ì´ˆ ë””ë°”ìš´ì‹±
  }, [])

  // ì œìŠ¤ì²˜ ì¸ì‹ì„ ìœ„í•œ WebSocket
  const { sendRawText, isConnected } = useWebSocket({}, {
    onMessage: useCallback((rawData: any) => {
      try {
        // ìƒˆë¡œìš´ ì›¹ìº  í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
        if (rawData && typeof rawData === 'object' && 'timestamp' in rawData && 'prediction_result' in rawData) {
          const analysisResult = rawData as WebcamFrameAnalysisResult
          console.log('ğŸ¥ ì›¹ìº  í”„ë ˆì„ ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹ :', analysisResult)
          
          // ë¶„ì„ ê²°ê³¼ ì €ì¥
          setWebcamAnalysisResult(analysisResult)
          
          // ì§‘ì¤‘ë„ ì ìˆ˜ ì¶”ì¶œ ë° ë³€í™˜
          const focusScore = analysisResult.prediction_result.prediction
          const confidence = analysisResult.prediction_result.confidence
          
          // ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
          const features: FocusAnalysisFeatures = {
            eyeStatus: {
              isOpen: analysisResult.eye_status.status === 'OPEN',
              confidence: confidence,
              earValue: analysisResult.eye_status.ear_value
            },
            headPose: {
              pitch: analysisResult.head_pose.pitch,
              yaw: analysisResult.head_pose.yaw,
              roll: analysisResult.head_pose.roll
            },
            focusScore: {
              score: focusScore,
              confidence: confidence
            },
            timestamp: analysisResult.timestamp
          }
          
          setFocusFeatures(features)
          setLastFocusScore(focusScore)
          
          // ì§‘ì¤‘ë„ ì ìˆ˜ ì—…ë°ì´íŠ¸ (ëŒ€ì‹œë³´ë“œ ìŠ¤í† ì–´)
          updateFocusScore(focusScore)
          
          // ì§‘ì¤‘ë„ ì ìˆ˜ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
          if (sessionId && isRunning) {
            saveFocusScoreToDatabase(sessionId, focusScore, confidence, features.timestamp)
          }
          
          // ì œìŠ¤ì²˜ ì¸ì‹ ê²°ê³¼ë„ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
          if (analysisResult.head_pose) {
            const gestureData = {
              gesture: determineGestureFromHeadPose(analysisResult.head_pose),
              timestamp: new Date(analysisResult.timestamp).toISOString()
            }
            setCurrentGesture(gestureData.gesture)
            setLastGestureTime(gestureData.timestamp)
            setGestureHistory((prev: Array<{ gesture: string; timestamp: string }>) => [
              gestureData,
              ...prev.slice(0, 49)
            ])
          }
        }
        // ê¸°ì¡´ ì œìŠ¤ì²˜ ì¸ì‹ ì‘ë‹µ ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        else if (rawData && typeof rawData === 'object' && 'gesture' in rawData && 'timestamp' in rawData) {
          const gestureData = rawData as { gesture: string; timestamp: string }
          setCurrentGesture(gestureData.gesture)
          setLastGestureTime(gestureData.timestamp)
          setGestureHistory((prev: Array<{ gesture: string; timestamp: string }>) => [
            gestureData,
            ...prev.slice(0, 49)
          ])
        }
        // ê¸°ì¡´ ì‘ë‹µ êµ¬ì¡° ì²˜ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
        else if (rawData && typeof rawData === 'object') {
          const data = rawData as any
          
          // í˜„ì¬ ë°›ê³  ìˆëŠ” ì‘ë‹µ êµ¬ì¡° ë¶„ì„
          const analysis = {
            timestamp: data.timestamp || 'N/A',
            eyeStatus: data.eye_status?.status || 'N/A',
            eyeValue: data.eye_status?.ear_value || 'N/A',
            handAction: data.hand_action?.action || 'N/A',
            handConfidence: data.hand_action?.confidence || 'N/A',
            headPose: {
              pitch: data.head_pose?.pitch || 'N/A',
              roll: data.head_pose?.roll || 'N/A',
              yaw: data.head_pose?.yaw || 'N/A'
            }
          }
          
          // ì§‘ì¤‘ ìƒíƒœ ê³„ì‚°ì„ ìœ„í•œ í”¼ì³ ë°ì´í„° êµ¬ì„±
          const focusFeatures = {
            visual: {
              eyeStatus: data.eye_status?.status || 'OPEN',
              earValue: data.eye_status?.ear_value || 0.3,
              headPose: {
                pitch: data.head_pose?.pitch || 0,
                yaw: data.head_pose?.yaw || 0,
                roll: data.head_pose?.roll || 0
              },
              gazeDirection: 'FORWARD' as const
            },
            // ê¸°ë³¸ê°’ë“¤ (ì‹¤ì œë¡œëŠ” ë‹¤ë¥¸ ì„¼ì„œì—ì„œ ë°›ì•„ì™€ì•¼ í•¨)
            audio: {
              isSpeaking: false,
              speechContent: '',
              isStudyRelated: true,
              confidence: 0.8,
              audioLevel: 20
            },
            behavior: {
              mouseActivity: true,
              keyboardActivity: true,
              tabSwitches: 0,
              idleTime: 0
            },
            time: {
              sessionDuration: 0,
              lastBreakTime: 0,
              consecutiveFocusTime: 0
            }
          }
          
          // ì§‘ì¤‘ ìƒíƒœ ê³„ì‚°
          const focusStatusResult = determineFocusStatus(focusFeatures)
          
          // ì œìŠ¤ì²˜ ì¸ì‹ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (ì§‘ì¤‘ ìƒíƒœ í¬í•¨)
          if (data.timestamp) {
            const features = {
              frameNumber: gestureFramesSent,
              eyeStatus: data.eye_status?.status?.substring(0, 10), // 10ìë¡œ ì œí•œ
              earValue: data.eye_status?.ear_value,
              headPose: {
                pitch: data.head_pose?.pitch,
                roll: data.head_pose?.roll,
                yaw: data.head_pose?.yaw
              },
              focusStatus: focusStatusResult.status,
              focusConfidence: focusStatusResult.confidence,
              focusScore: focusStatusResult.score
            }
            
            saveGestureFeatures(features)
          } else {
            console.warn('âš ï¸ ë°ì´í„° ì €ì¥ ì¡°ê±´ ë¯¸ì¶©ì¡±:', {
              sessionId: !!sessionId,
              timestamp: !!data.timestamp,
              sessionIdValue: sessionId,
              timestampValue: data.timestamp
            })
          }
          
        }
      } catch (error) {
        // ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜ë¥¼ ì œìŠ¤ì²˜ ì„œë²„ ì˜¤ë¥˜ë¡œ ë¶„ë¥˜
        const gestureError = classifyError(error, 'gesture')
        handleError(gestureError)
      }
    }, [sessionId, gestureFramesSent, saveGestureFeatures]),
    onOpen: () => {
    },
    onClose: () => {
    },
    onError: (error) => {
      // WebSocket ì˜¤ë¥˜ë¥¼ ì—ëŸ¬ í•¸ë“¤ëŸ¬ì— ì „ë‹¬
      const wsError = classifyError(error, 'websocket')
      handleError(wsError)
    }
  })
  
  // ì œìŠ¤ì²˜ ì¸ì‹ ì‹œì‘
  const startGestureRecognition = useCallback(() => {
    if (!mediaStream.stream || !isConnected || !enableGestureRecognition) {
      return
    }
    
    // ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ë¦¬í„´
    if (frameStreamerRef.current && frameStreamerRef.current.getIsStreaming()) {
      return
    }
    
    // ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ì¤‘ì§€
    if (frameStreamerRef.current) {
      frameStreamerRef.current.stop()
      frameStreamerRef.current = null
    }
    
    // ìˆ¨ê²¨ì§„ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ìƒì„± ë˜ëŠ” ì¬ì‚¬ìš©
    if (!hiddenVideoRef.current) {
      hiddenVideoRef.current = document.createElement('video')
      hiddenVideoRef.current.style.display = 'none'
      hiddenVideoRef.current.style.position = 'fixed'
      hiddenVideoRef.current.style.top = '-9999px'
      hiddenVideoRef.current.style.left = '-9999px'
      hiddenVideoRef.current.autoplay = true
      hiddenVideoRef.current.playsInline = true
      hiddenVideoRef.current.muted = true
      // ì›¹ìº  ì›ë³¸ í•´ìƒë„ ì‚¬ìš© (ê³ ì • í¬ê¸° ì œê±°)
      document.body.appendChild(hiddenVideoRef.current)
    }
    
    // ìŠ¤íŠ¸ë¦¼ ì—°ê²°
    hiddenVideoRef.current.srcObject = mediaStream.stream
    
    hiddenVideoRef.current.onloadedmetadata = () => {
      // ë¹„ë””ì˜¤ ì¬ìƒ ì‹œë„
      if (hiddenVideoRef.current) {
        hiddenVideoRef.current.play().catch(error => {
          console.warn('[VIDEO] ë¹„ë””ì˜¤ ìë™ ì¬ìƒ ì‹¤íŒ¨:', error)
        })
      }
      
      if (!frameStreamerRef.current && hiddenVideoRef.current) {
        // í”„ë ˆì„ ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„± ë° ì‹œì‘
        frameStreamerRef.current = new FrameStreamer(
          hiddenVideoRef.current,
          (base64) => {
            sendRawText(base64)
            setGestureFramesSent(prev => prev + 1)
          },
          (error) => {
            console.error('[GESTURE] í”„ë ˆì„ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜:', error)
            setIsGestureActive(false)
            
            // ì œìŠ¤ì²˜ ì„œë²„ ì˜¤ë¥˜ë¡œ ë¶„ë¥˜í•˜ì—¬ ì—ëŸ¬ í•¸ë“¤ëŸ¬ì— ì „ë‹¬
            const gestureError = classifyError(error, 'gesture')
            handleError(gestureError)
          },
          gestureJpegQuality
        )
        
        frameStreamerRef.current.setFrameRate(frameRate)
        frameStreamerRef.current.start()
        setIsGestureActive(true)
        setGestureFramesSent(0)
      }
    }
    
    hiddenVideoRef.current.onerror = (error) => {
      console.error('[VIDEO] ìˆ¨ê²¨ì§„ ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ì˜¤ë¥˜:', error)
      setIsGestureActive(false)
      
      // ë¹„ë””ì˜¤ ì—˜ë¦¬ë¨¼íŠ¸ ì˜¤ë¥˜ë¥¼ ì¹´ë©”ë¼ ì˜¤ë¥˜ë¡œ ë¶„ë¥˜
      const cameraError = classifyError(error, 'camera')
      handleError(cameraError)
    }
  }, [
    mediaStream.stream, 
    isConnected, 
    enableGestureRecognition, 
    frameRate, 
    gestureJpegQuality,
    sendRawText,
    setGestureFramesSent
  ])
  
  // ì œìŠ¤ì²˜ ì¸ì‹ ì¤‘ì§€
  const stopGestureRecognition = useCallback(() => {
    
    if (frameStreamerRef.current) {
      frameStreamerRef.current.stop()
      frameStreamerRef.current = null
    }
    
    if (hiddenVideoRef.current) {
      hiddenVideoRef.current.srcObject = null
      hiddenVideoRef.current.remove()
      hiddenVideoRef.current = null
    }
    
    if (isGestureActive) {
      setIsGestureActive(false)
      setGestureFramesSent(0)
    }
  }, [isGestureActive])
  
  // ì„¸ì…˜ ìƒíƒœì— ë”°ë¥¸ ìë™ ì œì–´
  useEffect(() => {
    
    if (isRunning && mediaStream.stream && mediaStream.isPermissionGranted) {
      // ì•½ê°„ì˜ ì§€ì—°ì„ ë‘ì–´ ìŠ¤íŠ¸ë¦¼ì´ ì•ˆì •í™”ë˜ë„ë¡ í•¨
      const timer = setTimeout(() => {
        startGestureRecognition()
      }, 1000)
      
      return () => clearTimeout(timer)
    } else {
      stopGestureRecognition()
    }
  }, [
    isRunning, 
    mediaStream.stream, 
    mediaStream.isPermissionGranted,
    isConnected,
    enableGestureRecognition
    // startGestureRecognition, stopGestureRecognition ì œê±°í•˜ì—¬ ë¬´í•œ ë£¨í”„ ë°©ì§€
  ])
  
  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      stopGestureRecognition()
      // ì €ì¥ íƒ€ì´ë¨¸ ì •ë¦¬
      if (saveTimeoutRef.current) {
        clearTimeout(saveTimeoutRef.current)
      }
    }
  }, []) // ì˜ì¡´ì„± ë°°ì—´ì„ ë¹„ì›Œì„œ ì–¸ë§ˆìš´íŠ¸ ì‹œì—ë§Œ ì‹¤í–‰
  
  // MediaStreamì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ë©´ì„œ ì œìŠ¤ì²˜ ê´€ë ¨ ê¸°ëŠ¥ ì¶”ê°€
  return {
    stream: mediaStream.stream,
    isLoading: mediaStream.isLoading,
    error: mediaStream.error,
    isPermissionGranted: mediaStream.isPermissionGranted,
    isPermissionDenied: mediaStream.isPermissionDenied,
    requestPermission: mediaStream.requestPermission,
    startStream: mediaStream.startStream,
    stopStream: mediaStream.stopStream,
    resetError: mediaStream.resetError,
    retryPermission: mediaStream.retryPermission,
    
    // ì œìŠ¤ì²˜ ì¸ì‹ ì¶”ê°€ ê¸°ëŠ¥
    isGestureRecognitionActive: isGestureActive,
    gestureWebSocketConnected: isConnected,
    gestureFramesSent,
    
    // ì§‘ì¤‘ ì„¸ì…˜ ì—ëŸ¬ ì²˜ë¦¬
    sessionStatus: errorHandlerState.status,
    sessionErrors: errorHandlerState.errors,
    lastSessionError: errorHandlerState.lastError,
    canRecoverFromError: errorHandlerState.lastError?.recoverable || false,
    retrySessionRecovery: retryRecovery,
    
    // ìƒˆë¡œìš´ ì›¹ìº  ë¶„ì„ ê²°ê³¼
    webcamAnalysisResult,
    focusFeatures,
    lastFocusScore,
  }
}

