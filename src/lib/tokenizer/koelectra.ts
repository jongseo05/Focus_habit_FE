// KoELECTRA í† í¬ë‚˜ì´ì € êµ¬í˜„
// WordPiece ì•Œê³ ë¦¬ì¦˜ê³¼ BERT ìŠ¤íƒ€ì¼ ì „ì²˜ë¦¬

export interface TokenizerConfig {
  vocab: string[];
  tokenizer: any;
  maxLength: number;
}

// ì „ì—­ ìºì‹œ
let vocabCache: string[] | null = null;
let tokenizerCache: any = null;

// ì–´íœ˜ ì‚¬ì „ ë¡œë“œ
export async function loadKOElectraVocab(): Promise<string[]> {
  if (vocabCache) return vocabCache;
  
  try {
    const response = await fetch("/models/koelectra/vocab.txt");
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    const text = await response.text();
    vocabCache = text.split("\n").filter(line => line.trim());
    console.log('ğŸ“š ì–´íœ˜ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ:', vocabCache.length, 'ê°œ í† í°');
    return vocabCache;
  } catch (error) {
    console.error("ì–´íœ˜ ì‚¬ì „ ë¡œë“œ ì‹¤íŒ¨:", error);
    throw error;
  }
}

// ì–´íœ˜ ì‚¬ì „ ìë™ ë¡œë“œ (ì´ˆê¸°í™” ì‹œ í˜¸ì¶œ)
export async function initializeTokenizer(): Promise<void> {
  try {
    await loadKOElectraVocab();
    await loadKOElectraTokenizer();
    console.log('âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ');
  } catch (error) {
    console.error('âŒ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
  }
}

// í† í¬ë‚˜ì´ì € ì„¤ì • ë¡œë“œ
export async function loadKOElectraTokenizer(): Promise<any> {
  if (tokenizerCache) return tokenizerCache;
  
                try {
                const response = await fetch("/models/koelectra/tokenizer.json");
                tokenizerCache = await response.json();
                return tokenizerCache;
              } catch (error) {
                console.error("í† í¬ë‚˜ì´ì € ì„¤ì • ë¡œë“œ ì‹¤íŒ¨:", error);
                throw error;
              }
}

// í…ìŠ¤íŠ¸ ì •ê·œí™” (BERT ìŠ¤íƒ€ì¼)
export function normalizeText(text: string): string {
  return text
    .trim()
    .replace(/\s+/g, " ") // ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    .normalize("NFKC"); // ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
}

// WordPiece í† í¬ë‚˜ì´ì§•
export function tokenizeWord(word: string, vocab: string[]): number[] {
  const tokens: number[] = [];
  let remaining = word;
  
  while (remaining.length > 0) {
    let found = false;
    
    // ê°€ì¥ ê¸´ ë§¤ì¹­ë˜ëŠ” ì„œë¸Œì›Œë“œ ì°¾ê¸° (greedy algorithm)
    for (let len = remaining.length; len > 0; len--) {
      const subword = remaining.substring(0, len);
      const index = vocab.indexOf(subword);
      
      if (index !== -1) {
        tokens.push(index);
        remaining = remaining.substring(len);
        found = true;
        break;
      }
    }
    
    if (!found) {
      // ë§¤ì¹­ë˜ëŠ” í† í°ì´ ì—†ìœ¼ë©´ [UNK] ì‚¬ìš©
      const unkIndex = vocab.indexOf("[UNK]");
      tokens.push(unkIndex !== -1 ? unkIndex : 1); // ê¸°ë³¸ê°’ 1
      break;
    }
  }
  
  return tokens;
}

// BERT ìŠ¤íƒ€ì¼ í† í¬ë‚˜ì´ì§•
export function tokenizeText(text: string, vocab: string[]): number[] {
  // 1. í…ìŠ¤íŠ¸ ì •ê·œí™”
  const normalizedText = normalizeText(text);
  
  // 2. ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
  const words = normalizedText.split(/\s+/);
  
  // 3. ê° ë‹¨ì–´ë¥¼ WordPieceë¡œ í† í¬ë‚˜ì´ì§•
  const tokens: number[] = [];
  
  for (const word of words) {
    if (word.length === 0) continue;
    
    const wordTokens = tokenizeWord(word, vocab);
    tokens.push(...wordTokens);
  }
  
  return tokens;
}

// KoELECTRA ì „ì²˜ë¦¬ (ë©”ì¸ í•¨ìˆ˜) - ë™ê¸° ë²„ì „ìœ¼ë¡œ ë³€ê²½
export function koelectraPreprocess(text: string, maxLength: number = 512): { input_ids: number[], attention_mask: number[] } {
  try {
    // 1. ì–´íœ˜ ì‚¬ì „ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì • ë¡œë“œ (ìºì‹œëœ ê°’ ì‚¬ìš©)
    if (!vocabCache) {
      console.warn("ì–´íœ˜ ì‚¬ì „ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìë™ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.");
      // ë¹„ë™ê¸° ë¡œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ê¸°ë³¸ê°’ ë°˜í™˜
      return {
        input_ids: new Array(maxLength).fill(0),
        attention_mask: new Array(maxLength).fill(0)
      };
    }
    
    // 2. BERT ìŠ¤íƒ€ì¼ í† í¬ë‚˜ì´ì§•
    const wordTokens = tokenizeText(text, vocabCache);
    
    // 3. BERT íŠ¹ìˆ˜ í† í° ì¶”ê°€
    const tokens: number[] = [];
    
    // [CLS] í† í° ì¶”ê°€
    const clsIndex = vocabCache.indexOf("[CLS]");
    tokens.push(clsIndex !== -1 ? clsIndex : 2);
    
    // ë‹¨ì–´ í† í°ë“¤ ì¶”ê°€
    tokens.push(...wordTokens);
    
    // [SEP] í† í° ì¶”ê°€
    const sepIndex = vocabCache.indexOf("[SEP]");
    tokens.push(sepIndex !== -1 ? sepIndex : 3);
    
    // 4. íŒ¨ë”© ë° attention mask ìƒì„±
    const input_ids: number[] = [];
    const attention_mask: number[] = [];
    
    for (let i = 0; i < maxLength; i++) {
      if (i < tokens.length) {
        input_ids.push(tokens[i]);
        attention_mask.push(1); // ì‹¤ì œ í† í°
      } else {
        const padIndex = vocabCache.indexOf("[PAD]");
        input_ids.push(padIndex !== -1 ? padIndex : 0);
        attention_mask.push(0); // íŒ¨ë”© í† í°
      }
    }
    
    return { input_ids, attention_mask };
  } catch (error) {
    console.error("í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨:", error);
    // ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
    return {
      input_ids: new Array(maxLength).fill(0),
      attention_mask: new Array(maxLength).fill(0)
    };
  }
}

// ë¹„ë™ê¸° ë²„ì „ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
export async function koelectraPreprocessAsync(text: string): Promise<number[]> {
  const result = koelectraPreprocess(text);
  return result.input_ids;
}

  // í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
  export async function testTokenizer(texts: string[]): Promise<void> {
    for (const text of texts) {
      try {
        const tokens = await koelectraPreprocess(text);
      } catch (error) {
        console.error(`âŒ "${text}" -> ì—ëŸ¬:`, error);
      }
    }
  } 