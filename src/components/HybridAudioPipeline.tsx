"use client"

import { useEffect, useRef, useState } from "react"
import { koelectraPreprocess, testTokenizer } from "@/lib/tokenizer/koelectra"
import { useKoELECTRA } from "@/hooks/useKoELECTRA"

// ê³µë¶€ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜)
async function analyzeStudyRelatedByKeywords(text: string): Promise<boolean> {
  try {
    // í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ê¸°ë³¸)
    const studyKeywords = [
      "ê³µë¶€", "í•™ìŠµ", "ìˆ˜ì—…", "ë¬¸ì œ", "ì±…", "ì½ê¸°", "ì“°ê¸°", "ê³„ì‚°", "ê³µì‹", "ì´ë¡ ",
      "ì‹œí—˜", "ê³¼ì œ", "í”„ë¡œì íŠ¸", "ë¦¬í¬íŠ¸", "ë…¼ë¬¸", "ì—°êµ¬", "ë¶„ì„", "ì‹¤í—˜",
      "ê°•ì˜", "êµê³¼ì„œ", "ì°¸ê³ ì„œ", "ë¬¸ì œì§‘", "ì—°ìŠµ", "ë³µìŠµ", "ì˜ˆìŠµ"
    ]
    
    const lowerText = text.toLowerCase()
    return studyKeywords.some(keyword => lowerText.includes(keyword))
  } catch (error) {
    console.error("ê³µë¶€ ê´€ë ¨ íŒë‹¨ ì‹¤íŒ¨:", error)
    return false
  }
}

function buildPacket({ mel, scene_tag, noise_db }: { mel: number[]; scene_tag: string; noise_db: number }) {
  return {
    timestamp: Date.now(),
    mel,
    scene_tag,
    noise_db
  }
}

type SpeechRecognitionType = typeof window extends { webkitSpeechRecognition: infer T } ? T : any;
const SpeechRecognition: any =
  typeof window !== "undefined" ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition : null;

export default function HybridAudioPipeline() {
  // KoELECTRA ëª¨ë¸ í›…
  const { 
    isLoaded: isModelLoaded, 
    isLoading: isModelLoading, 
    error: modelError, 
    inference: koelectraInference 
  } = useKoELECTRA({ autoLoad: true })

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ Ref
  const audioContextRef = useRef<AudioContext | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null)
  const workerRef = useRef<Worker | null>(null)

  // --- ìƒíƒœ ê´€ë¦¬ ---
  const recognitionRef = useRef<SpeechRecognitionType | null>(null)
  const speechTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false) // ìŒì„± êµ¬ê°„ ê°ì§€ ìƒíƒœ
  const [isListening, setIsListening] = useState(false)
  const [liveTranscript, setLiveTranscript] = useState("") // ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë  í…ìŠ¤íŠ¸
  const [isInitialized, setIsInitialized] = useState(false) // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ìƒíƒœ
  const [isInitializing, setIsInitializing] = useState(false) // ì´ˆê¸°í™” ì§„í–‰ ì¤‘ ìƒíƒœ
  const [error, setError] = useState<string | null>(null) // ì˜¤ë¥˜ ìƒíƒœ
  const speechBufferRef = useRef<string>("") // í•œ ë°œí™”ê°€ ëë‚  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ëŠ” ë²„í¼
  const featureBufferRef = useRef<any[]>([]) // í•œ ë°œí™” ë™ì•ˆì˜ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ëª¨ìœ¼ëŠ” ë²„í¼
  
  // íƒ€ì„ìŠ¤íƒ¬í”„ ê´€ë¦¬
  const speechStartTimeRef = useRef<number | null>(null)
  const speechEndTimeRef = useRef<number | null>(null)

  // useEffect í´ë¡œì €ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ Ref
  const stateRef = useRef({ isSpeaking, isListening });
  stateRef.current = { isSpeaking, isListening };

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜
  const initializeAudioPipeline = async () => {
    if (isInitialized || isInitializing) return;
    
    setIsInitializing(true);
    setError(null);
    
    try {
      // AudioContext ìƒì„± ë° ì‹¤í–‰ ìƒíƒœë¡œ ì „í™˜
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 })
      audioContextRef.current = audioContext
      
      // AudioContextê°€ suspended ìƒíƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ resume í˜¸ì¶œ
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { sampleRate: 16000, channelCount: 1 },
      })

      // AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì‹œë„
      try {
        await audioContext.audioWorklet.addModule("/audio/stft-mel-processor.js")
        const workletNode = new AudioWorkletNode(audioContext, "stft-mel-processor")
        workletNodeRef.current = workletNode

        const source = audioContext.createMediaStreamSource(stream)
        source.connect(workletNode)

        // Worker ë¡œë“œ
        try {
          // ì¡°ê±´ë¶€ Worker ìƒì„±
          const createWorker = () => {
            if (typeof window !== 'undefined' && typeof Worker !== 'undefined') {
              try {
                return new Worker("/audio/ml-inference-worker.js");
              } catch (error) {
                console.warn("Worker ìƒì„± ì‹¤íŒ¨:", error);
                return null;
              }
            }
            return null;
          };

          const worker = createWorker();
          if (worker) {
            workerRef.current = worker;

            // Workletì—ì„œ PCM ë°ì´í„°ë¥¼ Workerë¡œ ì „ë‹¬
            workletNode.port.onmessage = (e) => {
              if (e.data && e.data.pcm && workerRef.current) {
                workerRef.current.postMessage({ pcm: e.data.pcm });
              }
            };

            // Workerì—ì„œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ + scene_tag ìˆ˜ì‹ 
            worker.onmessage = (e) => {
              const { mel, scene_tag, noise_db } = e.data
              const packet = buildPacket({ mel, scene_tag, noise_db })

              if (scene_tag === "speech") {
                if (!stateRef.current.isSpeaking) {
                  setIsSpeaking(true)
                  speechBufferRef.current = ""
                  featureBufferRef.current = []
                }
                featureBufferRef.current.push(packet)
                
                // Speech Recognition ì‹œì‘
                if (recognitionRef.current && !stateRef.current.isListening) {
                  try {
                    recognitionRef.current.start()
                  } catch (err) {
                    console.error("[STT] recognition.start() ì—ëŸ¬:", err)
                  }
                }
                
                if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
                speechTimeoutRef.current = setTimeout(() => {
                  if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
                }, 2000)
              }
            }
          } else {
            throw new Error("Worker ìƒì„± ì‹¤íŒ¨");
          }

        } catch (workerError) {
          console.warn("[AUDIO] Worker ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜:", workerError)
          
          // Worker ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë¶„ì„ ëª¨ë“œ
          const analyser = audioContext.createAnalyser()
          analyser.fftSize = 256
          source.connect(analyser)
          
          const bufferLength = analyser.frequencyBinCount
          const dataArray = new Uint8Array(bufferLength)
          
          const checkAudioLevel = () => {
            analyser.getByteFrequencyData(dataArray)
            const average = dataArray.reduce((a, b) => a + b) / bufferLength
            
            // ê°„ë‹¨í•œ ìŒì„± ê°ì§€ (ì„ê³„ê°’ ê¸°ë°˜)
            if (average > 30) {
              if (!stateRef.current.isSpeaking) {
                setIsSpeaking(true)
                speechBufferRef.current = ""
                featureBufferRef.current = []
                console.log("[AUDIO] --- ğŸ¤ ë°œí™” ì‹œì‘ (ê¸°ë³¸ ëª¨ë“œ) ---")
              }
              
              // Speech Recognition ì‹œì‘
              if (recognitionRef.current && !stateRef.current.isListening) {
                try {
                  recognitionRef.current.start()
                } catch (err) {
                  console.error("[STT] recognition.start() ì—ëŸ¬:", err)
                }
              }
              
              if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
              speechTimeoutRef.current = setTimeout(() => {
                if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
              }, 2000)
            }
            
            requestAnimationFrame(checkAudioLevel)
          }
          
          checkAudioLevel()
        }

      } catch (workletError) {
        console.warn("[AUDIO] AudioWorklet ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œë¡œ ì „í™˜:", workletError)
        
        // AudioWorklet ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œ
        const analyser = audioContext.createAnalyser()
        analyser.fftSize = 256
        const source = audioContext.createMediaStreamSource(stream)
        source.connect(analyser)
        
        const bufferLength = analyser.frequencyBinCount
        const dataArray = new Uint8Array(bufferLength)
        
        const checkAudioLevel = () => {
          analyser.getByteFrequencyData(dataArray)
          const average = dataArray.reduce((a, b) => a + b) / bufferLength
          
          // ê°„ë‹¨í•œ ìŒì„± ê°ì§€ (ì„ê³„ê°’ ê¸°ë°˜)
          if (average > 30) {
            if (!stateRef.current.isSpeaking) {
              setIsSpeaking(true)
              speechBufferRef.current = ""
              featureBufferRef.current = []
            }
            
            // Speech Recognition ì‹œì‘
            if (recognitionRef.current && !stateRef.current.isListening) {
              try {
                recognitionRef.current.start()
              } catch (err) {
                console.error("[STT] recognition.start() ì—ëŸ¬:", err)
              }
            }
            
            if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
            speechTimeoutRef.current = setTimeout(() => {
              if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
            }, 2000)
          }
          
          requestAnimationFrame(checkAudioLevel)
        }
        
        checkAudioLevel()
      }

      setIsInitialized(true);

    } catch (error) {
      console.error("[AUDIO] ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì˜¤ë¥˜:", error)
      setError(`ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
    } finally {
      setIsInitializing(false);
    }
  };

  // ìŒì„± ì¸ì‹(STT) ì„¤ì •
  const setupSpeechRecognition = () => {
    if (!SpeechRecognition) {
      console.error("[STT] ì´ ë¸Œë¼ìš°ì €ëŠ” Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
      return
    }
    
    const recognition = new SpeechRecognition()
    recognition.continuous = true
    recognition.interimResults = true
    recognition.lang = "ko-KR"

    recognition.onresult = (event: any) => {
      let finalTranscript = "";
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        setLiveTranscript(event.results[i][0].transcript);
        if (event.results[i].isFinal) {
          finalTranscript += event.results[i][0].transcript;
        }
      }
      if (finalTranscript) {
        // ë°œí™” ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ì˜¬ ë•Œ)
        if (!speechStartTimeRef.current) {
          speechStartTimeRef.current = Date.now();
          console.log('ğŸ¤ ë°œí™” ì‹œì‘:', new Date().toLocaleTimeString());
        }
        speechBufferRef.current += finalTranscript.trim() + " ";
      }
    };

    recognition.onstart = () => {
      setIsListening(true);
    };
    recognition.onend = () => {
      setIsListening(false);
      if (stateRef.current.isSpeaking) {
        // ë°œí™” ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
        speechEndTimeRef.current = Date.now();
        console.log('ğŸ¤ ë°œí™” ì¢…ë£Œ:', new Date().toLocaleTimeString());
        processSpeechSegment();
        setIsSpeaking(false);
      }
    };
    recognition.onerror = (event: any) => {
      // abortedëŠ” ì •ìƒì ì¸ ì¢…ë£Œì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
      if (event.error === 'aborted') {
        console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì •ìƒ ì¢…ë£Œë¨');
        return;
      }
      
      // ë‹¤ë¥¸ ì˜¤ë¥˜ë“¤ì€ ë¡œê·¸ ì¶œë ¥
      console.error("[STT] recognition error:", event.error);
    };

    recognitionRef.current = recognition;
    console.log("[STT] recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í• ë‹¹ ì™„ë£Œ:", recognitionRef.current);
  };

  // ìˆ˜ì§‘ëœ ë°œí™” êµ¬ê°„ ë°ì´í„° ì¢…í•© ë¶„ì„
  const processSpeechSegment = async () => {
    const fullText = speechBufferRef.current.trim();
    if (!fullText) return;

    // íƒ€ì„ìŠ¤íƒ¬í”„ ê³„ì‚°
    const startTime = speechStartTimeRef.current;
    const endTime = speechEndTimeRef.current || Date.now();
    const duration = startTime ? endTime - startTime : 0;

    console.log('ğŸ¤ ë°œí™” ë¶„ì„ ì‹œì‘ ===================');
    console.log(`ğŸ“… íƒ€ì„ìŠ¤íƒ¬í”„: ${new Date(startTime || Date.now()).toLocaleTimeString()} ~ ${new Date(endTime).toLocaleTimeString()}`);
    console.log(`â±ï¸  ë°œí™” ì§€ì†ì‹œê°„: ${duration}ms (${(duration / 1000).toFixed(1)}ì´ˆ)`);
    console.log(`ğŸ’¬ ë°œí™” ë‚´ìš©: "${fullText}"`);

    try {
      // 1. KoELECTRA ëª¨ë¸ ë¶„ì„ (ìš°ì„ )
      let isStudyRelated = false;
      let confidence = 0;
      
      if (isModelLoaded && koelectraInference) {
        try {
          const koelectraResult = await koelectraInference(fullText);
          confidence = koelectraResult?.confidence || 0;
          isStudyRelated = confidence > 0.6; // ì‹ ë¢°ë„ 60% ì´ìƒì„ ê³µë¶€ ê´€ë ¨ìœ¼ë¡œ íŒë‹¨
          
          console.log(`ğŸ¤– KoELECTRA ë¶„ì„ ê²°ê³¼:`);
          console.log(`   - ì‹ ë¢°ë„: ${(confidence * 100).toFixed(1)}%`);
          console.log(`   - ê³µë¶€ ê´€ë ¨: ${isStudyRelated ? 'âœ… ì˜ˆ' : 'âŒ ì•„ë‹ˆì˜¤'}`);
          console.log(`   - ì²˜ë¦¬ ì‹œê°„: ${koelectraResult?.processingTime.toFixed(1)}ms`);
        } catch (error) {
          console.warn('âš ï¸ KoELECTRA ë¶„ì„ ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´:', error);
        }
      }

      // 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ (í´ë°± ë˜ëŠ” ë³´ì¡°)
      if (!isModelLoaded || !koelectraInference) {
        const keywordResult = await analyzeStudyRelatedByKeywords(fullText);
        isStudyRelated = keywordResult;
        console.log(`ğŸ” í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼:`);
        console.log(`   - ê³µë¶€ ê´€ë ¨: ${keywordResult ? 'âœ… ì˜ˆ' : 'âŒ ì•„ë‹ˆì˜¤'}`);
      }

      // 3. ë¬¸ë§¥ ë¶„ì„
      const context = analyzeTextContext(fullText);
      const contextualWeight = getContextualWeight(context);
      
      console.log(`ğŸ“Š ë¬¸ë§¥ ë¶„ì„ ê²°ê³¼:`);
      console.log(`   - ë¬¸ë§¥ ìœ í˜•: ${getContextLabel(context)}`);
      console.log(`   - ë¬¸ë§¥ ê°€ì¤‘ì¹˜: ${contextualWeight}`);

      // 4. ìµœì¢… íŒì •
      const finalResult = isStudyRelated ? 'ê³µë¶€ ê´€ë ¨' : 'ì¡ë‹´';
      const finalConfidence = isModelLoaded ? confidence : 0.5; // í‚¤ì›Œë“œ ê¸°ë°˜ì€ 50% ì‹ ë¢°ë„

      console.log(`ğŸ¯ ìµœì¢… íŒì •:`);
      console.log(`   - ê²°ê³¼: ${finalResult}`);
      console.log(`   - ì‹ ë¢°ë„: ${(finalConfidence * 100).toFixed(1)}%`);
      console.log(`   - ë¬¸ë§¥ ê°€ì¤‘ì¹˜: ${contextualWeight}`);
      
      // 5. ìƒì„¸ ì •ë³´ ì¶œë ¥
      console.log(`ğŸ“‹ ìƒì„¸ ì •ë³´:`);
      console.log(`   - ë°œí™” ì‹œì‘: ${new Date(startTime || Date.now()).toISOString()}`);
      console.log(`   - ë°œí™” ì¢…ë£Œ: ${new Date(endTime).toISOString()}`);
      console.log(`   - í…ìŠ¤íŠ¸ ê¸¸ì´: ${fullText.length}ì`);
      
      if (isStudyRelated) {
        console.log(`âœ… ì´ ë°œí™”ëŠ” ê³µë¶€/í•™ìŠµ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤.`);
        console.log(`   - ì§ˆë¬¸, í† ë¡ , ìˆ˜ì—… ë‚´ìš© ë“± í•™ìŠµ í™œë™ìœ¼ë¡œ ë¶„ë¥˜ë¨`);
      } else {
        console.log(`âŒ ì´ ë°œí™”ëŠ” ì¡ë‹´/ê°œì¸ì ì¸ ë‚´ìš©ì…ë‹ˆë‹¤.`);
        console.log(`   - í•™ìŠµê³¼ ë¬´ê´€í•œ ëŒ€í™”ë¡œ ë¶„ë¥˜ë¨`);
      }
      
      console.log('ğŸ¤ ë°œí™” ë¶„ì„ ì™„ë£Œ ===================\n');

    } catch (error) {
      console.error('âŒ ë°œí™” ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:', error);
    }

    // ë²„í¼ ì´ˆê¸°í™”
    speechBufferRef.current = "";
    featureBufferRef.current = [];
    speechStartTimeRef.current = null;
    speechEndTimeRef.current = null;
  };

  // í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const analyzeTextContext = (text: string):
    | 'discussion'
    | 'class'
    | 'presentation'
    | 'question'
    | 'frustration'
    | 'statement'
    | 'unknown' => {
    const lower = text.toLowerCase();

    // í† ë¡  ìƒí™©
    if (
      lower.includes("í† ë¡ ") ||
      lower.includes("ì°¬ì„±") ||
      lower.includes("ë°˜ëŒ€") ||
      lower.includes("ì˜ê²¬") ||
      lower.includes("ìƒê°ì€")
    ) {
      return 'discussion';
    }
    // ìˆ˜ì—… ìƒí™©
    if (
      lower.includes("ì„ ìƒë‹˜") ||
      lower.includes("êµê³¼ì„œ") ||
      lower.includes("ë¬¸ì œ") ||
      lower.includes("ì„¤ëª…") ||
      lower.includes("ìˆ˜ì—…")
    ) {
      return 'class';
    }
    // ë°œí‘œ ìƒí™©
    if (
      lower.includes("ë°œí‘œ") ||
      lower.includes("ê²°ë¡ ") ||
      lower.includes("ìš”ì•½") ||
      lower.includes("ì •ë¦¬")
    ) {
      return 'presentation';
    }
    // ì§ˆë¬¸
    if (
      lower.includes("ì–´ë–»ê²Œ") ||
      lower.includes("ì™œ") ||
      lower.includes("ë­ì•¼") ||
      lower.includes("ë¬´ì—‡") ||
      lower.includes("ê¶ê¸ˆ") ||
      text.endsWith("?")
    ) {
      return 'question';
    }
    // ì¢Œì ˆ
    if (
      lower.includes("ì•ˆë¼") ||
      lower.includes("ì§œì¦ë‚˜") ||
      lower.includes("ì•„ì˜¤")
    ) {
      return 'frustration';
    }
    // ì§„ìˆ 
    if (text.length > 5) {
      return 'statement';
    }
    return 'unknown';
  };

  // ë¬¸ë§¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextualWeight = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): number => {
    switch (context) {
      case 'discussion': return 0.95; // í† ë¡ : ì§‘ì¤‘ë„ ì•½ê°„ ë†’ìŒ
      case 'class': return 1.0; // ìˆ˜ì—…: ì§‘ì¤‘ë„ ìµœìƒ
      case 'presentation': return 0.9; // ë°œí‘œ: ì§‘ì¤‘ë„ ë†’ìŒ
      case 'question': return 0.8; // ì§ˆë¬¸: ë§‰í˜”ì„ ê°€ëŠ¥ì„±, ì§‘ì¤‘ë„ ì•½ê°„ ê°ì†Œ
      case 'frustration': return 0.5; // ì¢Œì ˆ: ëª…í™•í•œ ì§‘ì¤‘ë ¥ ì €í•˜ ì‹ í˜¸, ì§‘ì¤‘ë„ í¬ê²Œ ê°ì†Œ
      case 'statement': return 1.0; // í˜¼ì£ë§/ë‚´ìš© ì½ê¸°: í•™ìŠµ í™œë™ì˜ ì¼ë¶€ë¡œ íŒë‹¨, ê°€ì¤‘ì¹˜ ì—†ìŒ
      default: return 0.9; // ë¶ˆëª…í™•: ì§§ì€ ì¤‘ì–¼ê±°ë¦¼ ë“±, ì•½ê°„ì˜ ë°©í•´ë¡œ ê°„ì£¼
    }
  };

  // ë¬¸ë§¥ ìœ í˜•ì„ í•œê¸€ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextLabel = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): string => {
    switch (context) {
      case 'discussion': return 'í† ë¡ ';
      case 'class': return 'ìˆ˜ì—…';
      case 'presentation': return 'ë°œí‘œ';
      case 'question': return 'ì§ˆë¬¸';
      case 'frustration': return 'ì¢Œì ˆ';
      case 'statement': return 'ì§„ìˆ ';
      default: return 'ë¶ˆëª…í™•';
    }
  };

  useEffect(() => {
    // Speech Recognition ì„¤ì •ë§Œ ë¨¼ì € ìˆ˜í–‰
    setupSpeechRecognition();

    return () => {
      if (recognitionRef.current) recognitionRef.current.abort()
      if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
      if (audioContextRef.current) audioContextRef.current.close()
      if (workerRef.current) workerRef.current.terminate()
    }
  }, [])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ìë™ìœ¼ë¡œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
  useEffect(() => {
    if (!isInitialized && !isInitializing) {
      console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ìë™ ì´ˆê¸°í™” ì‹œì‘')
      initializeAudioPipeline()
    }
  }, [isInitialized, isInitializing])

  // í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
  const handleTokenizerTest = async () => {
    const testTexts = [
      "ì•ˆë…•í•˜ì„¸ìš”",
      "ê³µë¶€ë¥¼ í•˜ê³  ìˆì–´ìš”",
      "ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ê³  ìˆìŠµë‹ˆë‹¤",
      "ì´ë¡ ì„ ê³µë¶€í•˜ê³  ìˆì–´ìš”",
      "í† ë¡ ì„ í•˜ê³  ìˆì–´ìš”",
      "ì„ ìƒë‹˜ì´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
      "ì–´ë–»ê²Œ í’€ì–´ì•¼ í• ê¹Œìš”?",
      "ì§œì¦ë‚˜ìš” ì´ ë¬¸ì œê°€ ì•ˆ í’€ë ¤ìš”"
    ];
    
    await testTokenizer(testTexts);
  };

  return (
    <div className="p-4 bg-slate-100 rounded-lg">
      <h3 className="font-bold">í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸</h3>
      
      {/* í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ë²„íŠ¼ */}
      <div className="mb-4">
        <button 
          onClick={handleTokenizerTest}
          className="px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600 mr-2"
        >
          í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
        </button>
        <span className="text-sm text-gray-600">ìƒˆë¡œìš´ WordPiece í† í¬ë‚˜ì´ì €ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤</span>
      </div>
      
      {isInitializing && (
        <p className="text-blue-600 mb-4">ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...</p>
      )}
      
      {error && (
        <p className="text-red-600 mb-4">ì˜¤ë¥˜: {error}</p>
      )}
      
      {/* KoELECTRA ëª¨ë¸ ìƒíƒœ */}
      <div className="mb-4 p-3 bg-gray-50 rounded">
        <h4 className="font-semibold mb-2">ğŸ¤– KoELECTRA ëª¨ë¸ ìƒíƒœ</h4>
        <div className="space-y-1 text-sm">
          <p><b>ëª¨ë¸ ë¡œë“œ:</b> 
            {isModelLoading ? "ğŸ”„ ë¡œë”© ì¤‘..." : 
             isModelLoaded ? "âœ… ë¡œë“œë¨" : "âŒ ë¯¸ë¡œë“œ"}
          </p>
          {modelError && (
            <p className="text-red-600"><b>ëª¨ë¸ ì—ëŸ¬:</b> {modelError}</p>
          )}
        </div>
      </div>
      
      {isInitialized && (
        <div className="space-y-2">
          <p><b>ìŒì„± ì¸ì‹ ìƒíƒœ:</b> {isListening ? "ë“£ëŠ” ì¤‘..." : "ëŒ€ê¸° ì¤‘"}</p>
          <p><b>ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸:</b> {liveTranscript}</p>
        </div>
      )}
    </div>
  )
}
