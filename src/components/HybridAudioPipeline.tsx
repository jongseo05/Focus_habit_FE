"use client"

import { useEffect, useRef, useState, useCallback, useMemo } from "react"
import { koelectraPreprocess, testTokenizer, initializeTokenizer } from "@/lib/tokenizer/koelectra"
import { useKoELECTRA } from "@/hooks/useKoELECTRA"
import { useDashboardStore } from "@/stores/dashboardStore"

// ê³µë¶€ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜) - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
const analyzeStudyRelatedByKeywords = (() => {
  const studyKeywords = [
    "ê³µë¶€", "í•™ìŠµ", "ìˆ˜ì—…", "ë¬¸ì œ", "ì±…", "ì½ê¸°", "ì“°ê¸°", "ê³„ì‚°", "ê³µì‹", "ì´ë¡ ",
    "ì‹œí—˜", "ê³¼ì œ", "í”„ë¡œì íŠ¸", "ë¦¬í¬íŠ¸", "ë…¼ë¬¸", "ì—°êµ¬", "ë¶„ì„", "ì‹¤í—˜",
    "ê°•ì˜", "êµê³¼ì„œ", "ì°¸ê³ ì„œ", "ë¬¸ì œì§‘", "ì—°ìŠµ", "ë³µìŠµ", "ì˜ˆìŠµ"
  ]
  
  const keywordSet = new Set(studyKeywords.map(k => k.toLowerCase()))
  
  return (text: string): boolean => {
    const lowerText = text.toLowerCase()
    return studyKeywords.some(keyword => lowerText.includes(keyword))
  }
})()

// ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë””ë°”ìš´ìŠ¤ í›…
function useDebounce<T>(value: T, delay: number): T {
  const [debouncedValue, setDebouncedValue] = useState<T>(value)

  useEffect(() => {
    const handler = setTimeout(() => {
      setDebouncedValue(value)
    }, delay)

    return () => {
      clearTimeout(handler)
    }
  }, [value, delay])

  return debouncedValue
}



function buildPacket({ mel, scene_tag, noise_db }: { mel: number[]; scene_tag: string; noise_db: number }) {
  return {
    timestamp: Date.now(),
    mel,
    scene_tag,
    noise_db
  }
}

type SpeechRecognitionType = typeof window extends { webkitSpeechRecognition: infer T } ? T : any;
const SpeechRecognition: any =
  typeof window !== "undefined" ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition : null;

export default function HybridAudioPipeline() {
  // ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
  const { isRunning: isFocusSessionRunning, isPaused: isFocusSessionPaused } = useDashboardStore()
  
  // KoELECTRA ëª¨ë¸ í›…
  const { 
    isLoaded: isModelLoaded, 
    isLoading: isModelLoading, 
    error: modelError, 
    inference: koelectraInference
  } = useKoELECTRA({ 
    autoLoad: true
  })

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ Ref
  const audioContextRef = useRef<AudioContext | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null)
  const workerRef = useRef<Worker | null>(null)

  // --- ìƒíƒœ ê´€ë¦¬ ---
  const recognitionRef = useRef<SpeechRecognitionType | null>(null)
  const speechTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false) // ìŒì„± êµ¬ê°„ ê°ì§€ ìƒíƒœ
  const [isListening, setIsListening] = useState(false)
  const [liveTranscript, setLiveTranscript] = useState("") // ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë  í…ìŠ¤íŠ¸
  const [isInitialized, setIsInitialized] = useState(false) // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ìƒíƒœ
  const [isInitializing, setIsInitializing] = useState(false) // ì´ˆê¸°í™” ì§„í–‰ ì¤‘ ìƒíƒœ
  const [error, setError] = useState<string | null>(null) // ì˜¤ë¥˜ ìƒíƒœ
  const speechBufferRef = useRef<string>("") // í•œ ë°œí™”ê°€ ëë‚  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ëŠ” ë²„í¼
  const featureBufferRef = useRef<any[]>([]) // í•œ ë°œí™” ë™ì•ˆì˜ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ëª¨ìœ¼ëŠ” ë²„í¼
  
  // íƒ€ì„ìŠ¤íƒ¬í”„ ê´€ë¦¬
  const speechStartTimeRef = useRef<number | null>(null)
  const speechEndTimeRef = useRef<number | null>(null)

  // ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë””ë°”ìš´ìŠ¤ëœ í…ìŠ¤íŠ¸
  const debouncedLiveTranscript = useDebounce(liveTranscript, 100)

  // useEffect í´ë¡œì €ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ Ref
  const stateRef = useRef({ isSpeaking, isListening });
  stateRef.current = { isSpeaking, isListening };

  // ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ë³€í™” ê°ì§€ ë° ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì œì–´
  useEffect(() => {
    console.log('ğŸ¯ ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ë³€í™”:', { 
      isRunning: isFocusSessionRunning, 
      isPaused: isFocusSessionPaused 
    })

    // ì§‘ì¤‘ ëª¨ë“œê°€ ì¢…ë£Œë˜ê±°ë‚˜ ì¼ì‹œì •ì§€ëœ ê²½ìš°
    if (!isFocusSessionRunning || isFocusSessionPaused) {
      console.log('â¸ï¸ ì§‘ì¤‘ ëª¨ë“œ ì¼ì‹œì •ì§€/ì¢…ë£Œ - ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨')
      
      // ìŒì„± ì¸ì‹ ì¤‘ë‹¨
      if (recognitionRef.current && recognitionRef.current.state === 'active') {
        try {
          recognitionRef.current.stop()
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ë¨')
        } catch (error) {
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì¤‘ ì˜¤ë¥˜:', error)
        }
      }
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨
      setIsListening(false)
      console.log('ğŸ”‡ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨ë¨')
      
      // ë°œí™” ë²„í¼ ì´ˆê¸°í™”
      speechBufferRef.current = ""
      speechStartTimeRef.current = null
      speechEndTimeRef.current = null
      
      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
      setLiveTranscript("")
      setIsSpeaking(false)
      
      console.log('âœ… ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì¼ì‹œì •ì§€ ì™„ë£Œ')
    }
    
    // ì§‘ì¤‘ ëª¨ë“œê°€ ì¬ì‹œì‘ëœ ê²½ìš°
    else if (isFocusSessionRunning && !isFocusSessionPaused && isInitialized) {
      console.log('â–¶ï¸ ì§‘ì¤‘ ëª¨ë“œ ì¬ì‹œì‘ - ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘')
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¬ì‹œì‘
      setIsListening(true)
      console.log('ğŸ”Š ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¬ì‹œì‘ë¨')
      
      // ìŒì„± ì¸ì‹ ì¬ì‹œì‘
      if (recognitionRef.current && recognitionRef.current.state === 'inactive') {
        try {
          recognitionRef.current.start()
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ë¨')
        } catch (error) {
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜:', error)
        }
      }
      
      console.log('âœ… ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì¬ì‹œì‘ ì™„ë£Œ')
    }
  }, [isFocusSessionRunning, isFocusSessionPaused, isInitialized])

  // ëª¨ë¸ ìƒíƒœ ìš”ì•½
  const modelStatus = useMemo(() => ({
    status: isModelLoaded ? 'âœ… ë¡œë“œë¨' : isModelLoading ? 'ğŸ”„ ë¡œë”© ì¤‘' : 'âŒ ë¡œë“œ ì•ˆë¨'
  }), [isModelLoaded, isModelLoading])

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜ - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
  const initializeAudioPipeline = useCallback(async () => {
    if (isInitialized || isInitializing) return;
    
    setIsInitializing(true);
    setError(null);
    
    try {
      // AudioContext ìƒì„± ë° ì‹¤í–‰ ìƒíƒœë¡œ ì „í™˜
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ 
        sampleRate: 16000,
        latencyHint: 'interactive' // ì„±ëŠ¥ ìµœì í™”
      })
      audioContextRef.current = audioContext
      
      // AudioContext ìƒíƒœ í™•ì¸ ë° ë³µêµ¬
      console.log('ğŸµ AudioContext ì´ˆê¸° ìƒíƒœ:', audioContext.state);
      
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
        console.log('ğŸµ AudioContext resumed:', audioContext.state);
      }
      
      if (audioContext.state === 'closed') {
        throw new Error('AudioContextê°€ ë‹«í˜€ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.');
      }
      
      // AudioContext ìƒíƒœê°€ runningì¸ì§€ í™•ì¸
      if (audioContext.state !== 'running') {
        console.warn('âš ï¸ AudioContextê°€ running ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤:', audioContext.state);
        await audioContext.resume();
        console.log('ğŸµ AudioContext ê°•ì œ resume í›„ ìƒíƒœ:', audioContext.state);
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { 
          sampleRate: 16000, 
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        },
      })

              // AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì‹œë„ (AudioContext ìƒíƒœ í™•ì¸ í›„)
        try {
          if (audioContext.state !== 'running') {
            await audioContext.resume();
          }
          
          await audioContext.audioWorklet.addModule("/audio/stft-mel-processor.js")
          const workletNode = new AudioWorkletNode(audioContext, "stft-mel-processor", {
            numberOfInputs: 1,
            numberOfOutputs: 0,
            processorOptions: {
              sampleRate: 16000,
              frameSize: 1024,
              hopSize: 512
            }
          })
          workletNodeRef.current = workletNode

          const source = audioContext.createMediaStreamSource(stream)
          source.connect(workletNode)

        // Web Worker ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        const createWorker = () => {
          if (typeof window !== 'undefined' && typeof Worker !== 'undefined') {
            try {
              return new Worker("/audio/ml-inference-worker.js");
            } catch (error) {
              console.warn("Worker ìƒì„± ì‹¤íŒ¨:", error);
              return null;
            }
          }
          return null;
        };

        const worker = createWorker();
        workerRef.current = worker;

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ í•¨ìˆ˜ (ëª…í™•í•œ ìŒì„± ê°ì§€ ìµœì í™”)
        const checkAudioLevel = async () => {
          console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ í•¨ìˆ˜ í˜¸ì¶œë¨');
          
          // AudioContext ìƒíƒœ í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
          if (!audioContext) {
            console.error('âŒ AudioContextê°€ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨');
            return;
          }
          
          console.log('ğŸµ AudioContext ìƒíƒœ:', audioContext.state);
          
          if (audioContext.state === 'closed') {
            console.error('âŒ AudioContextê°€ ë‹«í˜€ìˆìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨');
            return;
          }
          
          // suspended ìƒíƒœë©´ resume ì‹œë„
          if (audioContext.state === 'suspended') {
            try {
              await audioContext.resume();
              console.log('ğŸµ AudioContext resumed from suspended');
            } catch (error) {
              console.warn('âš ï¸ AudioContext resume ì‹¤íŒ¨:', error);
              return;
            }
          }
          
          // ë””ë²„ê¹…: ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹¤í–‰ í™•ì¸
          console.log('ğŸ” ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹¤í–‰ ì¤‘... (isListening:', stateRef.current.isListening, ')');
          
          if (!stateRef.current.isListening) {
            console.log('ğŸ”‡ isListeningì´ false - ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨');
            return;
          }
          
          // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë¡œì§
          console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ì‹œì‘...');
          const analyser = audioContext.createAnalyser();
          analyser.fftSize = 512; // ë” ì •ë°€í•œ ë¶„ì„
          const bufferLength = analyser.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          
          source.connect(analyser);
          analyser.getByteFrequencyData(dataArray);
          console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ë²„í¼ ê¸¸ì´:', bufferLength, ')');
          
          // í‰ê·  ë ˆë²¨ ê³„ì‚° (ìŒì„± í•„í„°ë§)
          const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
          
          // ë””ë²„ê¹…: ì˜¤ë””ì˜¤ ë ˆë²¨ ê°’ ì¶œë ¥ (ë” ìì£¼)
          if (Math.random() < 0.3) { // 30% í™•ë¥ ë¡œ ë¡œê¹…
            console.log('ğŸ“Š í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨:', average.toFixed(1));
          }
          
          // ìŒì„± ê°ì§€ (ì ì ˆí•œ ì„ê³„ê°’)
          if (average > 25 && !stateRef.current.isSpeaking) {
            setIsSpeaking(true);
            speechStartTimeRef.current = Date.now();
            console.log('ğŸ¤ ìŒì„± ê°ì§€ë¨ (ë ˆë²¨:', average.toFixed(1), ')');
          } else if (average < 15 && stateRef.current.isSpeaking) {
            // ë°œí™” ì¢…ë£Œ ê°ì§€ (ì¡°ìš©í•´ì§€ë©´ ì¦‰ì‹œ ì¢…ë£Œ)
            setIsSpeaking(false);
            speechEndTimeRef.current = Date.now();
            console.log('ğŸ¤ ìŒì„± ì¢…ë£Œ ê°ì§€ë¨ (ë ˆë²¨:', average.toFixed(1), ')');
          }
          
          // ì¡°ìš©í•œ ìƒíƒœì—ì„œ ìŒì„± ì¸ì‹ ì¼ì‹œ ì¤‘ë‹¨ (ë°°ê²½ ì†ŒìŒ ë°©ì§€)
          if (average < 10 && recognitionRef.current && recognitionRef.current.state === 'active') {
            try {
              recognitionRef.current.stop();
              console.log('ğŸ”‡ ì¡°ìš©í•œ ìƒíƒœ - ìŒì„± ì¸ì‹ ì¼ì‹œ ì¤‘ë‹¨ (ë ˆë²¨:', average.toFixed(1), ')');
            } catch (error) {
              console.log('ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì¤‘...');
            }
          }
          
          requestAnimationFrame(() => checkAudioLevel());
        };

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        setIsListening(true); // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ trueë¡œ ì„¤ì •
        console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨');
        
        // ê°•ì œë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹œì‘
        setTimeout(() => {
          console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ê°•ì œ ì‹œì‘');
          checkAudioLevel();
        }, 100);

        // Speech Recognition ì„¤ì •
        setupSpeechRecognition();

        setIsInitialized(true);
        console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ (ì„±ëŠ¥ ìµœì í™” ì ìš©)');
      } catch (workletError) {
        console.warn("AudioWorklet ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¡œ ëŒ€ì²´:", workletError);
        
                 // ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ (ëª…í™•í•œ ìŒì„± ê°ì§€)
         const source = audioContext.createMediaStreamSource(stream);
         const checkAudioLevel = async () => {
           // AudioContext ìƒíƒœ í™•ì¸
           if (!audioContext || audioContext.state === 'closed') {
             console.error('âŒ AudioContextê°€ ë‹«í˜€ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨');
             return;
           }
           
           if (audioContext.state !== 'running') {
             console.warn('âš ï¸ AudioContextê°€ running ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤:', audioContext.state);
             return;
           }
           
           // ë””ë²„ê¹…: ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹¤í–‰ í™•ì¸
           console.log('ğŸ” ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹¤í–‰ ì¤‘... (isListening:', stateRef.current.isListening, ')');
           
           if (!stateRef.current.isListening) {
             console.log('ğŸ”‡ isListeningì´ false - ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨');
             return;
           }
           
           const analyser = audioContext.createAnalyser();
           analyser.fftSize = 512; // ë” ì •ë°€í•œ ë¶„ì„
           const bufferLength = analyser.frequencyBinCount;
           const dataArray = new Uint8Array(bufferLength);
           
                     source.connect(analyser);
          analyser.getByteFrequencyData(dataArray);
          
          const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
          
          // ë””ë²„ê¹…: ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ê°’ ì¶œë ¥ (ë” ìì£¼)
          if (Math.random() < 0.3) { // 30% í™•ë¥ ë¡œ ë¡œê¹…
            console.log('ğŸ“Š í˜„ì¬ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨:', average.toFixed(1));
          }
          
          // ìŒì„± ê°ì§€ (ì ì ˆí•œ ì„ê³„ê°’)
          if (average > 25 && !stateRef.current.isSpeaking) {
            setIsSpeaking(true);
            speechStartTimeRef.current = Date.now();
            console.log('ğŸ¤ ìŒì„± ê°ì§€ë¨ (ë ˆë²¨:', average.toFixed(1), ')');
          } else if (average < 15 && stateRef.current.isSpeaking) {
            // ë°œí™” ì¢…ë£Œ ê°ì§€ (ì¡°ìš©í•´ì§€ë©´ ì¦‰ì‹œ ì¢…ë£Œ)
            setIsSpeaking(false);
            speechEndTimeRef.current = Date.now();
            console.log('ğŸ¤ ìŒì„± ì¢…ë£Œ ê°ì§€ë¨ (ë ˆë²¨:', average.toFixed(1), ')');
          }
          
          requestAnimationFrame(() => checkAudioLevel());
        };

        setIsListening(true); // ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ trueë¡œ ì„¤ì •
        console.log('ğŸ¤ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨');
        
        // ê°•ì œë¡œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹œì‘
        setTimeout(() => {
          console.log('ğŸ¤ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ê°•ì œ ì‹œì‘');
          checkAudioLevel();
        }, 100);
        setupSpeechRecognition();
        setIsInitialized(true);
      }
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      setError(error instanceof Error ? error.message : 'ì´ˆê¸°í™” ì‹¤íŒ¨');
    } finally {
      setIsInitializing(false);
    }
  }, [isInitialized, isInitializing]);

  // Speech Recognition ì„¤ì • - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
  const setupSpeechRecognition = useCallback(() => {
    if (!SpeechRecognition) {
      console.warn("Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.");
      return;
    }

    // ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ì¤‘ë‹¨
    if (recognitionRef.current && stateRef.current.isListening) {
      try {
        recognitionRef.current.stop();
      } catch (error) {
        console.log('ê¸°ì¡´ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì¤‘...');
      }
    }

    const recognition = new SpeechRecognition();
    recognitionRef.current = recognition;

    // ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR';
    recognition.maxAlternatives = 1;

    let finalTranscript = '';
    let interimTranscript = '';

    recognition.onstart = () => {
      setIsListening(true);
      console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ë¨');
    };

    recognition.onresult = (event: any) => {
      let interimTranscript = '';
      let finalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
          // ì²« ë²ˆì§¸ finalTranscript ìˆ˜ì‹  ì‹œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
          if (speechStartTimeRef.current === null) {
            speechStartTimeRef.current = Date.now();
            console.log('ğŸ¤ ë°œí™” ì‹œì‘ ì‹œê°„ ê¸°ë¡:', new Date().toLocaleTimeString());
          }
        } else {
          interimTranscript += transcript;
        }
      }

      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë””ë°”ìš´ìŠ¤ ì ìš©)
      setLiveTranscript(interimTranscript);
      
      if (finalTranscript) {
        speechBufferRef.current += finalTranscript;
        console.log('ğŸ¤ ìµœì¢… í…ìŠ¤íŠ¸ ì¶”ê°€ë¨:', finalTranscript);
      }
    };

    recognition.onend = () => {
      setIsListening(false);
      speechEndTimeRef.current = Date.now();
      console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¢…ë£Œë¨ - ë°œí™” ë¶„ì„ ì‹œì‘');
      
      // ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ê°•ì œ ì‹¤í–‰)
      if (speechBufferRef.current.trim()) {
        console.log('ğŸ¤ ë°œí™” ë²„í¼ ë‚´ìš©:', speechBufferRef.current);
        processSpeechSegment();
      } else {
        console.log('ğŸ¤ ë°œí™” ë²„í¼ê°€ ë¹„ì–´ìˆìŒ - ë¶„ì„ ê±´ë„ˆëœ€');
      }
      
      // ì¦‰ì‹œ ì¬ì‹œì‘ (ì—°ì† ì¸ì‹ì„ ìœ„í•´) - ìƒíƒœ ì²´í¬ ê°•í™”
      if (recognitionRef.current && stateRef.current.isListening) {
        try {
          // ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œì‘
          if (recognitionRef.current.state === 'inactive') {
            recognitionRef.current.start();
            setIsListening(true);
            console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ë¨');
          } else {
            console.log('ğŸ¤ ìŒì„± ì¸ì‹ì´ ì´ë¯¸ í™œì„± ìƒíƒœì…ë‹ˆë‹¤ (ìƒíƒœ:', recognitionRef.current.state, ')');
          }
        } catch (error) {
          console.warn('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹¤íŒ¨:', error);
          // ì¬ì‹œì‘ ì‹¤íŒ¨ ì‹œ 200ms í›„ ì¬ì‹œë„
          setTimeout(() => {
            if (recognitionRef.current && stateRef.current.isListening) {
              try {
                if (recognitionRef.current.state === 'inactive') {
                  recognitionRef.current.start();
                  setIsListening(true);
                  console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¬ì‹œë„ ì„±ê³µ');
                }
              } catch (retryError) {
                console.error('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¬ì‹œë„ ì‹¤íŒ¨:', retryError);
              }
            }
          }, 200);
        }
      }
    };

    recognition.onerror = (event: any) => {
      // abortedëŠ” ì •ìƒì ì¸ ì¢…ë£Œì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
      if (event.error === 'aborted') {
        console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì •ìƒ ì¢…ë£Œë¨');
        return;
      }

      // no-speechëŠ” ì •ìƒì ì¸ ìƒí™©
      if (event.error === 'no-speech') {
        console.log('ğŸ¤ ìŒì„± ì—†ìŒ - ì •ìƒì ì¸ ìƒí™©');
        return;
      }

      // ë‹¤ë¥¸ ì˜¤ë¥˜ë“¤ì€ ë¡œê·¸ ì¶œë ¥
      console.error("[STT] recognition error:", event.error);
    };

    // ì•ˆì „í•˜ê²Œ ì‹œì‘
    try {
      recognition.start();
    } catch (error) {
      console.warn('ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', error);
    }
  }, []);

  // ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ - ì„±ëŠ¥ ìµœì í™” ì ìš©
  const processSpeechSegment = useCallback(async () => {
    const text = speechBufferRef.current.trim();
    if (!text) return;

    const startTime = performance.now();
    
    try {
      // ë°œí™” ì§€ì†ì‹œê°„ ê³„ì‚°
      const duration = speechStartTimeRef.current && speechEndTimeRef.current 
        ? (speechEndTimeRef.current - speechStartTimeRef.current) / 1000 
        : 0;

      // KoELECTRA ëª¨ë¸ ì¶”ë¡  (ìš°ì„ ìˆœìœ„)
      let isStudyRelated = false;
      let koelectraConfidence = 0;
      let analysisMethod = 'í‚¤ì›Œë“œ';

      if (isModelLoaded) {
        try {
          const result = await koelectraInference(text);
          if (result && result.confidence >= 0.6) {
            isStudyRelated = result.logits[1] > result.logits[0]; // ê³µë¶€ ê´€ë ¨ í´ë˜ìŠ¤ê°€ ë” ë†’ì€ ê²½ìš°
            koelectraConfidence = result.confidence;
            analysisMethod = 'KoELECTRA';
          } else {
            // ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
            isStudyRelated = analyzeStudyRelatedByKeywords(text);
          }
        } catch (error) {
          console.warn('KoELECTRA ì¶”ë¡  ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´:', error);
          isStudyRelated = analyzeStudyRelatedByKeywords(text);
        }
      } else {
        // ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜
        isStudyRelated = analyzeStudyRelatedByKeywords(text);
      }

      // ë¬¸ë§¥ ë¶„ì„
      const context = analyzeTextContext(text);
      const contextualWeight = getContextualWeight(context);
      const contextLabel = getContextLabel(context);

      // ìµœì¢… íŒì • (ë¬¸ë§¥ ê°€ì¤‘ì¹˜ ì ìš©)
      const finalJudgment = isStudyRelated && contextualWeight > 0.3;

      const processingTime = performance.now() - startTime;

      // êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
      console.log(`
ğŸ¤ ë°œí™” ë¶„ì„ ê²°ê³¼ (${new Date().toLocaleTimeString()}):
â”œâ”€ ì‹œê°„: ${new Date().toLocaleString()}
â”œâ”€ ì§€ì†ì‹œê°„: ${duration.toFixed(1)}ì´ˆ
â”œâ”€ ì›ë¬¸: "${text}"
â”œâ”€ ë¶„ì„ ë°©ë²•: ${analysisMethod}
â”œâ”€ KoELECTRA ì‹ ë¢°ë„: ${koelectraConfidence.toFixed(3)}
â”œâ”€ ê³µë¶€ ê´€ë ¨: ${isStudyRelated ? 'âœ…' : 'âŒ'}
â”œâ”€ ë¬¸ë§¥: ${contextLabel} (ê°€ì¤‘ì¹˜: ${contextualWeight.toFixed(2)})
â”œâ”€ ìµœì¢… íŒì •: ${finalJudgment ? 'ê³µë¶€ ê´€ë ¨ ë°œí™”' : 'ì¡ë‹´'}
â””â”€ ì²˜ë¦¬ ì‹œê°„: ${processingTime.toFixed(1)}ms
      `);

      // ë²„í¼ ì´ˆê¸°í™”
      speechBufferRef.current = "";
      speechStartTimeRef.current = null;
      speechEndTimeRef.current = null;

    } catch (error) {
      console.error('ë°œí™” ë¶„ì„ ì‹¤íŒ¨:', error);
      speechBufferRef.current = "";
    }
  }, [isModelLoaded, koelectraInference]);

  // í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const analyzeTextContext = (text: string):
    | 'discussion'
    | 'class'
    | 'presentation'
    | 'question'
    | 'frustration'
    | 'statement'
    | 'unknown' => {
    const lower = text.toLowerCase();

    // í† ë¡  ìƒí™©
    if (
      lower.includes("í† ë¡ ") ||
      lower.includes("ì°¬ì„±") ||
      lower.includes("ë°˜ëŒ€") ||
      lower.includes("ì˜ê²¬") ||
      lower.includes("ìƒê°ì€")
    ) {
      return 'discussion';
    }
    // ìˆ˜ì—… ìƒí™©
    if (
      lower.includes("ì„ ìƒë‹˜") ||
      lower.includes("êµê³¼ì„œ") ||
      lower.includes("ë¬¸ì œ") ||
      lower.includes("ì„¤ëª…") ||
      lower.includes("ìˆ˜ì—…")
    ) {
      return 'class';
    }
    // ë°œí‘œ ìƒí™©
    if (
      lower.includes("ë°œí‘œ") ||
      lower.includes("ê²°ë¡ ") ||
      lower.includes("ìš”ì•½") ||
      lower.includes("ì •ë¦¬")
    ) {
      return 'presentation';
    }
    // ì§ˆë¬¸
    if (
      lower.includes("ì–´ë–»ê²Œ") ||
      lower.includes("ì™œ") ||
      lower.includes("ë­ì•¼") ||
      lower.includes("ë¬´ì—‡") ||
      lower.includes("ê¶ê¸ˆ") ||
      text.endsWith("?")
    ) {
      return 'question';
    }
    // ì¢Œì ˆ
    if (
      lower.includes("ì•ˆë¼") ||
      lower.includes("ì§œì¦ë‚˜") ||
      lower.includes("ì•„ì˜¤")
    ) {
      return 'frustration';
    }
    // ì§„ìˆ 
    if (text.length > 5) {
      return 'statement';
    }
    return 'unknown';
  };

  // ë¬¸ë§¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextualWeight = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): number => {
    switch (context) {
      case 'discussion': return 0.95; // í† ë¡ : ì§‘ì¤‘ë„ ì•½ê°„ ë†’ìŒ
      case 'class': return 1.0; // ìˆ˜ì—…: ì§‘ì¤‘ë„ ìµœìƒ
      case 'presentation': return 0.9; // ë°œí‘œ: ì§‘ì¤‘ë„ ë†’ìŒ
      case 'question': return 0.8; // ì§ˆë¬¸: ë§‰í˜”ì„ ê°€ëŠ¥ì„±, ì§‘ì¤‘ë„ ì•½ê°„ ê°ì†Œ
      case 'frustration': return 0.5; // ì¢Œì ˆ: ëª…í™•í•œ ì§‘ì¤‘ë ¥ ì €í•˜ ì‹ í˜¸, ì§‘ì¤‘ë„ í¬ê²Œ ê°ì†Œ
      case 'statement': return 1.0; // í˜¼ì£ë§/ë‚´ìš© ì½ê¸°: í•™ìŠµ í™œë™ì˜ ì¼ë¶€ë¡œ íŒë‹¨, ê°€ì¤‘ì¹˜ ì—†ìŒ
      default: return 0.9; // ë¶ˆëª…í™•: ì§§ì€ ì¤‘ì–¼ê±°ë¦¼ ë“±, ì•½ê°„ì˜ ë°©í•´ë¡œ ê°„ì£¼
    }
  };

  // ë¬¸ë§¥ ìœ í˜•ì„ í•œê¸€ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextLabel = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): string => {
    switch (context) {
      case 'discussion': return 'í† ë¡ ';
      case 'class': return 'ìˆ˜ì—…';
      case 'presentation': return 'ë°œí‘œ';
      case 'question': return 'ì§ˆë¬¸';
      case 'frustration': return 'ì¢Œì ˆ';
      case 'statement': return 'ì§„ìˆ ';
      default: return 'ë¶ˆëª…í™•';
    }
  };

  useEffect(() => {
    // Speech Recognition ì„¤ì •ë§Œ ë¨¼ì € ìˆ˜í–‰
    setupSpeechRecognition();

    return () => {
      if (recognitionRef.current) recognitionRef.current.abort()
      if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
      // AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ (ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ ìœ ì§€)
      if (workerRef.current) workerRef.current.terminate()
    }
  }, [])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ í† í¬ë‚˜ì´ì € ë° ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
  useEffect(() => {
    const initializeComponents = async () => {
      // 1. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ìš°ì„ ìˆœìœ„)
      try {
        console.log('ğŸ“š í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹œì‘...');
        await initializeTokenizer();
        console.log('âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì™„ë£Œ');
      } catch (error) {
        console.error('âŒ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      }
      
      // 2. ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (í† í¬ë‚˜ì´ì € ë¡œë“œ í›„)
      setTimeout(() => {
        if (!isInitialized && !isInitializing) {
          console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ìë™ ì´ˆê¸°í™” ì‹œì‘')
          initializeAudioPipeline()
        }
      }, 500); // í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ í›„ 500ms ëŒ€ê¸°
    };
    
    initializeComponents();
  }, [isInitialized, isInitializing, initializeAudioPipeline])

  // í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
  const handleTokenizerTest = async () => {
    const testTexts = [
      "ì•ˆë…•í•˜ì„¸ìš”",
      "ê³µë¶€ë¥¼ í•˜ê³  ìˆì–´ìš”",
      "ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ê³  ìˆìŠµë‹ˆë‹¤",
      "ì´ë¡ ì„ ê³µë¶€í•˜ê³  ìˆì–´ìš”",
      "í† ë¡ ì„ í•˜ê³  ìˆì–´ìš”",
      "ì„ ìƒë‹˜ì´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
      "ì–´ë–»ê²Œ í’€ì–´ì•¼ í• ê¹Œìš”?",
      "ì§œì¦ë‚˜ìš” ì´ ë¬¸ì œê°€ ì•ˆ í’€ë ¤ìš”"
    ];
    
    await testTokenizer(testTexts);
  };

  return (
    <div className="p-4 bg-slate-100 rounded-lg">
      <h3 className="font-bold">í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸</h3>
      
      {/* í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ë²„íŠ¼ */}
      <div className="mb-4">
        <button 
          onClick={handleTokenizerTest}
          className="px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600 mr-2"
        >
          í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
        </button>
        <span className="text-sm text-gray-600">ìƒˆë¡œìš´ WordPiece í† í¬ë‚˜ì´ì €ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤</span>
      </div>
      
      {isInitializing && (
        <p className="text-blue-600 mb-4">ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...</p>
      )}
      
      {error && (
        <p className="text-red-600 mb-4">ì˜¤ë¥˜: {error}</p>
      )}
      
      {/* KoELECTRA ëª¨ë¸ ìƒíƒœ */}
      <div className="mb-4 p-3 bg-gray-50 rounded">
        <h4 className="font-semibold mb-2">ğŸ¤– KoELECTRA ëª¨ë¸ ìƒíƒœ</h4>
        <div className="space-y-1 text-sm">
          <p><b>ëª¨ë¸ ë¡œë“œ:</b> 
            {isModelLoading ? "ğŸ”„ ë¡œë”© ì¤‘..." : 
             isModelLoaded ? "âœ… ë¡œë“œë¨" : "âŒ ë¯¸ë¡œë“œ"}
          </p>
          {modelError && (
            <p className="text-red-600"><b>ëª¨ë¸ ì—ëŸ¬:</b> {modelError}</p>
          )}
        </div>
      </div>


      
      {isInitialized && (
        <div className="space-y-2">
          <p><b>ìŒì„± ì¸ì‹ ìƒíƒœ:</b> {isListening ? "ë“£ëŠ” ì¤‘..." : "ëŒ€ê¸° ì¤‘"}</p>
          <p><b>ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸:</b> {liveTranscript}</p>
        </div>
      )}
    </div>
  )
}
