"use client"

import { useEffect, useRef, useState, useCallback, useMemo } from "react"
import { useDashboardStore } from "@/stores/dashboardStore"

// ê³µë¶€ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜) - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
const analyzeStudyRelatedByKeywords = (() => {
  const studyKeywords = [
    "ê³µë¶€", "í•™ìŠµ", "ìˆ˜ì—…", "ë¬¸ì œ", "ì±…", "ì½ê¸°", "ì“°ê¸°", "ê³„ì‚°", "ê³µì‹", "ì´ë¡ ",
    "ì‹œí—˜", "ê³¼ì œ", "í”„ë¡œì íŠ¸", "ë¦¬í¬íŠ¸", "ë…¼ë¬¸", "ì—°êµ¬", "ë¶„ì„", "ì‹¤í—˜",
    "ê°•ì˜", "êµê³¼ì„œ", "ì°¸ê³ ì„œ", "ë¬¸ì œì§‘", "ì—°ìŠµ", "ë³µìŠµ", "ì˜ˆìŠµ"
  ]
  
  const keywordSet = new Set(studyKeywords.map(k => k.toLowerCase()))
  
  return (text: string): boolean => {
    const lowerText = text.toLowerCase()
    return studyKeywords.some(keyword => lowerText.includes(keyword))
  }
})()

  // ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë””ë°”ìš´ìŠ¤ í›…
  function useDebounce<T>(value: T, delay: number): T {
    const [debouncedValue, setDebouncedValue] = useState<T>(value)

    useEffect(() => {
      const handler = setTimeout(() => {
        setDebouncedValue(value)
      }, delay)

      return () => {
        clearTimeout(handler)
      }
    }, [value, delay])

    return debouncedValue
  }



function buildPacket({ mel, scene_tag, noise_db }: { mel: number[]; scene_tag: string; noise_db: number }) {
  return {
    timestamp: Date.now(),
    mel,
    scene_tag,
    noise_db
  }
}

type SpeechRecognitionType = typeof window extends { webkitSpeechRecognition: infer T } ? T : any;
const SpeechRecognition: any =
  typeof window !== "undefined" ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition : null;

export default function HybridAudioPipeline() {
  // ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
  const { 
    isRunning: isFocusSessionRunning, 
    isPaused: isFocusSessionPaused,
    focusScore,
    updateFocusScore
  } = useDashboardStore()
  


  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ Ref
  const audioContextRef = useRef<AudioContext | null>(null)
  // AudioWorklet ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
  // Web Worker ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
  const streamRef = useRef<MediaStream | null>(null) // ìŠ¤íŠ¸ë¦¼ì„ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©

  // --- ìƒíƒœ ê´€ë¦¬ ---
  const recognitionRef = useRef<SpeechRecognitionType | null>(null)
  const speechTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false) // ìŒì„± êµ¬ê°„ ê°ì§€ ìƒíƒœ
  const [isListening, setIsListening] = useState(false)
  const [isAnalyzing, setIsAnalyzing] = useState(false)
  const [isSpeechRecognitionActive, setIsSpeechRecognitionActive] = useState(false)
  const [liveTranscript, setLiveTranscript] = useState("") // ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë  í…ìŠ¤íŠ¸
  const [isInitialized, setIsInitialized] = useState(false) // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ìƒíƒœ
  const [isInitializing, setIsInitializing] = useState(false) // ì´ˆê¸°í™” ì§„í–‰ ì¤‘ ìƒíƒœ
  const [error, setError] = useState<string | null>(null) // ì˜¤ë¥˜ ìƒíƒœ
  const [currentAudioLevel, setCurrentAudioLevel] = useState<number>(0) // í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨
  const speechBufferRef = useRef<string>("") // í•œ ë°œí™”ê°€ ëë‚  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ëŠ” ë²„í¼
  const featureBufferRef = useRef<any[]>([]) // í•œ ë°œí™” ë™ì•ˆì˜ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ëª¨ìœ¼ëŠ” ë²„í¼
  
  // íƒ€ì„ìŠ¤íƒ¬í”„ ê´€ë¦¬
  const speechStartTimeRef = useRef<number | null>(null)
  const speechEndTimeRef = useRef<number | null>(null)
  const silenceStartTimeRef = useRef<number | null>(null) // ì¡°ìš©í•¨ ì‹œì‘ ì‹œê°„
  
  // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ ìƒíƒœ
  const audioLevelSpeechStartRef = useRef<number | null>(null) // ì˜¤ë””ì˜¤ ë ˆë²¨ë¡œ ê°ì§€í•œ ë°œí™” ì‹œì‘ ì‹œê°„
  const audioLevelSpeechEndRef = useRef<number | null>(null) // ì˜¤ë””ì˜¤ ë ˆë²¨ë¡œ ê°ì§€í•œ ë°œí™” ì¢…ë£Œ ì‹œê°„
  const isAudioLevelSpeakingRef = useRef<boolean>(false) // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ìƒíƒœ
  
  // ì˜¤ë””ì˜¤ ë ˆë²¨ ë³€í™” ê°ì§€ìš© ìƒíƒœ
  const previousAudioLevelRef = useRef<number>(0) // ì´ì „ ì˜¤ë””ì˜¤ ë ˆë²¨
  const audioLevelHistoryRef = useRef<number[]>([]) // ìµœê·¼ ì˜¤ë””ì˜¤ ë ˆë²¨ íˆìŠ¤í† ë¦¬ (ìµœëŒ€ 10ê°œ)
  const rapidDropDetectedRef = useRef<boolean>(false) // ê¸‰ê²©í•œ í•˜ë½ ê°ì§€ í”Œë˜ê·¸

  // ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë””ë°”ìš´ìŠ¤ëœ í…ìŠ¤íŠ¸
  const debouncedLiveTranscript = useDebounce(liveTranscript, 100)

  // useEffect í´ë¡œì €ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ Ref
  const stateRef = useRef({ isSpeaking, isListening });
  stateRef.current = { isSpeaking, isListening };

  // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸‰ê²©í•œ í•˜ë½ ê°ì§€ í•¨ìˆ˜
  const detectRapidAudioLevelDrop = useCallback((currentLevel: number): boolean => {
    // íˆìŠ¤í† ë¦¬ì— í˜„ì¬ ë ˆë²¨ ì¶”ê°€ (ìµœëŒ€ 10ê°œ ìœ ì§€)
    audioLevelHistoryRef.current.push(currentLevel);
    if (audioLevelHistoryRef.current.length > 10) {
      audioLevelHistoryRef.current.shift();
    }
    
    // ìµœì†Œ 3ê°œì˜ ë°ì´í„°ê°€ ìˆì–´ì•¼ ë¶„ì„
    if (audioLevelHistoryRef.current.length < 3) {
      previousAudioLevelRef.current = currentLevel;
      return false;
    }
    
    // ìµœê·¼ 3ê°œ ê°’ì˜ í‰ê· ê³¼ ì´ì „ 3ê°œ ê°’ì˜ í‰ê·  ë¹„êµ
    const recentValues = audioLevelHistoryRef.current.slice(-3);
    const previousValues = audioLevelHistoryRef.current.slice(-6, -3);
    
    if (previousValues.length < 3) {
      previousAudioLevelRef.current = currentLevel;
      return false;
    }
    
    const recentAverage = recentValues.reduce((sum, val) => sum + val, 0) / recentValues.length;
    const previousAverage = previousValues.reduce((sum, val) => sum + val, 0) / previousValues.length;
    
    // ê¸‰ê²©í•œ í•˜ë½ ê°ì§€ (ì´ì „ í‰ê·  ëŒ€ë¹„ 50% ì´ìƒ ê°ì†Œ)
    const dropRatio = recentAverage / previousAverage;
    const isRapidDrop = dropRatio < 0.5 && previousAverage > 30 && recentAverage < 25;
    
    if (isRapidDrop && !rapidDropDetectedRef.current) {
      console.log('ğŸ¤ ê¸‰ê²©í•œ ì˜¤ë””ì˜¤ ë ˆë²¨ í•˜ë½ ê°ì§€:', {
        ì´ì „í‰ê· : previousAverage.toFixed(1),
        í˜„ì¬í‰ê· : recentAverage.toFixed(1),
        í•˜ë½ë¹„ìœ¨: (dropRatio * 100).toFixed(1) + '%'
      });
      rapidDropDetectedRef.current = true;
    }
    
    previousAudioLevelRef.current = currentLevel;
    return isRapidDrop;
  }, []);

  // ìŒì„± ì¸ì‹ ì¬ì‹œì‘ í•¨ìˆ˜ (ì¤‘ì•™ ê´€ë¦¬) - ê°œì„ ëœ ë²„ì „
  const restartSpeechRecognition = useCallback(() => {
    if (!recognitionRef.current) {
      console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹¤íŒ¨: recognitionRefê°€ ì—†ìŒ');
      return false;
    }

    if (!stateRef.current.isListening) {
      console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹¤íŒ¨: isListeningì´ false');
      return false;
    }

    const currentState = recognitionRef.current.state;
    
    // inactive ë˜ëŠ” undefined ìƒíƒœì¼ ë•Œ ì¬ì‹œì‘ (ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ê³ ë ¤)
    if (currentState === 'inactive' || currentState === undefined) {
      try {
        recognitionRef.current.start();
        return true;
      } catch (error) {
        console.warn('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹¤íŒ¨:', error);
        return false;
      }
    } else if (currentState === 'active') {
      return true; // ì´ë¯¸ í™œì„± ìƒíƒœë©´ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
    } else {
      return false;
    }
  }, []);

  // ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ë³€í™” ê°ì§€ ë° ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì œì–´
  useEffect(() => {
    // ì§‘ì¤‘ ëª¨ë“œê°€ ì¢…ë£Œë˜ê±°ë‚˜ ì¼ì‹œì •ì§€ëœ ê²½ìš°
    if (!isFocusSessionRunning || isFocusSessionPaused) {
      // ìŒì„± ì¸ì‹ ì¤‘ë‹¨
      if (recognitionRef.current && recognitionRef.current.state === 'active') {
        try {
          recognitionRef.current.stop()
        } catch (error) {
          // ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì˜¤ë¥˜ëŠ” ë¬´ì‹œ
        }
      }
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨
      setIsListening(false)
      
      // ë°œí™” ë²„í¼ ì´ˆê¸°í™”
      speechBufferRef.current = ""
      speechStartTimeRef.current = null
      speechEndTimeRef.current = null
      silenceStartTimeRef.current = null
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ ìƒíƒœ ì´ˆê¸°í™”
      isAudioLevelSpeakingRef.current = false
      audioLevelSpeechStartRef.current = null
      audioLevelSpeechEndRef.current = null
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ë³€í™” ê°ì§€ ìƒíƒœ ì´ˆê¸°í™”
      previousAudioLevelRef.current = 0
      audioLevelHistoryRef.current = []
      rapidDropDetectedRef.current = false
      
      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
      setLiveTranscript("")
      setIsSpeaking(false)
    }
    
    // ì§‘ì¤‘ ëª¨ë“œê°€ ì¬ì‹œì‘ëœ ê²½ìš°
    else if (isFocusSessionRunning && !isFocusSessionPaused && isInitialized) {
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¬ì‹œì‘
      setIsListening(true)
      
      // ìŒì„± ì¸ì‹ ì¬ì‹œì‘ (ì¤‘ì•™ í•¨ìˆ˜ ì‚¬ìš©)
      restartSpeechRecognition();
    }
  }, [isFocusSessionRunning, isFocusSessionPaused, isInitialized, restartSpeechRecognition])



  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜ - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
  const initializeAudioPipeline = useCallback(async () => {
    if (isInitialized || isInitializing) return;
    
    setIsInitializing(true);
    setError(null);
    
    try {
      // AudioContext ìƒì„± ë° ì‹¤í–‰ ìƒíƒœë¡œ ì „í™˜
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ 
        sampleRate: 16000,
        latencyHint: 'interactive' // ì„±ëŠ¥ ìµœì í™”
      })
      audioContextRef.current = audioContext
      
      // AudioContext ìƒíƒœ í™•ì¸ ë° ë³µêµ¬
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
      }
      
      if (audioContext.state === 'closed') {
        throw new Error('AudioContextê°€ ë‹«í˜€ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.');
      }
      
      // AudioContext ìƒíƒœê°€ runningì¸ì§€ í™•ì¸
      if (audioContext.state !== 'running') {
        await audioContext.resume();
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ë° ìŠ¤íŠ¸ë¦¼ ìƒì„± (í•œ ë²ˆë§Œ)
      if (!streamRef.current) {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 16000
          }
        });
        streamRef.current = stream;
        
        // ì˜¤ë””ì˜¤ íŠ¸ë™ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
        const audioTrack = stream.getAudioTracks()[0];
        audioTrack.onended = () => {};
        audioTrack.onmute = () => {};
        audioTrack.onunmute = () => {};
      }
      
      const stream = streamRef.current;

              // ë§ˆì´í¬ íŠ¸ë™ì—ì„œ ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„° í™•ì¸
        const audioTrack = stream.getAudioTracks()[0];
        if (audioTrack) {
          // ì˜¤ë””ì˜¤ íŠ¸ë™ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
          audioTrack.onended = () => {};
          audioTrack.onmute = () => {};
          audioTrack.onunmute = () => {};
        }

              // AudioWorklet ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
        try {
          if (audioContext.state !== 'running') {
            await audioContext.resume();
          }
          
          // AudioWorklet ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
          


          // Web Worker ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ìš© ì†ŒìŠ¤ì™€ ë¶„ì„ê¸° (í•œ ë²ˆë§Œ ìƒì„±)
        const levelSource = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048; // ë” í° FFT í¬ê¸°ë¡œ ë³€ê²½
        analyser.smoothingTimeConstant = 0.1; // ë” ë¹ ë¥¸ ë°˜ì‘
        analyser.minDecibels = -90; // ë” ë‚®ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        analyser.maxDecibels = -10; // ë” ë†’ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        levelSource.connect(analyser);

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ í•¨ìˆ˜ (ëª…í™•í•œ ìŒì„± ê°ì§€ ìµœì í™”)
        const checkAudioLevel = async () => {
          // AudioContext ìƒíƒœ í™•ì¸
          if (!audioContext || audioContext.state === 'closed') {
            return;
          }
          
          if (audioContext.state === 'suspended') {
            try {
              await audioContext.resume();
            } catch (error) {
              return;
            }
          }
          
          if (!stateRef.current.isListening) {
            return;
          }
          
          // ìŠ¤íŠ¸ë¦¼ ìƒíƒœ í™•ì¸
          if (!streamRef.current || !streamRef.current.active) {
            return;
          }
          
          const audioTrack = streamRef.current.getAudioTracks()[0];
          if (!audioTrack || !audioTrack.enabled || audioTrack.muted) {
            return;
          }
          
          // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë¡œì§ - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ê¸° ì‚¬ìš©
          const bufferLength = analyser.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          analyser.getByteFrequencyData(dataArray);
          
          // ì‹œê°„ ë„ë©”ì¸ ë°ì´í„°ë¡œë„ ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° ì‹œë„
          const timeDataArray = new Uint8Array(bufferLength);
          analyser.getByteTimeDomainData(timeDataArray);
          
          // RMS (Root Mean Square) ê³„ì‚°ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •
          let rms = 0;
          for (let i = 0; i < timeDataArray.length; i++) {
            const sample = (timeDataArray[i] - 128) / 128; // -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            rms += sample * sample;
          }
          rms = Math.sqrt(rms / timeDataArray.length);
          const rmsLevel = rms * 100; // 0-100 ë²”ìœ„ë¡œ ë³€í™˜
          
          // ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë°ì´í„° ë¶„ì„ (ë” ë¯¼ê°í•˜ê²Œ)
          const freqAverage = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
          const freqMax = Math.max(...dataArray);
          const freqMin = Math.min(...dataArray);
          
          // ë” ë¯¼ê°í•œ ë ˆë²¨ ê³„ì‚° (ì£¼íŒŒìˆ˜ ìµœëŒ€ê°’ê³¼ RMS ì¤‘ ë” ë†’ì€ ê°’ ì‚¬ìš©)
          const finalLevel = Math.max(freqMax * 0.5, rmsLevel * 2);
          
          // ë””ë²„ê¹…: ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
          const hasAudioData = freqMax > 0 || rms > 0.001;
          
          setCurrentAudioLevel(finalLevel);
          
          // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ (ê¸‰ê²©í•œ í•˜ë½ ê°ì§€ ì¶”ê°€)
          const SPEECH_THRESHOLD = 40; // ë§í•  ë•Œ ì¼ë°˜ì ìœ¼ë¡œ 40 ì´ìƒ
          const SILENCE_THRESHOLD = 25; // ì¡°ìš©í•¨ ê¸°ì¤€ (10-25 ì‚¬ì´ê°€ ì¼ë°˜ ìƒí™©)
          const SILENCE_DURATION = 800; // 0.8ì´ˆ ì¡°ìš©í•˜ë©´ ë°œí™” ì¢…ë£Œë¡œ íŒë‹¨
          
          // ê¸‰ê²©í•œ ì˜¤ë””ì˜¤ ë ˆë²¨ í•˜ë½ ê°ì§€
          const isRapidDrop = detectRapidAudioLevelDrop(finalLevel);
          
          // ë°œí™” ì‹œì‘ ê°ì§€ (ì˜¤ë””ì˜¤ ë ˆë²¨ì´ ì„ê³„ê°’ì„ ë„˜ì„ ë•Œ)
          if (finalLevel > SPEECH_THRESHOLD && !isAudioLevelSpeakingRef.current) {
            isAudioLevelSpeakingRef.current = true;
            audioLevelSpeechStartRef.current = Date.now();
            silenceStartTimeRef.current = null; // ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
            rapidDropDetectedRef.current = false; // ê¸‰ê²©í•œ í•˜ë½ í”Œë˜ê·¸ ë¦¬ì…‹
            
            // ë°œí™” ì‹œì‘ ì‹œì  ì§‘ì¤‘ë„ ì €ì¥
            saveSpeechStartFocusScore();
            
            console.log('ğŸ¤ ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ì‹œì‘ ê°ì§€ (ë ˆë²¨:', finalLevel.toFixed(1), ')');
          }
          
          // ë°œí™” ì¢…ë£Œ ê°ì§€ (ê¸‰ê²©í•œ í•˜ë½ ë˜ëŠ” ì¼ì • ì‹œê°„ ì¡°ìš©í•¨)
          if ((isRapidDrop || finalLevel < SILENCE_THRESHOLD) && isAudioLevelSpeakingRef.current) {
            // ê¸‰ê²©í•œ í•˜ë½ì´ ê°ì§€ë˜ë©´ ì¦‰ì‹œ ë°œí™” ì¢…ë£Œ
            if (isRapidDrop) {
              isAudioLevelSpeakingRef.current = false;
              audioLevelSpeechEndRef.current = Date.now();
              
              // ì‹¤ì œ ë°œí™” ì§€ì†ì‹œê°„ ê³„ì‚°
              const actualSpeechDuration = audioLevelSpeechStartRef.current && audioLevelSpeechEndRef.current 
                ? (audioLevelSpeechEndRef.current - audioLevelSpeechStartRef.current) / 1000 
                : 0;
              
              console.log('ğŸ¤ ê¸‰ê²©í•œ í•˜ë½ìœ¼ë¡œ ì¸í•œ ë°œí™” ì¢…ë£Œ ê°ì§€:', {
                ë ˆë²¨: finalLevel.toFixed(1),
                ì‹¤ì œë°œí™”ì‹œê°„: actualSpeechDuration.toFixed(1) + 'ì´ˆ',
                í•˜ë½ê°ì§€: 'ì¦‰ì‹œ'
              });
              
              // ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±° (ì‹¤ì œ ë°œí™” ì‹œê°„ì´ 0.5ì´ˆ ì´ìƒì¼ ë•Œë§Œ)
              if (speechBufferRef.current.trim() && actualSpeechDuration > 0.5) {
                console.log('ğŸ¤ ê¸‰ê²©í•œ í•˜ë½ - ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±° - ë²„í¼ ë‚´ìš©:', speechBufferRef.current);
                processSpeechSegment();
              } else if (speechBufferRef.current.trim()) {
                console.log('ğŸ¤ ê¸‰ê²©í•œ í•˜ë½ - ë°œí™” ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŒ (', actualSpeechDuration.toFixed(1), 'ì´ˆ) - ë¶„ì„ ê±´ë„ˆëœ€');
                speechBufferRef.current = ""; // ë²„í¼ ì´ˆê¸°í™”
              } else {
                console.log('ğŸ¤ ê¸‰ê²©í•œ í•˜ë½ - ë°œí™” ë²„í¼ê°€ ë¹„ì–´ìˆìŒ - ë¶„ì„ ê±´ë„ˆëœ€');
              }
              
              // ìƒíƒœ ë¦¬ì…‹
              silenceStartTimeRef.current = null;
              audioLevelSpeechStartRef.current = null;
              audioLevelSpeechEndRef.current = null;
              rapidDropDetectedRef.current = false;
            } else {
              // ì¼ë°˜ì ì¸ ì¡°ìš©í•¨ ê°ì§€ (ê¸°ì¡´ ë¡œì§)
              if (!silenceStartTimeRef.current) {
                silenceStartTimeRef.current = Date.now();
              }
              
              const silenceDuration = Date.now() - silenceStartTimeRef.current;
              if (silenceDuration > SILENCE_DURATION) {
                isAudioLevelSpeakingRef.current = false;
                audioLevelSpeechEndRef.current = Date.now();
                
                const actualSpeechDuration = audioLevelSpeechStartRef.current && audioLevelSpeechEndRef.current 
                  ? (audioLevelSpeechEndRef.current - audioLevelSpeechStartRef.current) / 1000 
                  : 0;
                
                console.log('ğŸ¤ ì¡°ìš©í•¨ ì§€ì†ìœ¼ë¡œ ì¸í•œ ë°œí™” ì¢…ë£Œ ê°ì§€:', {
                  ë ˆë²¨: finalLevel.toFixed(1),
                  ì¡°ìš©í•¨ì§€ì†: silenceDuration + 'ms',
                  ì‹¤ì œë°œí™”ì‹œê°„: actualSpeechDuration.toFixed(1) + 'ì´ˆ'
                });
                
                // ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±° (ì‹¤ì œ ë°œí™” ì‹œê°„ì´ 0.5ì´ˆ ì´ìƒì¼ ë•Œë§Œ)
                if (speechBufferRef.current.trim() && actualSpeechDuration > 0.5) {
                  console.log('ğŸ¤ ì¡°ìš©í•¨ ì§€ì† - ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±° - ë²„í¼ ë‚´ìš©:', speechBufferRef.current);
                  processSpeechSegment();
                } else if (speechBufferRef.current.trim()) {
                  console.log('ğŸ¤ ì¡°ìš©í•¨ ì§€ì† - ë°œí™” ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŒ (', actualSpeechDuration.toFixed(1), 'ì´ˆ) - ë¶„ì„ ê±´ë„ˆëœ€');
                  speechBufferRef.current = ""; // ë²„í¼ ì´ˆê¸°í™”
                } else {
                  console.log('ğŸ¤ ì¡°ìš©í•¨ ì§€ì† - ë°œí™” ë²„í¼ê°€ ë¹„ì–´ìˆìŒ - ë¶„ì„ ê±´ë„ˆëœ€');
                }
                
                // íƒ€ì´ë¨¸ ë¦¬ì…‹
                silenceStartTimeRef.current = null;
                audioLevelSpeechStartRef.current = null;
                audioLevelSpeechEndRef.current = null;
              }
            }
          } else if (finalLevel >= SILENCE_THRESHOLD && isAudioLevelSpeakingRef.current) {
            // ë‹¤ì‹œ ì†Œë¦¬ê°€ ë‚˜ë©´ ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
            silenceStartTimeRef.current = null;
          }
          
          // ê¸°ì¡´ ìŒì„± ê°ì§€ ë¡œì§ë„ ìœ ì§€ (UI í‘œì‹œìš©)
          if (finalLevel > 5 && !stateRef.current.isSpeaking) {
            setIsSpeaking(true);
            speechStartTimeRef.current = Date.now();
          } else if (finalLevel < 2 && stateRef.current.isSpeaking) {
            setIsSpeaking(false);
            speechEndTimeRef.current = Date.now();
          }
          
          requestAnimationFrame(() => checkAudioLevel());
        };

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        setIsListening(true); // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ trueë¡œ ì„¤ì •
        
        // ê°•ì œë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹œì‘
        setTimeout(() => {
          checkAudioLevel();
        }, 100);

        // Speech Recognition ì„¤ì •
        setupSpeechRecognition();

        setIsInitialized(true);
        console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ');
              } catch (error) {
        // AudioWorklet ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
        
        // ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ (ëª…í™•í•œ ìŒì„± ê°ì§€)
        // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ìš© ì†ŒìŠ¤ì™€ ë¶„ì„ê¸° (í•œ ë²ˆë§Œ ìƒì„±)
        const source = audioContext.createMediaStreamSource(streamRef.current);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048; // ë” í° FFT í¬ê¸°ë¡œ ë³€ê²½
        analyser.smoothingTimeConstant = 0.1; // ë” ë¹ ë¥¸ ë°˜ì‘
        analyser.minDecibels = -90; // ë” ë‚®ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        analyser.maxDecibels = -10; // ë” ë†’ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        source.connect(analyser);
         
         const checkAudioLevel = async () => {
           // AudioContext ìƒíƒœ í™•ì¸
           if (!audioContext || audioContext.state === 'closed') {
             return;
           }
           
           if (audioContext.state !== 'running') {
             return;
           }
           
           if (!stateRef.current.isListening) {
             return;
           }
           
           // ìŠ¤íŠ¸ë¦¼ ìƒíƒœ í™•ì¸
           if (!streamRef.current || !streamRef.current.active) {
             return;
           }
           
           const audioTrack = streamRef.current.getAudioTracks()[0];
           if (!audioTrack || !audioTrack.enabled || audioTrack.muted) {
             return;
           }
           
           // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë¡œì§ - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ê¸° ì‚¬ìš©
           const bufferLength = analyser.frequencyBinCount;
           const dataArray = new Uint8Array(bufferLength);
           analyser.getByteFrequencyData(dataArray);
           
           // ì‹œê°„ ë„ë©”ì¸ ë°ì´í„°ë¡œë„ ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° ì‹œë„
           const timeDataArray = new Uint8Array(bufferLength);
           analyser.getByteTimeDomainData(timeDataArray);
           
           // RMS (Root Mean Square) ê³„ì‚°ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •
           let rms = 0;
           for (let i = 0; i < timeDataArray.length; i++) {
             const sample = (timeDataArray[i] - 128) / 128; // -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
             rms += sample * sample;
           }
           rms = Math.sqrt(rms / timeDataArray.length);
           const rmsLevel = rms * 100;
           
           // ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë°ì´í„° ë¶„ì„ (ë” ë¯¼ê°í•˜ê²Œ)
           const freqAverage = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
           const freqMax = Math.max(...dataArray);
           const freqMin = Math.min(...dataArray);
           
           // ë” ë¯¼ê°í•œ ë ˆë²¨ ê³„ì‚° (ì£¼íŒŒìˆ˜ ìµœëŒ€ê°’ê³¼ RMS ì¤‘ ë” ë†’ì€ ê°’ ì‚¬ìš©)
           const finalLevel = Math.max(freqMax * 0.5, rmsLevel * 2);
           
           // ë””ë²„ê¹…: ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
           const hasAudioData = freqMax > 0 || rms > 0.001;
           
           setCurrentAudioLevel(finalLevel);
           
           // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ (ê¸‰ê²©í•œ í•˜ë½ ê°ì§€ ì¶”ê°€)
           const SPEECH_THRESHOLD = 40; // ë§í•  ë•Œ ì¼ë°˜ì ìœ¼ë¡œ 40 ì´ìƒ
           const SILENCE_THRESHOLD = 25; // ì¡°ìš©í•¨ ê¸°ì¤€ (10-25 ì‚¬ì´ê°€ ì¼ë°˜ ìƒí™©)
           const SILENCE_DURATION = 800; // 0.8ì´ˆ ì¡°ìš©í•˜ë©´ ë°œí™” ì¢…ë£Œë¡œ íŒë‹¨
           
           // ê¸‰ê²©í•œ ì˜¤ë””ì˜¤ ë ˆë²¨ í•˜ë½ ê°ì§€
           const isRapidDrop = detectRapidAudioLevelDrop(finalLevel);
          
          // ë°œí™” ì‹œì‘ ê°ì§€ (ì˜¤ë””ì˜¤ ë ˆë²¨ì´ ì„ê³„ê°’ì„ ë„˜ì„ ë•Œ)
          if (finalLevel > SPEECH_THRESHOLD && !isAudioLevelSpeakingRef.current) {
            isAudioLevelSpeakingRef.current = true;
            audioLevelSpeechStartRef.current = Date.now();
            silenceStartTimeRef.current = null; // ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
            rapidDropDetectedRef.current = false; // ê¸‰ê²©í•œ í•˜ë½ í”Œë˜ê·¸ ë¦¬ì…‹

          }
          
          // ë°œí™” ì¢…ë£Œ ê°ì§€ (ê¸‰ê²©í•œ í•˜ë½ ë˜ëŠ” ì¼ì • ì‹œê°„ ì¡°ìš©í•¨)
          if ((isRapidDrop || finalLevel < SILENCE_THRESHOLD) && isAudioLevelSpeakingRef.current) {
            // ê¸‰ê²©í•œ í•˜ë½ì´ ê°ì§€ë˜ë©´ ì¦‰ì‹œ ë°œí™” ì¢…ë£Œ
            if (isRapidDrop) {
              isAudioLevelSpeakingRef.current = false;
              audioLevelSpeechEndRef.current = Date.now();
              
              // ì‹¤ì œ ë°œí™” ì§€ì†ì‹œê°„ ê³„ì‚°
              const actualSpeechDuration = audioLevelSpeechStartRef.current && audioLevelSpeechEndRef.current 
                ? (audioLevelSpeechEndRef.current - audioLevelSpeechStartRef.current) / 1000 
                : 0;
              
                             // ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±° (ì‹¤ì œ ë°œí™” ì‹œê°„ì´ 0.5ì´ˆ ì´ìƒì¼ ë•Œë§Œ)
               if (speechBufferRef.current.trim() && actualSpeechDuration > 0.5) {
                 processSpeechSegment();
               } else if (speechBufferRef.current.trim()) {
                 speechBufferRef.current = ""; // ë²„í¼ ì´ˆê¸°í™”
               }
               
               // ìƒíƒœ ë¦¬ì…‹
               silenceStartTimeRef.current = null;
               audioLevelSpeechStartRef.current = null;
               audioLevelSpeechEndRef.current = null;
               rapidDropDetectedRef.current = false;
             } else {
               // ì¼ë°˜ì ì¸ ì¡°ìš©í•¨ ê°ì§€ (ê¸°ì¡´ ë¡œì§)
               if (!silenceStartTimeRef.current) {
                 silenceStartTimeRef.current = Date.now();
               }
               
               const silenceDuration = Date.now() - silenceStartTimeRef.current;
               if (silenceDuration > SILENCE_DURATION) {
                 isAudioLevelSpeakingRef.current = false;
                 audioLevelSpeechEndRef.current = Date.now();
                 
                 const actualSpeechDuration = audioLevelSpeechStartRef.current && audioLevelSpeechEndRef.current 
                   ? (audioLevelSpeechEndRef.current - audioLevelSpeechStartRef.current) / 1000 
                   : 0;
                 
                 // ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±° (ì‹¤ì œ ë°œí™” ì‹œê°„ì´ 0.5ì´ˆ ì´ìƒì¼ ë•Œë§Œ)
                 if (speechBufferRef.current.trim() && actualSpeechDuration > 0.5) {
                   processSpeechSegment();
                 } else if (speechBufferRef.current.trim()) {
                   speechBufferRef.current = ""; // ë²„í¼ ì´ˆê¸°í™”
                 }
                 
                 // íƒ€ì´ë¨¸ ë¦¬ì…‹
                 silenceStartTimeRef.current = null;
                 audioLevelSpeechStartRef.current = null;
                 audioLevelSpeechEndRef.current = null;
               }
             }
           } else if (finalLevel >= SILENCE_THRESHOLD && isAudioLevelSpeakingRef.current) {
             // ë‹¤ì‹œ ì†Œë¦¬ê°€ ë‚˜ë©´ ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
             silenceStartTimeRef.current = null;
           }
           
           // ê¸°ì¡´ ìŒì„± ê°ì§€ ë¡œì§ë„ ìœ ì§€ (UI í‘œì‹œìš©)
           if (finalLevel > 5 && !stateRef.current.isSpeaking) {
             setIsSpeaking(true);
             speechStartTimeRef.current = Date.now();
           } else if (finalLevel < 2 && stateRef.current.isSpeaking) {
             setIsSpeaking(false);
             speechEndTimeRef.current = Date.now();
           }
           
           requestAnimationFrame(() => checkAudioLevel());
         };

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        setIsListening(true); // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ trueë¡œ ì„¤ì •
        
        // ê°•ì œë¡œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹œì‘
        setTimeout(() => {
          checkAudioLevel();
        }, 100);
        
        // ìŒì„± ì¸ì‹ ì„¤ì • ë° ì‹œì‘
        setupSpeechRecognition();
        
        // ì´ˆê¸°í™” ì™„ë£Œ
        setIsInitialized(true);
        console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ - ìŒì„± ì¸ì‹ ìƒíƒœ:', {
          isListening: true,
          isInitialized: true,
          isFocusSessionRunning,
          isFocusSessionPaused
        });
      }
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      setError(error instanceof Error ? error.message : 'ì´ˆê¸°í™” ì‹¤íŒ¨');
    } finally {
      setIsInitializing(false);
    }
  }, [isInitialized, isInitializing]);

  // Speech Recognition ì„¤ì • - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
  const setupSpeechRecognition = useCallback(() => {
    if (!SpeechRecognition) {
      console.warn("Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.");
      return;
    }

    // ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆë‹¤ë©´ ì •ë¦¬
    if (recognitionRef.current) {
      try {
        // ìƒíƒœ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨
        if (recognitionRef.current.state === 'active' || recognitionRef.current.state === 'starting') {
          recognitionRef.current.stop();
          console.log('ğŸ¤ ê¸°ì¡´ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ë¨');
        }
      } catch (error) {
        console.log('ğŸ¤ ê¸°ì¡´ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì¤‘ ì˜¤ë¥˜:', error);
      }
    }

    const recognition = new SpeechRecognition();
    recognitionRef.current = recognition;

    // ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR';
    recognition.maxAlternatives = 1;

    let finalTranscript = '';
    let interimTranscript = '';

    recognition.onstart = () => { 
      setIsSpeechRecognitionActive(true);
      console.log('ğŸ¤ Speech Recognition ì‹œì‘ë¨');
    };

    recognition.onresult = (event: any) => {
      console.log('ğŸ¤ Speech Recognition onresult ì´ë²¤íŠ¸ ë°œìƒ:', {
        resultIndex: event.resultIndex,
        resultsLength: event.results.length,
        hasResults: !!event.results
      });

      let interimTranscript = '';
      let finalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
          // ì²« ë²ˆì§¸ finalTranscript ìˆ˜ì‹  ì‹œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
          if (speechStartTimeRef.current === null) {
            speechStartTimeRef.current = Date.now();
          }
        } else {
          interimTranscript += transcript;
        }
      }

      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë””ë°”ìš´ìŠ¤ ì ìš©)
      setLiveTranscript(interimTranscript);
      
      if (finalTranscript) {
        speechBufferRef.current += finalTranscript;
      }
    };

    recognition.onend = () => {
      setIsSpeechRecognitionActive(false);
      speechEndTimeRef.current = Date.now();
      
      // ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ì—ì„œë§Œ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½)
      // ì—¬ê¸°ì„œëŠ” ì²˜ë¦¬í•˜ì§€ ì•Šê³ , ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ì—ì„œë§Œ processSpeechSegment í˜¸ì¶œ
      
      // ìë™ ì¬ì‹œì‘ (ì¤‘ì•™ í•¨ìˆ˜ ì‚¬ìš©)
      const restartSuccess = restartSpeechRecognition();
      if (!restartSuccess) {
        // ì¬ì‹œì‘ ì‹¤íŒ¨ ì‹œ 1ì´ˆ í›„ ì¬ì‹œë„
        setTimeout(() => {
          restartSpeechRecognition();
        }, 1000);
      }
    };

    recognition.onerror = (event: any) => {
      // abortedëŠ” ì •ìƒì ì¸ ì¢…ë£Œì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
      if (event.error === 'aborted') {
        return;
      }

      // no-speechëŠ” ì •ìƒì ì¸ ìƒí™©
      if (event.error === 'no-speech') {
        return;
      }

      // ë‹¤ë¥¸ ì˜¤ë¥˜ë“¤ì€ ë¡œê·¸ ì¶œë ¥
      console.error("[STT] recognition error:", event.error);
    };

          // ì•ˆì „í•˜ê²Œ ì‹œì‘ (ìƒíƒœ í™•ì¸ í›„)
      try {
        recognition.start();
        setIsSpeechRecognitionActive(true);
      } catch (error) {
        console.warn('ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', error);
        setIsSpeechRecognitionActive(false);
      }
  }, []);

  // ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ - ì„±ëŠ¥ ìµœì í™” ì ìš©
  const processSpeechSegment = useCallback(async () => {
    console.log('ğŸ¤ processSpeechSegment í˜¸ì¶œë¨ - í˜„ì¬ ìƒíƒœ:', {
      isAnalyzing,
      bufferText: speechBufferRef.current,
      bufferLength: speechBufferRef.current.length
    });
    
    if (isAnalyzing) {
      console.log('ğŸ¤ ì´ë¯¸ ë¶„ì„ ì¤‘ - ê±´ë„ˆëœ€');
      return;
    }
    setIsAnalyzing(true);
    const text = speechBufferRef.current.trim();
    if (!text) { 
      console.log('ğŸ¤ ë²„í¼ê°€ ë¹„ì–´ìˆìŒ - ë¶„ì„ ê±´ë„ˆëœ€');
      setIsAnalyzing(false); 
      return; 
    }

    const startTime = performance.now();
    
    try {
      // ì‹¤ì œ ë°œí™” ì§€ì†ì‹œê°„ ê³„ì‚° (ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜)
      const actualSpeechDuration = audioLevelSpeechStartRef.current && audioLevelSpeechEndRef.current 
        ? (audioLevelSpeechEndRef.current - audioLevelSpeechStartRef.current) / 1000 
        : 0;
      
      // ê¸°ì¡´ ì§€ì†ì‹œê°„ ê³„ì‚° (ë°±ì—…ìš©)
      const duration = speechStartTimeRef.current && speechEndTimeRef.current 
        ? (speechEndTimeRef.current - speechStartTimeRef.current) / 1000 
        : 0;

      // íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´
      const startTimestamp = speechStartTimeRef.current ? new Date(speechStartTimeRef.current).toLocaleTimeString() : 'ì•Œ ìˆ˜ ì—†ìŒ';
      const endTimestamp = speechEndTimeRef.current ? new Date(speechEndTimeRef.current).toLocaleTimeString() : 'ì•Œ ìˆ˜ ì—†ìŒ';

      // GPT ë°œí™”ë¶„ì„ API í˜¸ì¶œ
      const gptResult = await analyzeSpeechWithGPT(text);

      // ë¬¸ë§¥ ë¶„ì„
      const context = analyzeTextContext(text);
      const contextualWeight = getContextualWeight(context);
      const contextLabel = getContextLabel(context);

      // ìµœì¢… íŒì • (ë¬¸ë§¥ ê°€ì¤‘ì¹˜ ì ìš©)
      const finalJudgment = gptResult.isStudyRelated && contextualWeight > 0.3;

      const processingTime = performance.now() - startTime;

      // êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
      console.log(`
ğŸ¤ ë°œí™” ë¶„ì„ ê²°ê³¼ (${new Date().toLocaleTimeString()}):
â”œâ”€ ì‹œì‘ ì‹œê°„: ${startTimestamp}
â”œâ”€ ì¢…ë£Œ ì‹œê°„: ${endTimestamp}
â”œâ”€ ì‹¤ì œ ë°œí™” ì‹œê°„: ${actualSpeechDuration.toFixed(1)}ì´ˆ
â”œâ”€ ì „ì²´ ë¶„ì„ ì‹œê°„: ${duration.toFixed(1)}ì´ˆ
â”œâ”€ ì›ë¬¸: "${text}"
â”œâ”€ ë¶„ì„ ë°©ë²•: GPT
â”œâ”€ GPT ì‹ ë¢°ë„: ${gptResult.confidence.toFixed(3)}
â”œâ”€ ê³µë¶€ ê´€ë ¨: ${gptResult.isStudyRelated ? 'âœ…' : 'âŒ'}
â”œâ”€ ë¬¸ë§¥: ${contextLabel} (ê°€ì¤‘ì¹˜: ${contextualWeight.toFixed(2)})
â”œâ”€ ìµœì¢… íŒì •: ${finalJudgment ? 'ê³µë¶€ ê´€ë ¨ ë°œí™”' : 'ì¡ë‹´'}
â””â”€ ì²˜ë¦¬ ì‹œê°„: ${processingTime.toFixed(1)}ms
      `);

      // ë²„í¼ ì´ˆê¸°í™” (ë¶„ì„ ì™„ë£Œ í›„ ì¦‰ì‹œ)
      speechBufferRef.current = "";
      speechStartTimeRef.current = null;
      speechEndTimeRef.current = null;
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ê°ì§€ ìƒíƒœë„ ì´ˆê¸°í™”
      isAudioLevelSpeakingRef.current = false;
      audioLevelSpeechStartRef.current = null;
      audioLevelSpeechEndRef.current = null;
      
      // ë°œí™” ë¶„ì„ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
      setLastSpeechAnalysis({
        isStudyRelated: gptResult.isStudyRelated,
        confidence: gptResult.confidence,
        reasoning: gptResult.reasoning,
        timestamp: Date.now(),
        shouldOverrideFocus: finalJudgment // ë°œí™” ë¶„ì„ ê²°ê³¼ë¥¼ ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ë¡œì§ì— ì‚¬ìš©
      });

      // ê³µë¶€ ê´€ë ¨ ë°œí™”ê°€ ê°ì§€ëœ ê²½ìš° ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ì‹¤í–‰
      if (finalJudgment) {
        console.log('ğŸ¤ ê³µë¶€ ê´€ë ¨ ë°œí™” ê°ì§€ - ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ì‹¤í–‰');
        await overrideFocusScoresDuringSpeech();
      }

      console.log('ğŸ¤ ë°œí™” ë¶„ì„ ì™„ë£Œ - ë²„í¼ ë° ìƒíƒœ ì´ˆê¸°í™”ë¨');

    } catch (error) {
      console.error('ë°œí™” ë¶„ì„ ì‹¤íŒ¨:', error);
      speechBufferRef.current = "";
    } finally {
      setIsAnalyzing(false);
      // ë°œí™” ë¶„ì„ ì™„ë£Œ í›„ ìŒì„± ì¸ì‹ ìƒíƒœ í™•ì¸ ë° ì¬ì‹œì‘ ë³´ì¥
      console.log('ğŸ¤ ë°œí™” ë¶„ì„ ì™„ë£Œ - ìŒì„± ì¸ì‹ ìƒíƒœ:', {
        hasRecognition: !!recognitionRef.current,
        recognitionState: recognitionRef.current?.state,
        isFocusSessionRunning,
        isFocusSessionPaused,
        isListening: stateRef.current.isListening
      });
      
      // ë°œí™” ë¶„ì„ ì™„ë£Œ í›„ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ë³´ì¥
      if (isFocusSessionRunning && !isFocusSessionPaused) {
        setTimeout(() => {
          console.log('ğŸ¤ ë°œí™” ë¶„ì„ ì™„ë£Œ í›„ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ë³´ì¥');
          restartSpeechRecognition();
        }, 500);
      }
    }
  }, [isAnalyzing, restartSpeechRecognition]);

  // GPT ë°œí™”ë¶„ì„ API í˜¸ì¶œ í•¨ìˆ˜
  const analyzeSpeechWithGPT = async (transcript: string): Promise<{ isStudyRelated: boolean; confidence: number; reasoning: string }> => {
    try {
      const response = await fetch('/api/classify-speech', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ transcript }),
      });

      if (!response.ok) {
        throw new Error(`GPT API í˜¸ì¶œ ì‹¤íŒ¨: ${response.status}`);
      }

      const result = await response.json();
      return {
        isStudyRelated: result.isStudyRelated,
        confidence: result.confidence || 0.8,
        reasoning: result.reasoning || 'GPT ë¶„ì„ ê²°ê³¼'
      };
    } catch (error) {
      console.error('GPT ë°œí™”ë¶„ì„ ì˜¤ë¥˜:', error);
      // GPT API ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ìœ¼ë¡œ í´ë°±
      return {
        isStudyRelated: analyzeStudyRelatedByKeywords(transcript),
        confidence: 0.5,
        reasoning: 'í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ (GPT API ì‹¤íŒ¨)'
      };
    }
  };

  // í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const analyzeTextContext = (text: string):
    | 'discussion'
    | 'class'
    | 'presentation'
    | 'question'
    | 'frustration'
    | 'statement'
    | 'unknown' => {
    const lower = text.toLowerCase();

    // í† ë¡  ìƒí™©
    if (
      lower.includes("í† ë¡ ") ||
      lower.includes("ì°¬ì„±") ||
      lower.includes("ë°˜ëŒ€") ||
      lower.includes("ì˜ê²¬") ||
      lower.includes("ìƒê°ì€")
    ) {
      return 'discussion';
    }
    // ìˆ˜ì—… ìƒí™©
    if (
      lower.includes("ì„ ìƒë‹˜") ||
      lower.includes("êµê³¼ì„œ") ||
      lower.includes("ë¬¸ì œ") ||
      lower.includes("ì„¤ëª…") ||
      lower.includes("ìˆ˜ì—…")
    ) {
      return 'class';
    }
    // ë°œí‘œ ìƒí™©
    if (
      lower.includes("ë°œí‘œ") ||
      lower.includes("ê²°ë¡ ") ||
      lower.includes("ìš”ì•½") ||
      lower.includes("ì •ë¦¬")
    ) {
      return 'presentation';
    }
    // ì§ˆë¬¸
    if (
      lower.includes("ì–´ë–»ê²Œ") ||
      lower.includes("ì™œ") ||
      lower.includes("ë­ì•¼") ||
      lower.includes("ë¬´ì—‡") ||
      lower.includes("ê¶ê¸ˆ") ||
      text.endsWith("?")
    ) {
      return 'question';
    }
    // ì¢Œì ˆ
    if (
      lower.includes("ì•ˆë¼") ||
      lower.includes("ì§œì¦ë‚˜") ||
      lower.includes("ì•„ì˜¤")
    ) {
      return 'frustration';
    }
    // ì§„ìˆ 
    if (text.length > 5) {
      return 'statement';
    }
    return 'unknown';
  };

  // ë¬¸ë§¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextualWeight = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): number => {
    switch (context) {
      case 'discussion': return 0.95; // í† ë¡ : ì§‘ì¤‘ë„ ì•½ê°„ ë†’ìŒ
      case 'class': return 1.0; // ìˆ˜ì—…: ì§‘ì¤‘ë„ ìµœìƒ
      case 'presentation': return 0.9; // ë°œí‘œ: ì§‘ì¤‘ë„ ë†’ìŒ
      case 'question': return 0.8; // ì§ˆë¬¸: ë§‰í˜”ì„ ê°€ëŠ¥ì„±, ì§‘ì¤‘ë„ ì•½ê°„ ê°ì†Œ
      case 'frustration': return 0.5; // ì¢Œì ˆ: ëª…í™•í•œ ì§‘ì¤‘ë ¥ ì €í•˜ ì‹ í˜¸, ì§‘ì¤‘ë„ í¬ê²Œ ê°ì†Œ
      case 'statement': return 1.0; // í˜¼ì£ë§/ë‚´ìš© ì½ê¸°: í•™ìŠµ í™œë™ì˜ ì¼ë¶€ë¡œ íŒë‹¨, ê°€ì¤‘ì¹˜ ì—†ìŒ
      default: return 0.9; // ë¶ˆëª…í™•: ì§§ì€ ì¤‘ì–¼ê±°ë¦¼ ë“±, ì•½ê°„ì˜ ë°©í•´ë¡œ ê°„ì£¼
    }
  };

  // ë¬¸ë§¥ ìœ í˜•ì„ í•œê¸€ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextLabel = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): string => {
    switch (context) {
      case 'discussion': return 'í† ë¡ ';
      case 'class': return 'ìˆ˜ì—…';
      case 'presentation': return 'ë°œí‘œ';
      case 'question': return 'ì§ˆë¬¸';
      case 'frustration': return 'ì¢Œì ˆ';
      case 'statement': return 'ì§„ìˆ ';
      default: return 'ë¶ˆëª…í™•';
    }
  };

  // ë°œí™” ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ìƒíƒœ
  const [lastSpeechAnalysis, setLastSpeechAnalysis] = useState<{
    isStudyRelated: boolean;
    confidence: number;
    reasoning: string;
    timestamp: number;
    shouldOverrideFocus: boolean;
  } | null>(null);

  // ë°œí™” ì‹œì‘ ì‹œì ì˜ ì§‘ì¤‘ë„ ì €ì¥
  const [speechStartFocusScore, setSpeechStartFocusScore] = useState<number | null>(null);
  const [speechStartTime, setSpeechStartTime] = useState<number | null>(null);

  // ë°œí™” ì‹œì  ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ë¡œì§
  const shouldOverrideFocusScore = useCallback((currentFocusScore: number): boolean => {
    if (!lastSpeechAnalysis || !speechStartFocusScore || !speechStartTime) return false;
    
    // ìµœê·¼ 5ì´ˆ ë‚´ ë°œí™” ë¶„ì„ ê²°ê³¼ê°€ ìˆê³ , í•™ìŠµ ê´€ë ¨ ë°œí™”ì¸ ê²½ìš°
    const timeSinceSpeech = Date.now() - lastSpeechAnalysis.timestamp;
    const isRecentSpeech = timeSinceSpeech < 5000; // 5ì´ˆ ë‚´
    
    return isRecentSpeech && 
           lastSpeechAnalysis.isStudyRelated && 
           lastSpeechAnalysis.confidence > 0.7;
  }, [lastSpeechAnalysis, speechStartFocusScore, speechStartTime]);

  // ì§‘ì¤‘ë„ ì ìˆ˜ ê³„ì‚° ì‹œ ë°œí™” ë¶„ì„ ê²°ê³¼ ë°˜ì˜
  const calculateAdjustedFocusScore = useCallback((baseFocusScore: number): number => {
    if (shouldOverrideFocusScore(baseFocusScore)) {
      // í•™ìŠµ ê´€ë ¨ ë°œí™”ê°€ ê°ì§€ëœ ê²½ìš°, ë°œí™” ì‹œì‘ ì§ì „ ì§‘ì¤‘ë„ë¡œ ë®ì–´ì“°ê¸°
      console.log(`ğŸ¤ ë°œí™” ë¶„ì„ ê¸°ë°˜ ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸°: ${baseFocusScore} â†’ ${speechStartFocusScore}`);
      return speechStartFocusScore!;
    }
    return baseFocusScore;
  }, [shouldOverrideFocusScore, speechStartFocusScore]);

  // ë°œí™” ì‹œì‘ ì‹œì  ì§‘ì¤‘ë„ ì €ì¥
  const saveSpeechStartFocusScore = useCallback(() => {
    if (focusScore !== undefined) {
      setSpeechStartFocusScore(focusScore);
      setSpeechStartTime(Date.now());
      console.log(`ğŸ¤ ë°œí™” ì‹œì‘ ì‹œì  ì§‘ì¤‘ë„ ì €ì¥: ${focusScore}ì `);
    }
  }, [focusScore]);

  // ë°œí™” ë¶„ì„ í›„ ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ë° DB ì €ì¥
  const overrideFocusScoresDuringSpeech = useCallback(async () => {
    if (!lastSpeechAnalysis?.isStudyRelated || !speechStartFocusScore || !speechStartTime) {
      return;
    }

    try {
      // ë°œí™” ì‹œì‘ ì‹œì ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ì§‘ì¤‘ë„ ê°’ì„ ë°œí™” ì‹œì‘ ì§ì „ ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸°
      const speechDuration = Date.now() - speechStartTime;
      const overrideData = {
        startTime: speechStartTime,
        endTime: Date.now(),
        duration: speechDuration,
        originalFocusScore: focusScore,
        overrideFocusScore: speechStartFocusScore,
        reason: `ê³µë¶€ ê´€ë ¨ ë°œí™” ê°ì§€: ${lastSpeechAnalysis.reasoning}`,
        confidence: lastSpeechAnalysis.confidence
      };

      console.log('ğŸ¤ ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ë°ì´í„°:', overrideData);

      // DBì— ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ì •ë³´ ì €ì¥ (API í˜¸ì¶œ)
      const response = await fetch('/api/focus-session/override-focus-scores', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(overrideData),
      });

      if (response.ok) {
        console.log('âœ… ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° DB ì €ì¥ ì™„ë£Œ');
        // ì§‘ì¤‘ë„ ì ìˆ˜ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
        updateFocusScore(speechStartFocusScore);
      } else {
        console.error('âŒ ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° DB ì €ì¥ ì‹¤íŒ¨:', response.status);
      }
    } catch (error) {
      console.error('âŒ ì§‘ì¤‘ë„ ë®ì–´ì“°ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
    }
  }, [lastSpeechAnalysis, speechStartFocusScore, speechStartTime, focusScore, updateFocusScore]);

  useEffect(() => {
    // Speech Recognition ì„¤ì •ë§Œ ë¨¼ì € ìˆ˜í–‰ (í•œ ë²ˆë§Œ)
    if (!recognitionRef.current) {
      setupSpeechRecognition();
    }

    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.abort();
        } catch (error) {
          console.log('ğŸ¤ Speech Recognition ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);
        }
      }
      if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current);
      // AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ (ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ ìœ ì§€)
      // Web Worker ì œê±°ë¨ (GPT ê¸°ë°˜ ë°œí™”ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´)
    }
  }, [setupSpeechRecognition])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
  useEffect(() => {
    const initializeComponents = async () => {
      // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
      if (!isInitialized && !isInitializing) {
        initializeAudioPipeline()
      }
    };
    
    initializeComponents();
  }, [isInitialized, isInitializing, initializeAudioPipeline])



  return (
    <div className="p-4 bg-slate-100 rounded-lg">
      <h3 className="font-bold">í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸</h3>
      
      {isInitializing && (
        <p className="text-blue-600 mb-4">ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...</p>
      )}
      
      {error && (
        <p className="text-red-600 mb-4">ì˜¤ë¥˜: {error}</p>
      )}
      



      
      {isInitialized && (
        <div className="space-y-3 p-3 bg-white rounded border">
          <h4 className="font-semibold text-sm">ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ìƒíƒœ</h4>
          
          {/* ìŒì„± ê°ì§€ ìƒíƒœ */}
          <div className="flex items-center gap-2">
            <div className={`w-3 h-3 rounded-full ${isSpeaking ? 'bg-red-500 animate-pulse' : 'bg-gray-300'}`}></div>
            <span className="text-sm">
              {isSpeaking ? "ğŸ¤ ë§í•˜ëŠ” ì¤‘..." : "ğŸ”‡ ì¡°ìš©í•¨"}
            </span>
          </div>
          
          {/* ì˜¤ë””ì˜¤ ë ˆë²¨ ê¸°ë°˜ ë°œí™” ìƒíƒœ */}
          <div className="flex items-center gap-2">
            <div className={`w-3 h-3 rounded-full ${isAudioLevelSpeakingRef.current ? 'bg-green-500 animate-pulse' : 'bg-gray-300'}`}></div>
            <span className="text-sm">
              {isAudioLevelSpeakingRef.current ? "ğŸ”Š ì‹¤ì œ ë°œí™” ê°ì§€" : "ğŸ”‡ ë°œí™” ì—†ìŒ"}
            </span>
          </div>
          
          {/* ì˜¤ë””ì˜¤ ë ˆë²¨ í‘œì‹œ */}
          <div className="space-y-1">
            <div className="flex justify-between text-xs">
              <span>ì˜¤ë””ì˜¤ ë ˆë²¨</span>
              <span>{currentAudioLevel.toFixed(1)}</span>
            </div>
            <div className="w-full bg-gray-200 rounded-full h-2">
              <div 
                className={`h-2 rounded-full transition-all duration-200 ${
                  currentAudioLevel > 25 ? 'bg-red-500' : 
                  currentAudioLevel > 15 ? 'bg-yellow-500' : 'bg-gray-400'
                }`}
                style={{ width: `${Math.min(currentAudioLevel * 2, 100)}%` }}
              ></div>
            </div>
          </div>
          
          {/* ì‹œìŠ¤í…œ ìƒíƒœ */}
          <div className="grid grid-cols-2 gap-2 text-xs">
            <div>
              <span className="text-gray-600">ìŒì„± ì¸ì‹:</span>
              <span className={`ml-1 ${isSpeechRecognitionActive ? 'text-green-600' : 'text-gray-400'}`}>
                {isSpeechRecognitionActive ? 'í™œì„±' : 'ëŒ€ê¸°'}
              </span>
            </div>
            <div>
              <span className="text-gray-600">ì˜¤ë””ì˜¤ ëª¨ë‹ˆí„°ë§:</span>
              <span className={`ml-1 ${isListening ? 'text-green-600' : 'text-gray-400'}`}>
                {isListening ? 'í™œì„±' : 'ë¹„í™œì„±'}
              </span>
            </div>
          </div>
          
          {/* ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ */}
          {liveTranscript && (
            <div className="p-2 bg-blue-50 rounded text-xs">
              <span className="text-gray-600">ì¸ì‹ëœ í…ìŠ¤íŠ¸:</span>
              <p className="mt-1 text-gray-800">{liveTranscript}</p>
            </div>
          )}
        </div>
      )}
    </div>
  )
}
