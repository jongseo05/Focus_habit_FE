// onnxruntime-web import
import * as ort from "onnxruntime-web";

// KOElectra ONNX ëª¨ë¸ ì„¸ì…˜
let koelectraSession: ort.InferenceSession | null = null;

// KOElectra ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
async function loadKOElectraModel() {
  if (!koelectraSession) {
    koelectraSession = await ort.InferenceSession.create("/models/koelectra/koelectra.onnx");
  }
}

// KOElectra ONNX ì¶”ë¡  í•¨ìˆ˜
async function isStudyRelatedONNX(text: string): Promise<boolean> {
  await loadKOElectraModel();
  if (!koelectraSession) {
    console.error("KOElectra ONNX ëª¨ë¸ ì„¸ì…˜ì´ nullì…ë‹ˆë‹¤.");
    return false;
  }
  const inputArr = await koelectraPreprocess(text); // [seq_length]
  const inputTensor = new ort.Tensor("int64", inputArr, [1, inputArr.length]);
  const feeds: Record<string, ort.Tensor> = { input_ids: inputTensor };
  const results = await koelectraSession.run(feeds);
  const outputKey = Object.keys(results)[0];
  const logits = results[outputKey]?.data;
  if (!Array.isArray(logits) || logits.length === 0 || typeof logits[0] !== "number") {
    console.error("KOElectra ONNX ëª¨ë¸ ì¶œë ¥ê°’ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.", logits);
    return false;
  }
  return logits[0] > 0.5;
}
// KOElectra vocab/tokenizer ë¡œë“œ ë° í† í¬ë‚˜ì´ì € êµ¬í˜„
let koelectraVocab: { [key: string]: number } = {};
let koelectraTokenizer: any = null;
const koelectraMaxLength = 32; // KOElectra ëª¨ë¸ ì…ë ¥ ê¸¸ì´(ì˜ˆì‹œ)

// vocab.txt íŒŒì¼ì„ fetchí•˜ì—¬ íŒŒì‹±
async function loadKOElectraVocab() {
  if (Object.keys(koelectraVocab).length > 0) return;
  const res = await fetch("/models/koelectra/vocab.txt");
  const text = await res.text();
  text.split("\n").forEach((line, idx) => {
    const token = line.trim();
    if (token) koelectraVocab[token] = idx;
  });
}

// tokenizer.jsonì„ fetchí•˜ì—¬ íŒŒì‹± (WordPiece ë“±)
async function loadKOElectraTokenizer() {
  if (koelectraTokenizer) return;
  const res = await fetch("/models/koelectra/tokenizer.json");
  koelectraTokenizer = await res.json();
}

// í…ìŠ¤íŠ¸ë¥¼ KOElectra ë°©ì‹ìœ¼ë¡œ í† í°í™” ë° ì¸ë±ì‹±
async function koelectraPreprocess(text: string): Promise<number[]> {
  await loadKOElectraVocab();
  // tokenizer.json êµ¬ì¡°ì— ë”°ë¼ ì‹¤ì œ WordPiece í† í¬ë‚˜ì´ì € êµ¬í˜„ í•„ìš”
  // ì—¬ê¸°ì„œëŠ” ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ í† í°í™” í›„ vocab ì¸ë±ì‹±(ê°„ë‹¨ ë²„ì „)
  const tokens = text.trim().split(/\s+/);
  const indices = tokens.map(t => koelectraVocab[t] ?? koelectraVocab["[UNK]"] ?? 0);
  while (indices.length < koelectraMaxLength) indices.push(0);
  return indices.slice(0, koelectraMaxLength);
}
// ML ëª¨ë¸ Ref (TensorFlow.js ë“±)
let studyModel: any = null;

// ML ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (ë¹„ë™ê¸°)
async function loadStudyModel() {
  if (!studyModel && (window as any).tf) {
    studyModel = await (window as any).tf.loadLayersModel("/model/model.json");
  }
}

// ê³µë¶€ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜ (ML ê¸°ë°˜)
async function isStudyRelated(text: string): Promise<boolean> {
  // KOElectra ONNX ê¸°ë°˜ ì¶”ë¡  í•¨ìˆ˜ ì‚¬ìš©
  try {
    return await isStudyRelatedONNX(text);
  } catch (err) {
    console.error("KOElectra ONNX ì¶”ë¡  ì˜¤ë¥˜:", err);
    return false;
  }
}
"use client"

import { useEffect, useRef, useState } from "react"

// íŒ¨í‚· ë¹Œë”: ì¶”ë¡  ê²°ê³¼ë¥¼ JSON íŒ¨í‚·ìœ¼ë¡œ ë³€í™˜
function buildPacket({ mel, scene_tag, noise_db }: { mel: number[]; scene_tag: string; noise_db: number }) {
  return {
    timestamp: Date.now(),
    scene_tag,
    noise_db,
    mel,
  }
}


// SpeechRecognition íƒ€ì… ì„ ì–¸ (ê°„ë‹¨ ë²„ì „)
type SpeechRecognitionType = typeof window extends { webkitSpeechRecognition: infer T } ? T : any;
const SpeechRecognition: any =
  typeof window !== "undefined" ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition : null;

export default function HybridAudioPipeline() {
  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ Ref
  const audioContextRef = useRef<AudioContext | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null)
  const workerRef = useRef<Worker | null>(null)

  // --- ìƒíƒœ ê´€ë¦¬ ---
  const recognitionRef = useRef<SpeechRecognitionType | null>(null)
  const speechTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false) // ìŒì„± êµ¬ê°„ ê°ì§€ ìƒíƒœ
  const [isListening, setIsListening] = useState(false)
  const [liveTranscript, setLiveTranscript] = useState("") // ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë  í…ìŠ¤íŠ¸
  const speechBufferRef = useRef<string>("") // í•œ ë°œí™”ê°€ ëë‚  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ëŠ” ë²„í¼
  const featureBufferRef = useRef<any[]>([]) // í•œ ë°œí™” ë™ì•ˆì˜ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ëª¨ìœ¼ëŠ” ë²„í¼

  // useEffect í´ë¡œì €ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ Ref
  const stateRef = useRef({ isSpeaking, isListening });
  stateRef.current = { isSpeaking, isListening };

  useEffect(() => {
    let isActive = true

    // 1. ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì„¤ì • (ê¸°ì¡´ ë¡œì§)
    async function setupAudioPipeline() {
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 })
      audioContextRef.current = audioContext

      await audioContext.audioWorklet.addModule("/audio/stft-mel-processor.js")
      const workletNode = new AudioWorkletNode(audioContext, "stft-mel-processor")
      workletNodeRef.current = workletNode

      const worker = new Worker("/audio/ml-inference-worker.js")
      workerRef.current = worker

      // Workerì—ì„œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ + scene_tag ìˆ˜ì‹ 
      worker.onmessage = (e) => {
        if (!isActive) return

        const { mel, scene_tag, noise_db } = e.data
        const packet = buildPacket({ mel, scene_tag, noise_db })

        if (scene_tag === "speech") {
          if (!stateRef.current.isSpeaking) {
            setIsSpeaking(true)
            speechBufferRef.current = ""
            featureBufferRef.current = []
            console.log("[AUDIO] --- ğŸ¤ ë°œí™” ì‹œì‘ ---")
          }
          featureBufferRef.current.push(packet)
          // ë””ë²„ê¹…: ìŒì„± scene_tag ë°œìƒ
          console.log("[AUDIO] scene_tag=speech, isListening:", stateRef.current.isListening)
          if (recognitionRef.current && !stateRef.current.isListening) {
            try {
              console.log("[STT] recognition.start() í˜¸ì¶œ ì‹œë„")
              recognitionRef.current.start()
              console.log("[STT] recognition.start() í˜¸ì¶œ ì™„ë£Œ")
            } catch (err) {
              console.error("[STT] recognition.start() ì—ëŸ¬:", err)
            }
          }
          if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
          speechTimeoutRef.current = setTimeout(() => {
            if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
          }, 2000)
        }
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { sampleRate: 16000, channelCount: 1 },
      })
      const source = audioContext.createMediaStreamSource(stream)
      source.connect(workletNode)
    }

    // 2. ìŒì„± ì¸ì‹(STT) ì„¤ì •
    function setupSpeechRecognition() {
      if (!SpeechRecognition) {
        console.error("[STT] ì´ ë¸Œë¼ìš°ì €ëŠ” Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
      }
      const recognition = new SpeechRecognition()
      recognition.continuous = true
      recognition.interimResults = true
      recognition.lang = "ko-KR"

      recognition.onresult = (event: any) => {
        console.log("[STT] onresult ì´ë²¤íŠ¸ ë°œìƒ");
        let finalTranscript = "";
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          setLiveTranscript(event.results[i][0].transcript);
          console.log("[STT] ì‹¤ì‹œê°„ ì¸ì‹ í…ìŠ¤íŠ¸:", event.results[i][0].transcript);
          if (event.results[i].isFinal) {
            finalTranscript += event.results[i][0].transcript;
          }
        }
        if (finalTranscript) {
          speechBufferRef.current += finalTranscript.trim() + " ";
          console.log("[STT] ìµœì¢… ì¸ì‹ í…ìŠ¤íŠ¸ ëˆ„ì :", speechBufferRef.current);
        }
      };

      recognition.onstart = () => {
        setIsListening(true);
        console.log("[STT] recognition ì‹œì‘ë¨");
      };
      recognition.onend = () => {
        setIsListening(false);
        console.log("[STT] recognition ì¢…ë£Œë¨");
        if (stateRef.current.isSpeaking) {
          console.log("[STT] --- ğŸ¤ ë°œí™” ì¢…ë£Œ ---");
          processSpeechSegment();
          setIsSpeaking(false);
        }
      };
      recognition.onerror = (event: any) => {
        console.error("[STT] recognition error:", event.error);
      };

      recognitionRef.current = recognition;
      console.log("[STT] recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í• ë‹¹ ì™„ë£Œ:", recognitionRef.current);
    }

    // 3. ìˆ˜ì§‘ëœ ë°œí™” êµ¬ê°„ ë°ì´í„° ì¢…í•© ë¶„ì„
    async function processSpeechSegment() {
      const fullText = speechBufferRef.current.trim();
      if (!fullText) return;

      console.log("======[ ìµœì¢… ë¶„ì„ ]======");
      console.log("ì¸ì‹ëœ ë¬¸ì¥:", fullText);

      // ê³µë¶€ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨ (ML ê¸°ë°˜)
      const isStudy = await isStudyRelated(fullText);
      console.log("ê³µë¶€ ê´€ë ¨ ì—¬ë¶€:", isStudy);

      // ì„œë²„ë¡œ ê²°ê³¼ ì „ì†¡
      fetch("/api/send-study-status", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ isStudy }),
      })
        .then(res => res.ok ? console.log("ì„œë²„ ì „ì†¡ ì„±ê³µ") : console.error("ì„œë²„ ì „ì†¡ ì‹¤íŒ¨"))
        .catch(err => console.error("ì„œë²„ í†µì‹  ì˜¤ë¥˜", err));
    }

    // í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ê³ ë„í™”)
    function analyzeTextContext(text: string):
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown' {
      const lower = text.toLowerCase();

      // í† ë¡  ìƒí™©
      if (
        lower.includes("í† ë¡ ") ||
        lower.includes("ì°¬ì„±") ||
        lower.includes("ë°˜ëŒ€") ||
        lower.includes("ì˜ê²¬") ||
        lower.includes("ìƒê°ì€")
      ) {
        return 'discussion';
      }
      // ìˆ˜ì—… ìƒí™©
      if (
        lower.includes("ì„ ìƒë‹˜") ||
        lower.includes("êµê³¼ì„œ") ||
        lower.includes("ë¬¸ì œ") ||
        lower.includes("ì„¤ëª…") ||
        lower.includes("ìˆ˜ì—…")
      ) {
        return 'class';
      }
      // ë°œí‘œ ìƒí™©
      if (
        lower.includes("ë°œí‘œ") ||
        lower.includes("ê²°ë¡ ") ||
        lower.includes("ìš”ì•½") ||
        lower.includes("ì •ë¦¬")
      ) {
        return 'presentation';
      }
      // ì§ˆë¬¸
      if (
        lower.includes("ì–´ë–»ê²Œ") ||
        lower.includes("ì™œ") ||
        lower.includes("ë­ì•¼") ||
        lower.includes("ë¬´ì—‡") ||
        lower.includes("ê¶ê¸ˆ") ||
        text.endsWith("?")
      ) {
        return 'question';
      }
      // ì¢Œì ˆ
      if (
        lower.includes("ì•ˆë¼") ||
        lower.includes("ì§œì¦ë‚˜") ||
        lower.includes("ì•„ì˜¤")
      ) {
        return 'frustration';
      }
      // ì§„ìˆ 
      if (text.length > 5) {
        return 'statement';
      }
      return 'unknown';
    }

    // ë¬¸ë§¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜ (ìƒí™©ë³„ ê°€ì¤‘ì¹˜ ì¶”ê°€)
    function getContextualWeight(
      context:
        | 'discussion'
        | 'class'
        | 'presentation'
        | 'question'
        | 'frustration'
        | 'statement'
        | 'unknown'
    ): number {
      switch (context) {
        case 'discussion': return 0.95; // í† ë¡ : ì§‘ì¤‘ë„ ì•½ê°„ ë†’ìŒ
        case 'class': return 1.0; // ìˆ˜ì—…: ì§‘ì¤‘ë„ ìµœìƒ
        case 'presentation': return 0.9; // ë°œí‘œ: ì§‘ì¤‘ë„ ë†’ìŒ
        case 'question': return 0.8; // ì§ˆë¬¸: ë§‰í˜”ì„ ê°€ëŠ¥ì„±, ì§‘ì¤‘ë„ ì•½ê°„ ê°ì†Œ
        case 'frustration': return 0.5; // ì¢Œì ˆ: ëª…í™•í•œ ì§‘ì¤‘ë ¥ ì €í•˜ ì‹ í˜¸, ì§‘ì¤‘ë„ í¬ê²Œ ê°ì†Œ
        case 'statement': return 1.0; // í˜¼ì£ë§/ë‚´ìš© ì½ê¸°: í•™ìŠµ í™œë™ì˜ ì¼ë¶€ë¡œ íŒë‹¨, ê°€ì¤‘ì¹˜ ì—†ìŒ
        default: return 0.9; // ë¶ˆëª…í™•: ì§§ì€ ì¤‘ì–¼ê±°ë¦¼ ë“±, ì•½ê°„ì˜ ë°©í•´ë¡œ ê°„ì£¼
      }
    }

    setupAudioPipeline()
    setupSpeechRecognition()

    return () => {
      isActive = false
      if (recognitionRef.current) recognitionRef.current.abort()
      if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
      if (audioContextRef.current) audioContextRef.current.close()
      if (workerRef.current) workerRef.current.terminate()
    }
  }, []) // ì˜ì¡´ì„± ë°°ì—´ì„ ë¹„ì›Œ ë§ˆìš´íŠ¸ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ìˆ˜ì •

  return (
    <div className="p-4 bg-slate-100 rounded-lg">
      <h3 className="font-bold">í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸</h3>
      <p><b>ìŒì„± ì¸ì‹ ìƒíƒœ:</b> {isListening ? "ë“£ëŠ” ì¤‘..." : "ëŒ€ê¸° ì¤‘"}</p>
      <p><b>ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸:</b> {liveTranscript}</p>
    </div>
  )
}
