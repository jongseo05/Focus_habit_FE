"use client"

// onnxruntime-web import
import * as ort from "onnxruntime-web";
import { useEffect, useRef, useState } from "react"

// ONNX ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ë“¤
async function loadKOElectraModel() {
  const response = await fetch("/models/koelectra/koelectra.onnx")
  const arrayBuffer = await response.arrayBuffer()
  return new Uint8Array(arrayBuffer)
}

async function isStudyRelatedONNX(text: string): Promise<boolean> {
  try {
    const model = await loadKOElectraModel()
    // ì‹¤ì œ ONNX ì¶”ë¡  ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„
    // í˜„ì¬ëŠ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨
    const studyKeywords = ["ê³µë¶€", "í•™ìŠµ", "ìˆ˜ì—…", "ë¬¸ì œ", "ì±…", "ì½ê¸°", "ì“°ê¸°", "ê³„ì‚°", "ê³µì‹", "ì´ë¡ "]
    return studyKeywords.some(keyword => text.includes(keyword))
  } catch (error) {
    console.error("ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", error)
    return false
  }
}

async function loadKOElectraVocab() {
  const response = await fetch("/models/koelectra/vocab.txt")
  const text = await response.text()
  return text.split("\n").filter(line => line.trim())
}

async function loadKOElectraTokenizer() {
  const response = await fetch("/models/koelectra/tokenizer.json")
  return response.json()
}

async function koelectraPreprocess(text: string): Promise<number[]> {
  try {
    const vocab = await loadKOElectraVocab()
    const tokenizer = await loadKOElectraTokenizer()
    
    // ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì§• (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
    const tokens = text.split(" ").map(word => {
      const index = vocab.indexOf(word)
      return index >= 0 ? index : 0
    })
    
    return tokens.slice(0, 512) // ìµœëŒ€ ê¸¸ì´ ì œí•œ
  } catch (error) {
    console.error("í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨:", error)
    return []
  }
}

async function loadStudyModel() {
  // ì‹¤ì œ ëª¨ë¸ ë¡œë“œ ë¡œì§
  return null
}

async function isStudyRelated(text: string): Promise<boolean> {
  try {
    // 1. ONNX ëª¨ë¸ ê¸°ë°˜ íŒë‹¨ ì‹œë„
    const onnxResult = await isStudyRelatedONNX(text)
    if (onnxResult !== null) return onnxResult
    
    // 2. í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (í´ë°±)
    const studyKeywords = [
      "ê³µë¶€", "í•™ìŠµ", "ìˆ˜ì—…", "ë¬¸ì œ", "ì±…", "ì½ê¸°", "ì“°ê¸°", "ê³„ì‚°", "ê³µì‹", "ì´ë¡ ",
      "ì‹œí—˜", "ê³¼ì œ", "í”„ë¡œì íŠ¸", "ë¦¬í¬íŠ¸", "ë…¼ë¬¸", "ì—°êµ¬", "ë¶„ì„", "ì‹¤í—˜",
      "ê°•ì˜", "êµê³¼ì„œ", "ì°¸ê³ ì„œ", "ë¬¸ì œì§‘", "ì—°ìŠµ", "ë³µìŠµ", "ì˜ˆìŠµ"
    ]
    
    const lowerText = text.toLowerCase()
    return studyKeywords.some(keyword => lowerText.includes(keyword))
  } catch (error) {
    console.error("ê³µë¶€ ê´€ë ¨ íŒë‹¨ ì‹¤íŒ¨:", error)
    return false
  }
}

function buildPacket({ mel, scene_tag, noise_db }: { mel: number[]; scene_tag: string; noise_db: number }) {
  return {
    timestamp: Date.now(),
    mel,
    scene_tag,
    noise_db
  }
}

type SpeechRecognitionType = typeof window extends { webkitSpeechRecognition: infer T } ? T : any;
const SpeechRecognition: any =
  typeof window !== "undefined" ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition : null;

export default function HybridAudioPipeline() {
  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ Ref
  const audioContextRef = useRef<AudioContext | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null)
  const workerRef = useRef<Worker | null>(null)

  // --- ìƒíƒœ ê´€ë¦¬ ---
  const recognitionRef = useRef<SpeechRecognitionType | null>(null)
  const speechTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false) // ìŒì„± êµ¬ê°„ ê°ì§€ ìƒíƒœ
  const [isListening, setIsListening] = useState(false)
  const [liveTranscript, setLiveTranscript] = useState("") // ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë  í…ìŠ¤íŠ¸
  const [isInitialized, setIsInitialized] = useState(false) // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ìƒíƒœ
  const [isInitializing, setIsInitializing] = useState(false) // ì´ˆê¸°í™” ì§„í–‰ ì¤‘ ìƒíƒœ
  const [error, setError] = useState<string | null>(null) // ì˜¤ë¥˜ ìƒíƒœ
  const speechBufferRef = useRef<string>("") // í•œ ë°œí™”ê°€ ëë‚  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ëŠ” ë²„í¼
  const featureBufferRef = useRef<any[]>([]) // í•œ ë°œí™” ë™ì•ˆì˜ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ëª¨ìœ¼ëŠ” ë²„í¼

  // useEffect í´ë¡œì €ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ Ref
  const stateRef = useRef({ isSpeaking, isListening });
  stateRef.current = { isSpeaking, isListening };

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜
  const initializeAudioPipeline = async () => {
    if (isInitialized || isInitializing) return;
    
    setIsInitializing(true);
    setError(null);
    
    try {
      console.log("[AUDIO] ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘");
      
      // AudioContext ìƒì„± ë° ì‹¤í–‰ ìƒíƒœë¡œ ì „í™˜
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 })
      audioContextRef.current = audioContext
      
      // AudioContextê°€ suspended ìƒíƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ resume í˜¸ì¶œ
      if (audioContext.state === 'suspended') {
        console.log("[AUDIO] AudioContext suspended ìƒíƒœ, resume í˜¸ì¶œ");
        await audioContext.resume()
      }

      console.log("[AUDIO] AudioContext ìƒíƒœ:", audioContext.state);

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      console.log("[AUDIO] ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­");
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { sampleRate: 16000, channelCount: 1 },
      })
      console.log("[AUDIO] ë§ˆì´í¬ ê¶Œí•œ íšë“ ì„±ê³µ");

      // AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì‹œë„
      try {
        console.log("[AUDIO] AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì‹œë„");
        await audioContext.audioWorklet.addModule("/audio/stft-mel-processor.js")
        const workletNode = new AudioWorkletNode(audioContext, "stft-mel-processor")
        workletNodeRef.current = workletNode
        console.log("[AUDIO] AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ");

        const source = audioContext.createMediaStreamSource(stream)
        source.connect(workletNode)

        // Worker ë¡œë“œ
        try {
          console.log("[AUDIO] Worker ëª¨ë“œë¡œ ì‹œì‘")
          
          // ì¡°ê±´ë¶€ Worker ìƒì„±
          const createWorker = () => {
            if (typeof window !== 'undefined' && typeof Worker !== 'undefined') {
              try {
                return new Worker("/audio/ml-inference-worker.js");
              } catch (error) {
                console.warn("Worker ìƒì„± ì‹¤íŒ¨:", error);
                return null;
              }
            }
            return null;
          };

          const worker = createWorker();
          if (worker) {
            workerRef.current = worker;
            console.log("[AUDIO] Worker ìƒì„± ì„±ê³µ");

            // Workletì—ì„œ PCM ë°ì´í„°ë¥¼ Workerë¡œ ì „ë‹¬
            workletNode.port.onmessage = (e) => {
              console.log("[AUDIO] Workletì—ì„œ PCM ìˆ˜ì‹ :", e.data);
              if (e.data && e.data.pcm && workerRef.current) {
                console.log("[AUDIO] Workerë¡œ PCM ì „ë‹¬");
                workerRef.current.postMessage({ pcm: e.data.pcm });
              }
            };

            // Workerì—ì„œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ + scene_tag ìˆ˜ì‹ 
            worker.onmessage = (e) => {
              console.log("[AUDIO] Workerì—ì„œ ê²°ê³¼ ìˆ˜ì‹ :", e.data);
              const { mel, scene_tag, noise_db } = e.data
              const packet = buildPacket({ mel, scene_tag, noise_db })

              if (scene_tag === "speech") {
                console.log("[AUDIO] scene_tag === 'speech' ê°ì§€ë¨");
                if (!stateRef.current.isSpeaking) {
                  setIsSpeaking(true)
                  speechBufferRef.current = ""
                  featureBufferRef.current = []
                  console.log("[AUDIO] --- ğŸ¤ ë°œí™” ì‹œì‘ (Worker) ---")
                }
                featureBufferRef.current.push(packet)
                
                // Speech Recognition ì‹œì‘
                console.log("[STT] ìŒì„± ê°ì§€ë¨ (Worker ëª¨ë“œ), Speech Recognition ì‹œì‘ ì‹œë„");
                console.log("[STT] recognitionRef.current:", recognitionRef.current);
                console.log("[STT] stateRef.current.isListening:", stateRef.current.isListening);
                
                if (recognitionRef.current && !stateRef.current.isListening) {
                  try {
                    console.log("[STT] recognition.start() í˜¸ì¶œ ì‹œë„")
                    recognitionRef.current.start()
                    console.log("[STT] recognition.start() í˜¸ì¶œ ì™„ë£Œ")
                  } catch (err) {
                    console.error("[STT] recognition.start() ì—ëŸ¬:", err)
                  }
                } else {
                  console.log("[STT] Speech Recognition ì‹œì‘ ì¡°ê±´ ë¶ˆì¶©ì¡± (Worker ëª¨ë“œ)");
                  console.log("[STT] - recognitionRef.current ì¡´ì¬:", !!recognitionRef.current);
                  console.log("[STT] - ì´ë¯¸ ë“£ëŠ” ì¤‘:", stateRef.current.isListening);
                }
                
                if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
                speechTimeoutRef.current = setTimeout(() => {
                  if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
                }, 2000)
              }
            }
          } else {
            throw new Error("Worker ìƒì„± ì‹¤íŒ¨");
          }

        } catch (workerError) {
          console.warn("[AUDIO] Worker ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜:", workerError)
          
          // Worker ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë¶„ì„ ëª¨ë“œ
          const analyser = audioContext.createAnalyser()
          analyser.fftSize = 256
          source.connect(analyser)
          
          const bufferLength = analyser.frequencyBinCount
          const dataArray = new Uint8Array(bufferLength)
          
          const checkAudioLevel = () => {
            analyser.getByteFrequencyData(dataArray)
            const average = dataArray.reduce((a, b) => a + b) / bufferLength
            
            // ê°„ë‹¨í•œ ìŒì„± ê°ì§€ (ì„ê³„ê°’ ê¸°ë°˜)
            if (average > 30) {
              if (!stateRef.current.isSpeaking) {
                setIsSpeaking(true)
                speechBufferRef.current = ""
                featureBufferRef.current = []
                console.log("[AUDIO] --- ğŸ¤ ë°œí™” ì‹œì‘ (ê¸°ë³¸ ëª¨ë“œ) ---")
              }
              
              // Speech Recognition ì‹œì‘
              console.log("[STT] ìŒì„± ê°ì§€ë¨ (ê¸°ë³¸ ëª¨ë“œ), Speech Recognition ì‹œì‘ ì‹œë„");
              console.log("[STT] recognitionRef.current:", recognitionRef.current);
              console.log("[STT] stateRef.current.isListening:", stateRef.current.isListening);
              
              if (recognitionRef.current && !stateRef.current.isListening) {
                try {
                  console.log("[STT] recognition.start() í˜¸ì¶œ ì‹œë„")
                  recognitionRef.current.start()
                  console.log("[STT] recognition.start() í˜¸ì¶œ ì™„ë£Œ")
                } catch (err) {
                  console.error("[STT] recognition.start() ì—ëŸ¬:", err)
                }
              } else {
                console.log("[STT] Speech Recognition ì‹œì‘ ì¡°ê±´ ë¶ˆì¶©ì¡± (ê¸°ë³¸ ëª¨ë“œ)");
                console.log("[STT] - recognitionRef.current ì¡´ì¬:", !!recognitionRef.current);
                console.log("[STT] - ì´ë¯¸ ë“£ëŠ” ì¤‘:", stateRef.current.isListening);
              }
              
              if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
              speechTimeoutRef.current = setTimeout(() => {
                if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
              }, 2000)
            }
            
            requestAnimationFrame(checkAudioLevel)
          }
          
          checkAudioLevel()
        }

      } catch (workletError) {
        console.warn("[AUDIO] AudioWorklet ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œë¡œ ì „í™˜:", workletError)
        
        // AudioWorklet ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œ
        const analyser = audioContext.createAnalyser()
        analyser.fftSize = 256
        const source = audioContext.createMediaStreamSource(stream)
        source.connect(analyser)
        
        const bufferLength = analyser.frequencyBinCount
        const dataArray = new Uint8Array(bufferLength)
        
        const checkAudioLevel = () => {
          analyser.getByteFrequencyData(dataArray)
          const average = dataArray.reduce((a, b) => a + b) / bufferLength
          
          // ê°„ë‹¨í•œ ìŒì„± ê°ì§€ (ì„ê³„ê°’ ê¸°ë°˜)
          if (average > 30) {
            if (!stateRef.current.isSpeaking) {
              setIsSpeaking(true)
              speechBufferRef.current = ""
              featureBufferRef.current = []
              console.log("[AUDIO] --- ğŸ¤ ë°œí™” ì‹œì‘ (ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œ) ---")
            }
            
            // Speech Recognition ì‹œì‘
            console.log("[STT] ìŒì„± ê°ì§€ë¨ (ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œ), Speech Recognition ì‹œì‘ ì‹œë„");
            console.log("[STT] recognitionRef.current:", recognitionRef.current);
            console.log("[STT] stateRef.current.isListening:", stateRef.current.isListening);
            
            if (recognitionRef.current && !stateRef.current.isListening) {
              try {
                console.log("[STT] recognition.start() í˜¸ì¶œ ì‹œë„")
                recognitionRef.current.start()
                console.log("[STT] recognition.start() í˜¸ì¶œ ì™„ë£Œ")
              } catch (err) {
                console.error("[STT] recognition.start() ì—ëŸ¬:", err)
              }
            } else {
              console.log("[STT] Speech Recognition ì‹œì‘ ì¡°ê±´ ë¶ˆì¶©ì¡± (ê¸°ë³¸ ë¶„ì„ê¸° ëª¨ë“œ)");
              console.log("[STT] - recognitionRef.current ì¡´ì¬:", !!recognitionRef.current);
              console.log("[STT] - ì´ë¯¸ ë“£ëŠ” ì¤‘:", stateRef.current.isListening);
            }
            
            if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
            speechTimeoutRef.current = setTimeout(() => {
              if (recognitionRef.current && stateRef.current.isListening) recognitionRef.current.stop()
            }, 2000)
          }
          
          requestAnimationFrame(checkAudioLevel)
        }
        
        checkAudioLevel()
      }

      setIsInitialized(true);
      console.log("[AUDIO] ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ");

    } catch (error) {
      console.error("[AUDIO] ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì„¤ì • ì˜¤ë¥˜:", error)
      setError(`ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
    } finally {
      setIsInitializing(false);
    }
  };

  // ìŒì„± ì¸ì‹(STT) ì„¤ì •
  const setupSpeechRecognition = () => {
    console.log("[STT] Speech Recognition ì„¤ì • ì‹œì‘");
    console.log("[STT] SpeechRecognition ê°ì²´:", SpeechRecognition);
    
    if (!SpeechRecognition) {
      console.error("[STT] ì´ ë¸Œë¼ìš°ì €ëŠ” Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
      return
    }
    
    const recognition = new SpeechRecognition()
    recognition.continuous = true
    recognition.interimResults = true
    recognition.lang = "ko-KR"
    
    console.log("[STT] Recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„±:", recognition);

    recognition.onresult = (event: any) => {
      console.log("[STT] onresult ì´ë²¤íŠ¸ ë°œìƒ");
      let finalTranscript = "";
      for (let i = event.resultIndex; i < event.results.length; ++i) {
        setLiveTranscript(event.results[i][0].transcript);
        console.log("[STT] ì‹¤ì‹œê°„ ì¸ì‹ í…ìŠ¤íŠ¸:", event.results[i][0].transcript);
        if (event.results[i].isFinal) {
          finalTranscript += event.results[i][0].transcript;
        }
      }
      if (finalTranscript) {
        speechBufferRef.current += finalTranscript.trim() + " ";
        console.log("[STT] ìµœì¢… ì¸ì‹ í…ìŠ¤íŠ¸ ëˆ„ì :", speechBufferRef.current);
      }
    };

    recognition.onstart = () => {
      setIsListening(true);
      console.log("[STT] recognition ì‹œì‘ë¨");
    };
    recognition.onend = () => {
      setIsListening(false);
      console.log("[STT] recognition ì¢…ë£Œë¨");
      if (stateRef.current.isSpeaking) {
        console.log("[STT] --- ğŸ¤ ë°œí™” ì¢…ë£Œ ---");
        processSpeechSegment();
        setIsSpeaking(false);
      }
    };
    recognition.onerror = (event: any) => {
      console.error("[STT] recognition error:", event.error);
    };

    recognitionRef.current = recognition;
    console.log("[STT] recognition ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í• ë‹¹ ì™„ë£Œ:", recognitionRef.current);
  };

  // ìˆ˜ì§‘ëœ ë°œí™” êµ¬ê°„ ë°ì´í„° ì¢…í•© ë¶„ì„
  const processSpeechSegment = async () => {
    const fullText = speechBufferRef.current.trim();
    if (!fullText) return;

    console.log("======[ ìµœì¢… ë¶„ì„ ]======");
    console.log("ì¸ì‹ëœ ë¬¸ì¥:", fullText);

    // ê³µë¶€ ê´€ë ¨ ì—¬ë¶€ íŒë‹¨ (ML ê¸°ë°˜)
    const isStudy = await isStudyRelated(fullText);
    console.log("ê³µë¶€ ê´€ë ¨ ì—¬ë¶€:", isStudy);

    // ë¬¸ë§¥ ë¶„ì„
    const context = analyzeTextContext(fullText);
    const contextualWeight = getContextualWeight(context);
    console.log("ë¬¸ë§¥ ë¶„ì„:", context, "ê°€ì¤‘ì¹˜:", contextualWeight);
    console.log("ë¶„ì„ ê²°ê³¼ - í•™ìŠµ ê´€ë ¨:", isStudy, "í…ìŠ¤íŠ¸:", fullText);
  };

  // í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const analyzeTextContext = (text: string):
    | 'discussion'
    | 'class'
    | 'presentation'
    | 'question'
    | 'frustration'
    | 'statement'
    | 'unknown' => {
    const lower = text.toLowerCase();

    // í† ë¡  ìƒí™©
    if (
      lower.includes("í† ë¡ ") ||
      lower.includes("ì°¬ì„±") ||
      lower.includes("ë°˜ëŒ€") ||
      lower.includes("ì˜ê²¬") ||
      lower.includes("ìƒê°ì€")
    ) {
      return 'discussion';
    }
    // ìˆ˜ì—… ìƒí™©
    if (
      lower.includes("ì„ ìƒë‹˜") ||
      lower.includes("êµê³¼ì„œ") ||
      lower.includes("ë¬¸ì œ") ||
      lower.includes("ì„¤ëª…") ||
      lower.includes("ìˆ˜ì—…")
    ) {
      return 'class';
    }
    // ë°œí‘œ ìƒí™©
    if (
      lower.includes("ë°œí‘œ") ||
      lower.includes("ê²°ë¡ ") ||
      lower.includes("ìš”ì•½") ||
      lower.includes("ì •ë¦¬")
    ) {
      return 'presentation';
    }
    // ì§ˆë¬¸
    if (
      lower.includes("ì–´ë–»ê²Œ") ||
      lower.includes("ì™œ") ||
      lower.includes("ë­ì•¼") ||
      lower.includes("ë¬´ì—‡") ||
      lower.includes("ê¶ê¸ˆ") ||
      text.endsWith("?")
    ) {
      return 'question';
    }
    // ì¢Œì ˆ
    if (
      lower.includes("ì•ˆë¼") ||
      lower.includes("ì§œì¦ë‚˜") ||
      lower.includes("ì•„ì˜¤")
    ) {
      return 'frustration';
    }
    // ì§„ìˆ 
    if (text.length > 5) {
      return 'statement';
    }
    return 'unknown';
  };

  // ë¬¸ë§¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextualWeight = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): number => {
    switch (context) {
      case 'discussion': return 0.95; // í† ë¡ : ì§‘ì¤‘ë„ ì•½ê°„ ë†’ìŒ
      case 'class': return 1.0; // ìˆ˜ì—…: ì§‘ì¤‘ë„ ìµœìƒ
      case 'presentation': return 0.9; // ë°œí‘œ: ì§‘ì¤‘ë„ ë†’ìŒ
      case 'question': return 0.8; // ì§ˆë¬¸: ë§‰í˜”ì„ ê°€ëŠ¥ì„±, ì§‘ì¤‘ë„ ì•½ê°„ ê°ì†Œ
      case 'frustration': return 0.5; // ì¢Œì ˆ: ëª…í™•í•œ ì§‘ì¤‘ë ¥ ì €í•˜ ì‹ í˜¸, ì§‘ì¤‘ë„ í¬ê²Œ ê°ì†Œ
      case 'statement': return 1.0; // í˜¼ì£ë§/ë‚´ìš© ì½ê¸°: í•™ìŠµ í™œë™ì˜ ì¼ë¶€ë¡œ íŒë‹¨, ê°€ì¤‘ì¹˜ ì—†ìŒ
      default: return 0.9; // ë¶ˆëª…í™•: ì§§ì€ ì¤‘ì–¼ê±°ë¦¼ ë“±, ì•½ê°„ì˜ ë°©í•´ë¡œ ê°„ì£¼
    }
  };

  useEffect(() => {
    // Speech Recognition ì„¤ì •ë§Œ ë¨¼ì € ìˆ˜í–‰
    setupSpeechRecognition();

    return () => {
      if (recognitionRef.current) recognitionRef.current.abort()
      if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
      if (audioContextRef.current) audioContextRef.current.close()
      if (workerRef.current) workerRef.current.terminate()
    }
  }, [])

  return (
    <div className="p-4 bg-slate-100 rounded-lg">
      <h3 className="font-bold">í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸</h3>
      
      {!isInitialized && !isInitializing && (
        <button 
          onClick={initializeAudioPipeline}
          className="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600 mb-4"
        >
          ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì‹œì‘
        </button>
      )}
      
      {isInitializing && (
        <p className="text-blue-600 mb-4">ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...</p>
      )}
      
      {error && (
        <p className="text-red-600 mb-4">ì˜¤ë¥˜: {error}</p>
      )}
      
      {isInitialized && (
        <div className="space-y-2">
          <p><b>ìŒì„± ì¸ì‹ ìƒíƒœ:</b> {isListening ? "ë“£ëŠ” ì¤‘..." : "ëŒ€ê¸° ì¤‘"}</p>
          <p><b>ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸:</b> {liveTranscript}</p>
        </div>
      )}
    </div>
  )
}
