"use client"

import { useEffect, useRef, useState, useCallback, useMemo } from "react"
import { koelectraPreprocess, testTokenizer, initializeTokenizer } from "@/lib/tokenizer/koelectra"
import { useKoELECTRA } from "@/hooks/useKoELECTRA"
import { useDashboardStore } from "@/stores/dashboardStore"

// ê³µë¶€ ê´€ë ¨ í…ìŠ¤íŠ¸ ë¶„ì„ í•¨ìˆ˜ (í‚¤ì›Œë“œ ê¸°ë°˜) - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
const analyzeStudyRelatedByKeywords = (() => {
  const studyKeywords = [
    "ê³µë¶€", "í•™ìŠµ", "ìˆ˜ì—…", "ë¬¸ì œ", "ì±…", "ì½ê¸°", "ì“°ê¸°", "ê³„ì‚°", "ê³µì‹", "ì´ë¡ ",
    "ì‹œí—˜", "ê³¼ì œ", "í”„ë¡œì íŠ¸", "ë¦¬í¬íŠ¸", "ë…¼ë¬¸", "ì—°êµ¬", "ë¶„ì„", "ì‹¤í—˜",
    "ê°•ì˜", "êµê³¼ì„œ", "ì°¸ê³ ì„œ", "ë¬¸ì œì§‘", "ì—°ìŠµ", "ë³µìŠµ", "ì˜ˆìŠµ"
  ]
  
  const keywordSet = new Set(studyKeywords.map(k => k.toLowerCase()))
  
  return (text: string): boolean => {
    const lowerText = text.toLowerCase()
    return studyKeywords.some(keyword => lowerText.includes(keyword))
  }
})()

// ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë””ë°”ìš´ìŠ¤ í›…
function useDebounce<T>(value: T, delay: number): T {
  const [debouncedValue, setDebouncedValue] = useState<T>(value)

  useEffect(() => {
    const handler = setTimeout(() => {
      setDebouncedValue(value)
    }, delay)

    return () => {
      clearTimeout(handler)
    }
  }, [value, delay])

  return debouncedValue
}



function buildPacket({ mel, scene_tag, noise_db }: { mel: number[]; scene_tag: string; noise_db: number }) {
  return {
    timestamp: Date.now(),
    mel,
    scene_tag,
    noise_db
  }
}

type SpeechRecognitionType = typeof window extends { webkitSpeechRecognition: infer T } ? T : any;
const SpeechRecognition: any =
  typeof window !== "undefined" ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition : null;

export default function HybridAudioPipeline() {
  // ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
  const { isRunning: isFocusSessionRunning, isPaused: isFocusSessionPaused } = useDashboardStore()
  
  // KoELECTRA ëª¨ë¸ í›…
  const { 
    isLoaded: isModelLoaded, 
    isLoading: isModelLoading, 
    error: modelError, 
    inference: koelectraInference
  } = useKoELECTRA({ 
    autoLoad: true
  })

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ Ref
  const audioContextRef = useRef<AudioContext | null>(null)
  const workletNodeRef = useRef<AudioWorkletNode | null>(null)
  const workerRef = useRef<Worker | null>(null)
  const streamRef = useRef<MediaStream | null>(null) // ìŠ¤íŠ¸ë¦¼ì„ í•œ ë²ˆë§Œ ìƒì„±í•˜ê³  ì¬ì‚¬ìš©

  // --- ìƒíƒœ ê´€ë¦¬ ---
  const recognitionRef = useRef<SpeechRecognitionType | null>(null)
  const speechTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const [isSpeaking, setIsSpeaking] = useState(false) // ìŒì„± êµ¬ê°„ ê°ì§€ ìƒíƒœ
  const [isListening, setIsListening] = useState(false)
  const [isAnalyzing, setIsAnalyzing] = useState(false)
  const [isSpeechRecognitionActive, setIsSpeechRecognitionActive] = useState(false)
  const [liveTranscript, setLiveTranscript] = useState("") // ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë  í…ìŠ¤íŠ¸
  const [isInitialized, setIsInitialized] = useState(false) // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ìƒíƒœ
  const [isInitializing, setIsInitializing] = useState(false) // ì´ˆê¸°í™” ì§„í–‰ ì¤‘ ìƒíƒœ
  const [error, setError] = useState<string | null>(null) // ì˜¤ë¥˜ ìƒíƒœ
  const [currentAudioLevel, setCurrentAudioLevel] = useState<number>(0) // í˜„ì¬ ì˜¤ë””ì˜¤ ë ˆë²¨
  const speechBufferRef = useRef<string>("") // í•œ ë°œí™”ê°€ ëë‚  ë•Œê¹Œì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ìœ¼ëŠ” ë²„í¼
  const featureBufferRef = useRef<any[]>([]) // í•œ ë°œí™” ë™ì•ˆì˜ ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ëª¨ìœ¼ëŠ” ë²„í¼
  
  // íƒ€ì„ìŠ¤íƒ¬í”„ ê´€ë¦¬
  const speechStartTimeRef = useRef<number | null>(null)
  const speechEndTimeRef = useRef<number | null>(null)
  const silenceStartTimeRef = useRef<number | null>(null) // ì¡°ìš©í•¨ ì‹œì‘ ì‹œê°„

  // ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë””ë°”ìš´ìŠ¤ëœ í…ìŠ¤íŠ¸
  const debouncedLiveTranscript = useDebounce(liveTranscript, 100)

  // useEffect í´ë¡œì €ì—ì„œ ìµœì‹  ìƒíƒœë¥¼ ì°¸ì¡°í•˜ê¸° ìœ„í•œ Ref
  const stateRef = useRef({ isSpeaking, isListening });
  stateRef.current = { isSpeaking, isListening };

  // ì§‘ì¤‘ ëª¨ë“œ ìƒíƒœ ë³€í™” ê°ì§€ ë° ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì œì–´
  useEffect(() => {
    // ì§‘ì¤‘ ëª¨ë“œê°€ ì¢…ë£Œë˜ê±°ë‚˜ ì¼ì‹œì •ì§€ëœ ê²½ìš°
    if (!isFocusSessionRunning || isFocusSessionPaused) {
      // ìŒì„± ì¸ì‹ ì¤‘ë‹¨
      if (recognitionRef.current && recognitionRef.current.state === 'active') {
        try {
          recognitionRef.current.stop()
        } catch (error) {
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì¤‘ ì˜¤ë¥˜:', error)
        }
      }
      
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¤‘ë‹¨
      setIsListening(false)
      
      // ë°œí™” ë²„í¼ ì´ˆê¸°í™”
      speechBufferRef.current = ""
      speechStartTimeRef.current = null
      speechEndTimeRef.current = null
      silenceStartTimeRef.current = null
      
      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”
      setLiveTranscript("")
      setIsSpeaking(false)
    }
    
    // ì§‘ì¤‘ ëª¨ë“œê°€ ì¬ì‹œì‘ëœ ê²½ìš°
    else if (isFocusSessionRunning && !isFocusSessionPaused && isInitialized) {
      // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì¬ì‹œì‘
      setIsListening(true)
      
      // ìŒì„± ì¸ì‹ ì¬ì‹œì‘
      if (recognitionRef.current && recognitionRef.current.state === 'inactive') {
        try {
          recognitionRef.current.start()
        } catch (error) {
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜:', error)
        }
      }
    }
  }, [isFocusSessionRunning, isFocusSessionPaused, isInitialized])

  // ëª¨ë¸ ìƒíƒœ ìš”ì•½
  const modelStatus = useMemo(() => ({
    status: isModelLoaded ? 'âœ… ë¡œë“œë¨' : isModelLoading ? 'ğŸ”„ ë¡œë”© ì¤‘' : 'âŒ ë¡œë“œ ì•ˆë¨'
  }), [isModelLoaded, isModelLoading])

  // ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í•¨ìˆ˜ - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
  const initializeAudioPipeline = useCallback(async () => {
    if (isInitialized || isInitializing) return;
    
    setIsInitializing(true);
    setError(null);
    
    try {
      // AudioContext ìƒì„± ë° ì‹¤í–‰ ìƒíƒœë¡œ ì „í™˜
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ 
        sampleRate: 16000,
        latencyHint: 'interactive' // ì„±ëŠ¥ ìµœì í™”
      })
      audioContextRef.current = audioContext
      
      // AudioContext ìƒíƒœ í™•ì¸ ë° ë³µêµ¬
      if (audioContext.state === 'suspended') {
        await audioContext.resume()
      }
      
      if (audioContext.state === 'closed') {
        throw new Error('AudioContextê°€ ë‹«í˜€ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.');
      }
      
      // AudioContext ìƒíƒœê°€ runningì¸ì§€ í™•ì¸
      if (audioContext.state !== 'running') {
        await audioContext.resume();
      }

      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ë° ìŠ¤íŠ¸ë¦¼ ìƒì„± (í•œ ë²ˆë§Œ)
      if (!streamRef.current) {
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 16000
          }
        });
        streamRef.current = stream;
        
        // ì˜¤ë””ì˜¤ íŠ¸ë™ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
        const audioTrack = stream.getAudioTracks()[0];
        audioTrack.onended = () => console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ì¢…ë£Œë¨');
        audioTrack.onmute = () => console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ìŒì†Œê±°ë¨');
        audioTrack.onunmute = () => console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ìŒì†Œê±° í•´ì œë¨');
        
        console.log('ğŸ¤ ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ:', {
          streamId: stream.id,
          streamActive: stream.active,
          audioTrackEnabled: audioTrack.enabled,
          audioTrackMuted: audioTrack.muted
        });
      }
      
      const stream = streamRef.current;

      // ë§ˆì´í¬ íŠ¸ë™ì—ì„œ ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„° í™•ì¸
      const audioTrack = stream.getAudioTracks()[0];
      if (audioTrack) {
        console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ìƒì„¸ ì •ë³´:', {
          id: audioTrack.id,
          label: audioTrack.label,
          enabled: audioTrack.enabled,
          muted: audioTrack.muted,
          readyState: audioTrack.readyState,
          settings: audioTrack.getSettings(),
          constraints: audioTrack.getConstraints()
        });
        
        // ì˜¤ë””ì˜¤ íŠ¸ë™ ì´ë²¤íŠ¸ ë¦¬ìŠ¤ë„ˆ ì¶”ê°€
        audioTrack.onended = () => console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ì¢…ë£Œë¨');
        audioTrack.onmute = () => console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ìŒì†Œê±°ë¨');
        audioTrack.onunmute = () => console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ ìŒì†Œê±° í•´ì œë¨');
        
        console.log('ğŸ¤ ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™” ì™„ë£Œ:', {
          streamId: stream.id,
          streamActive: stream.active,
          audioTrackEnabled: audioTrack.enabled,
          audioTrackMuted: audioTrack.muted
        });
      }

              // AudioWorklet ëª¨ë“ˆ ë¡œë“œ ì‹œë„ (AudioContext ìƒíƒœ í™•ì¸ í›„)
        try {
          if (audioContext.state !== 'running') {
            await audioContext.resume();
          }
          
          await audioContext.audioWorklet.addModule("/audio/stft-mel-processor.js")
          const workletNode = new AudioWorkletNode(audioContext, "stft-mel-processor", {
            numberOfInputs: 1,
            numberOfOutputs: 0,
            processorOptions: {
              sampleRate: 16000,
              frameSize: 1024,
              hopSize: 512
            }
          })
          workletNodeRef.current = workletNode

          // AudioWorkletìš© ì†ŒìŠ¤
          const workletSource = audioContext.createMediaStreamSource(stream)
          workletSource.connect(workletNode)
          
          console.log('ğŸ¤ ì‹¤ì œ ì˜¤ë””ì˜¤ ë¶„ì„ ìŠ¤íŠ¸ë¦¼ ì—°ê²°:', {
            audioContextState: audioContext.state,
            streamId: stream.id,
            streamActive: stream.active,
            workletSourceContext: workletSource.context === audioContext,
            workletNodeState: workletNode.context.state
          });

        // Web Worker ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        const createWorker = () => {
          if (typeof window !== 'undefined' && typeof Worker !== 'undefined') {
            try {
              return new Worker("/audio/ml-inference-worker.js");
            } catch (error) {
              console.warn("Worker ìƒì„± ì‹¤íŒ¨:", error);
              return null;
            }
          }
          return null;
        };

        const worker = createWorker();
        workerRef.current = worker;

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ìš© ì†ŒìŠ¤ì™€ ë¶„ì„ê¸° (í•œ ë²ˆë§Œ ìƒì„±)
        const levelSource = audioContext.createMediaStreamSource(stream);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048; // ë” í° FFT í¬ê¸°ë¡œ ë³€ê²½
        analyser.smoothingTimeConstant = 0.1; // ë” ë¹ ë¥¸ ë°˜ì‘
        analyser.minDecibels = -90; // ë” ë‚®ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        analyser.maxDecibels = -10; // ë” ë†’ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        levelSource.connect(analyser);

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ í•¨ìˆ˜ (ëª…í™•í•œ ìŒì„± ê°ì§€ ìµœì í™”)
        const checkAudioLevel = async () => {
          // AudioContext ìƒíƒœ í™•ì¸
          if (!audioContext || audioContext.state === 'closed') {
            return;
          }
          
          if (audioContext.state === 'suspended') {
            try {
              await audioContext.resume();
            } catch (error) {
              return;
            }
          }
          
          if (!stateRef.current.isListening) {
            return;
          }
          
          // ìŠ¤íŠ¸ë¦¼ ìƒíƒœ í™•ì¸
          if (!streamRef.current || !streamRef.current.active) {
            console.warn('ğŸ¤ ìŠ¤íŠ¸ë¦¼ì´ ë¹„í™œì„± ìƒíƒœì…ë‹ˆë‹¤');
            return;
          }
          
          const audioTrack = streamRef.current.getAudioTracks()[0];
          if (!audioTrack || !audioTrack.enabled || audioTrack.muted) {
            console.warn('ğŸ¤ ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤:', {
              enabled: audioTrack?.enabled,
              muted: audioTrack?.muted,
              readyState: audioTrack?.readyState
            });
            return;
          }
          
          // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë¡œì§ - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ê¸° ì‚¬ìš©
          const bufferLength = analyser.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          analyser.getByteFrequencyData(dataArray);
          
          // ì‹œê°„ ë„ë©”ì¸ ë°ì´í„°ë¡œë„ ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° ì‹œë„
          const timeDataArray = new Uint8Array(bufferLength);
          analyser.getByteTimeDomainData(timeDataArray);
          
          // ë””ë²„ê¹…: ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„° í™•ì¸ (ë§¤ìš° ë“œë¬¼ê²Œë§Œ)
          if (Math.random() < 0.001 && streamRef.current) { // 0.1% í™•ë¥ ë¡œë§Œ ë¡œê¹…
            console.log('ğŸ¤ ì‹¤ì œ ì˜¤ë””ì˜¤ ë¶„ì„ ë°ì´í„° í™•ì¸:', {
              streamId: streamRef.current.id,
              audioContextState: audioContext.state,
              sourceConnected: levelSource.context === audioContext,
              dataArrayMax: Math.max(...dataArray),
              dataArrayMin: Math.min(...dataArray),
              dataArrayRange: `${Math.min(...dataArray)}-${Math.max(...dataArray)}`,
              nonZeroCount: dataArray.filter(val => val > 0).length,
              timeDataMax: Math.max(...timeDataArray),
              timeDataMin: Math.min(...timeDataArray),
              timeDataRange: `${Math.min(...timeDataArray)}-${Math.max(...timeDataArray)}`,
              streamActive: streamRef.current.active,
              audioTrackEnabled: streamRef.current.getAudioTracks()[0].enabled,
              audioTrackMuted: streamRef.current.getAudioTracks()[0].muted,
              // ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
              bufferLength,
              fftSize: analyser.fftSize,
              smoothingTimeConstant: analyser.smoothingTimeConstant,
              minDecibels: analyser.minDecibels,
              maxDecibels: analyser.maxDecibels
            });
          }
          
          // RMS (Root Mean Square) ê³„ì‚°ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •
          let rms = 0;
          for (let i = 0; i < timeDataArray.length; i++) {
            const sample = (timeDataArray[i] - 128) / 128; // -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
            rms += sample * sample;
          }
          rms = Math.sqrt(rms / timeDataArray.length);
          const rmsLevel = rms * 100; // 0-100 ë²”ìœ„ë¡œ ë³€í™˜
          
          // ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë°ì´í„° ë¶„ì„ (ë” ë¯¼ê°í•˜ê²Œ)
          const freqAverage = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
          const freqMax = Math.max(...dataArray);
          const freqMin = Math.min(...dataArray);
          
          // ë” ë¯¼ê°í•œ ë ˆë²¨ ê³„ì‚° (ì£¼íŒŒìˆ˜ ìµœëŒ€ê°’ê³¼ RMS ì¤‘ ë” ë†’ì€ ê°’ ì‚¬ìš©)
          const finalLevel = Math.max(freqMax * 0.5, rmsLevel * 2);
          
          // ë””ë²„ê¹…: ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
          const hasAudioData = freqMax > 0 || rms > 0.001;
          
          if (!hasAudioData && Math.random() < 0.01) { // 1% í™•ë¥ ë¡œ ë¡œê¹…
            console.warn('ğŸ¤ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤:', {
              freqMax,
              freqMin,
              freqAverage: freqAverage.toFixed(2),
              rms: rms.toFixed(4),
              rmsLevel: rmsLevel.toFixed(2),
              finalLevel: finalLevel.toFixed(2),
              streamActive: streamRef.current?.active,
              audioTrackEnabled: audioTrack?.enabled,
              audioTrackMuted: audioTrack?.muted
            });
          }
          
          setCurrentAudioLevel(finalLevel);
          
          // ë””ë²„ê¹…: ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ìƒíƒœ í™•ì¸ (ë§¤ìš° ë“œë¬¼ê²Œë§Œ)
          if (Math.random() < 0.001) { // 0.1% í™•ë¥ ë¡œë§Œ ë¡œê¹…
            console.log('ğŸ” ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ìƒíƒœ:', {
              analyserFftSize: analyser.fftSize,
              analyserFrequencyBinCount: analyser.frequencyBinCount,
              analyserSmoothingTimeConstant: analyser.smoothingTimeConstant,
              analyserMinDecibels: analyser.minDecibels,
              analyserMaxDecibels: analyser.maxDecibels,
              sourceConnected: levelSource.context === audioContext,
              dataArrayMax: Math.max(...dataArray),
              dataArrayMin: Math.min(...dataArray),
              dataArraySum: dataArray.reduce((sum, val) => sum + val, 0),
              nonZeroCount: dataArray.filter(val => val > 0).length,
              rmsLevel: rmsLevel.toFixed(2),
              timeDataMax: Math.max(...timeDataArray),
              timeDataMin: Math.min(...timeDataArray),
              finalLevel: finalLevel.toFixed(2),
              isSpeaking: stateRef.current.isSpeaking,
              audioContextState: audioContext.state,
              streamActive: stream.active,
              streamId: stream.id,
              // ì¶”ê°€ ë””ë²„ê¹… ì •ë³´
              freqAverage: freqAverage.toFixed(2),
              rms: rms.toFixed(4),
              timeDataRange: `${Math.min(...timeDataArray)}-${Math.max(...timeDataArray)}`,
              dataArrayRange: `${Math.min(...dataArray)}-${Math.max(...dataArray)}`
            });
          }
          
          // ìŒì„± ê°ì§€ (ì „ì²´ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì— ë§ëŠ” ì„ê³„ê°’)
          if (finalLevel > 5 && !stateRef.current.isSpeaking) {
            setIsSpeaking(true);
            speechStartTimeRef.current = Date.now();
            silenceStartTimeRef.current = null; // ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
            console.log('ğŸ¤ ìŒì„± ê°ì§€ë¨ (ë ˆë²¨:', finalLevel.toFixed(1), ')');
          } else if (finalLevel < 2 && stateRef.current.isSpeaking) {
            // ì¡°ìš©í•¨ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            if (!silenceStartTimeRef.current) {
              silenceStartTimeRef.current = Date.now();
            }
            
            // 0.3ì´ˆ ì´ìƒ ì¡°ìš©í•˜ë©´ ë°œí™” ì¢…ë£Œë¡œ íŒë‹¨ (ë” ë¹ ë¥´ê²Œ)
            const silenceDuration = Date.now() - silenceStartTimeRef.current;
            if (silenceDuration > 200) {
              setIsSpeaking(false);
              speechEndTimeRef.current = Date.now();
              console.log('ğŸ¤ ìŒì„± ì¢…ë£Œ ê°ì§€ë¨ (ë ˆë²¨:', finalLevel.toFixed(1), ', ì¡°ìš©í•¨ ì§€ì†:', silenceDuration + 'ms)');
              
              // ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±°
              if (speechBufferRef.current.trim()) {
                processSpeechSegment();
              }
              
              // íƒ€ì´ë¨¸ ë¦¬ì…‹
              silenceStartTimeRef.current = null;
            }
          } else if (finalLevel >= 2 && stateRef.current.isSpeaking) {
            // ë‹¤ì‹œ ì†Œë¦¬ê°€ ë‚˜ë©´ ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
            silenceStartTimeRef.current = null;
          }
          
          requestAnimationFrame(() => checkAudioLevel());
        };

        // ì˜¤ë””ì˜¤ ë ˆë²¨ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        setIsListening(true); // ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ trueë¡œ ì„¤ì •
        
        // ê°•ì œë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹œì‘
        setTimeout(() => {
          checkAudioLevel();
        }, 100);

        // Speech Recognition ì„¤ì •
        setupSpeechRecognition();

        setIsInitialized(true);
        console.log('ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ');
      } catch (workletError) {
        console.warn("AudioWorklet ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¡œ ëŒ€ì²´:", workletError);
        
        // ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ (ëª…í™•í•œ ìŒì„± ê°ì§€)
        // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ìš© ì†ŒìŠ¤ì™€ ë¶„ì„ê¸° (í•œ ë²ˆë§Œ ìƒì„±)
        const source = audioContext.createMediaStreamSource(streamRef.current);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048; // ë” í° FFT í¬ê¸°ë¡œ ë³€ê²½
        analyser.smoothingTimeConstant = 0.1; // ë” ë¹ ë¥¸ ë°˜ì‘
        analyser.minDecibels = -90; // ë” ë‚®ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        analyser.maxDecibels = -10; // ë” ë†’ì€ ë°ì‹œë²¨ ì„ê³„ê°’
        source.connect(analyser);
         
         console.log('ğŸ¤ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ìŠ¤íŠ¸ë¦¼ ì—°ê²°:', {
           audioContextState: audioContext.state,
           streamId: streamRef.current.id,
           streamActive: streamRef.current.active,
           sourceContext: source.context === audioContext
         });
         
         const checkAudioLevel = async () => {
           // AudioContext ìƒíƒœ í™•ì¸
           if (!audioContext || audioContext.state === 'closed') {
             return;
           }
           
           if (audioContext.state !== 'running') {
             return;
           }
           
           if (!stateRef.current.isListening) {
             return;
           }
           
           // ìŠ¤íŠ¸ë¦¼ ìƒíƒœ í™•ì¸
           if (!streamRef.current || !streamRef.current.active) {
             console.warn('ğŸ¤ ê¸°ë³¸ ì²´í¬: ìŠ¤íŠ¸ë¦¼ì´ ë¹„í™œì„± ìƒíƒœì…ë‹ˆë‹¤');
             return;
           }
           
           const audioTrack = streamRef.current.getAudioTracks()[0];
           if (!audioTrack || !audioTrack.enabled || audioTrack.muted) {
             console.warn('ğŸ¤ ê¸°ë³¸ ì²´í¬: ì˜¤ë””ì˜¤ íŠ¸ë™ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤:', {
               enabled: audioTrack?.enabled,
               muted: audioTrack?.muted,
               readyState: audioTrack?.readyState
             });
             return;
           }
           
           // ì˜¤ë””ì˜¤ ë ˆë²¨ ë¶„ì„ ë¡œì§ - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ê¸° ì‚¬ìš©
           const bufferLength = analyser.frequencyBinCount;
           const dataArray = new Uint8Array(bufferLength);
           analyser.getByteFrequencyData(dataArray);
           
           // ì‹œê°„ ë„ë©”ì¸ ë°ì´í„°ë¡œë„ ì˜¤ë””ì˜¤ ë ˆë²¨ ê³„ì‚° ì‹œë„
           const timeDataArray = new Uint8Array(bufferLength);
           analyser.getByteTimeDomainData(timeDataArray);
           
           // RMS (Root Mean Square) ê³„ì‚°ìœ¼ë¡œ ì˜¤ë””ì˜¤ ë ˆë²¨ ì¸¡ì •
           let rms = 0;
           for (let i = 0; i < timeDataArray.length; i++) {
             const sample = (timeDataArray[i] - 128) / 128; // -1 to 1 ë²”ìœ„ë¡œ ì •ê·œí™”
             rms += sample * sample;
           }
           rms = Math.sqrt(rms / timeDataArray.length);
           const rmsLevel = rms * 100; // 0-100 ë²”ìœ„ë¡œ ë³€í™˜
           
           // ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë°ì´í„° ë¶„ì„ (ë” ë¯¼ê°í•˜ê²Œ)
           const freqAverage = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
           const freqMax = Math.max(...dataArray);
           const freqMin = Math.min(...dataArray);
           
           // ë” ë¯¼ê°í•œ ë ˆë²¨ ê³„ì‚° (ì£¼íŒŒìˆ˜ ìµœëŒ€ê°’ê³¼ RMS ì¤‘ ë” ë†’ì€ ê°’ ì‚¬ìš©)
           const finalLevel = Math.max(freqMax * 0.5, rmsLevel * 2);
           
           // ë””ë²„ê¹…: ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
           const hasAudioData = freqMax > 0 || rms > 0.001;
           
           if (!hasAudioData && Math.random() < 0.01) { // 1% í™•ë¥ ë¡œ ë¡œê¹…
             console.warn('ğŸ¤ ê¸°ë³¸ ì²´í¬: ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤:', {
               freqMax,
               freqMin,
               freqAverage: freqAverage.toFixed(2),
               rms: rms.toFixed(4),
               rmsLevel: rmsLevel.toFixed(2),
               finalLevel: finalLevel.toFixed(2),
               streamActive: streamRef.current?.active,
               audioTrackEnabled: audioTrack?.enabled,
               audioTrackMuted: audioTrack?.muted
             });
           }
           
           setCurrentAudioLevel(finalLevel);
           
           // ë””ë²„ê¹…: ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ìƒíƒœ í™•ì¸ (ë§¤ìš° ë“œë¬¼ê²Œë§Œ)
           if (Math.random() < 0.001) { // 0.1% í™•ë¥ ë¡œë§Œ ë¡œê¹…
             console.log('ğŸ” ì˜¤ë””ì˜¤ ë¶„ì„ê¸° ìƒíƒœ:', {
               analyserFftSize: analyser.fftSize,
               analyserFrequencyBinCount: analyser.frequencyBinCount,
               analyserSmoothingTimeConstant: analyser.smoothingTimeConstant,
               analyserMinDecibels: analyser.minDecibels,
               analyserMaxDecibels: analyser.maxDecibels,
               sourceConnected: source.context === audioContext,
               dataArrayMax: Math.max(...dataArray),
               dataArrayMin: Math.min(...dataArray),
               dataArraySum: dataArray.reduce((sum, val) => sum + val, 0),
               nonZeroCount: dataArray.filter(val => val > 0).length,
               rmsLevel: rmsLevel.toFixed(2),
               timeDataMax: Math.max(...timeDataArray),
               timeDataMin: Math.min(...timeDataArray),
               finalLevel: finalLevel.toFixed(2),
               isSpeaking: stateRef.current.isSpeaking
             });
           }
           
           // ìŒì„± ê°ì§€ (ì „ì²´ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì— ë§ëŠ” ì„ê³„ê°’)
           if (finalLevel > 5 && !stateRef.current.isSpeaking) {
             setIsSpeaking(true);
             speechStartTimeRef.current = Date.now();
             silenceStartTimeRef.current = null; // ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
             console.log('ğŸ¤ ìŒì„± ê°ì§€ë¨ (ë ˆë²¨:', finalLevel.toFixed(1), ')');
           } else if (finalLevel < 2 && stateRef.current.isSpeaking) {
             // ì¡°ìš©í•¨ ì‹œì‘ ì‹œê°„ ê¸°ë¡
             if (!silenceStartTimeRef.current) {
               silenceStartTimeRef.current = Date.now();
             }
             
             // 0.3ì´ˆ ì´ìƒ ì¡°ìš©í•˜ë©´ ë°œí™” ì¢…ë£Œë¡œ íŒë‹¨ (ë” ë¹ ë¥´ê²Œ)
             const silenceDuration = Date.now() - silenceStartTimeRef.current;
             if (silenceDuration > 200) {
               setIsSpeaking(false);
               speechEndTimeRef.current = Date.now();
               console.log('ğŸ¤ ìŒì„± ì¢…ë£Œ ê°ì§€ë¨ (ë ˆë²¨:', finalLevel.toFixed(1), ', ì¡°ìš©í•¨ ì§€ì†:', silenceDuration + 'ms)');
               
               // ë°œí™” ë¶„ì„ íŠ¸ë¦¬ê±°
               if (speechBufferRef.current.trim()) {
                 processSpeechSegment();
               }
               
               // íƒ€ì´ë¨¸ ë¦¬ì…‹
               silenceStartTimeRef.current = null;
             }
           } else if (finalLevel >= 2 && stateRef.current.isSpeaking) {
             // ë‹¤ì‹œ ì†Œë¦¬ê°€ ë‚˜ë©´ ì¡°ìš©í•¨ íƒ€ì´ë¨¸ ë¦¬ì…‹
             silenceStartTimeRef.current = null;
           }
           
           requestAnimationFrame(() => checkAudioLevel());
         };

        setIsListening(true); // ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ trueë¡œ ì„¤ì •
        
        // ê°•ì œë¡œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ ì‹œì‘
        setTimeout(() => {
          checkAudioLevel();
        }, 100);
        setupSpeechRecognition();
        setIsInitialized(true);
      }
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      setError(error instanceof Error ? error.message : 'ì´ˆê¸°í™” ì‹¤íŒ¨');
    } finally {
      setIsInitializing(false);
    }
  }, [isInitialized, isInitializing]);

  // Speech Recognition ì„¤ì • - ë©”ëª¨ì´ì œì´ì…˜ ì ìš©
  const setupSpeechRecognition = useCallback(() => {
    if (!SpeechRecognition) {
      console.warn("Speech Recognition APIë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.");
      return;
    }

    // ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ì¤‘ë‹¨
    if (recognitionRef.current && stateRef.current.isListening) {
      try {
        recognitionRef.current.stop();
      } catch (error) {
        console.log('ê¸°ì¡´ ìŒì„± ì¸ì‹ ì¤‘ë‹¨ ì¤‘...');
      }
    }

    const recognition = new SpeechRecognition();
    recognitionRef.current = recognition;

    // ì„±ëŠ¥ ìµœì í™” ì„¤ì •
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ko-KR';
    recognition.maxAlternatives = 1;

    let finalTranscript = '';
    let interimTranscript = '';

    recognition.onstart = () => { setIsSpeechRecognitionActive(true); };

    recognition.onresult = (event: any) => {
      let interimTranscript = '';
      let finalTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
          // ì²« ë²ˆì§¸ finalTranscript ìˆ˜ì‹  ì‹œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
          if (speechStartTimeRef.current === null) {
            speechStartTimeRef.current = Date.now();
            console.log('ğŸ¤ ë°œí™” ì‹œì‘ ì‹œê°„ ê¸°ë¡:', new Date().toLocaleTimeString());
          }
        } else {
          interimTranscript += transcript;
        }
      }

      // ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë””ë°”ìš´ìŠ¤ ì ìš©)
      setLiveTranscript(interimTranscript);
      
      if (finalTranscript) {
        speechBufferRef.current += finalTranscript;
        console.log('ğŸ¤ ìµœì¢… í…ìŠ¤íŠ¸ ì¶”ê°€ë¨:', finalTranscript);
      }
    };

    recognition.onend = () => {
      setIsSpeechRecognitionActive(false);
      speechEndTimeRef.current = Date.now();
      console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¢…ë£Œë¨ - ë°œí™” ë¶„ì„ ì‹œì‘');
      
      // ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ (ê°•ì œ ì‹¤í–‰)
      if (speechBufferRef.current.trim()) {
        console.log('ğŸ¤ ë°œí™” ë²„í¼ ë‚´ìš©:', speechBufferRef.current);
        processSpeechSegment();
      } else {
        console.log('ğŸ¤ ë°œí™” ë²„í¼ê°€ ë¹„ì–´ìˆìŒ - ë¶„ì„ ê±´ë„ˆëœ€');
      }
      
      // ì¦‰ì‹œ ì¬ì‹œì‘ (ì—°ì† ì¸ì‹ì„ ìœ„í•´) - ìƒíƒœ ì²´í¬ ê°•í™”
      if (recognitionRef.current && stateRef.current.isListening) {
        try {
          // ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œì‘
          const currentState = recognitionRef.current.state;
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ìƒíƒœ í™•ì¸:', currentState);
          
          if (currentState === 'inactive') {
            recognitionRef.current.start();
            setIsListening(true);
            console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ë¨');
          } else {
            console.log('ğŸ¤ ìŒì„± ì¸ì‹ì´ ì´ë¯¸ í™œì„± ìƒíƒœì…ë‹ˆë‹¤ (ìƒíƒœ:', currentState, ')');
          }
        } catch (error) {
          console.warn('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹¤íŒ¨:', error);
          // ì¬ì‹œì‘ ì‹¤íŒ¨ ì‹œ 200ms í›„ ì¬ì‹œë„
          setTimeout(() => {
            if (recognitionRef.current && stateRef.current.isListening) {
              try {
                const retryState = recognitionRef.current.state;
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œë„ - ìƒíƒœ:', retryState);
                if (retryState === 'inactive') {
                  recognitionRef.current.start();
                  setIsListening(true);
                  console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¬ì‹œë„ ì„±ê³µ');
                }
              } catch (retryError) {
                console.error('ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¬ì‹œë„ ì‹¤íŒ¨:', retryError);
              }
            }
          }, 200);
        }
      }
    };

    recognition.onerror = (event: any) => {
      // abortedëŠ” ì •ìƒì ì¸ ì¢…ë£Œì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬
      if (event.error === 'aborted') {
        console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì •ìƒ ì¢…ë£Œë¨');
        return;
      }

      // no-speechëŠ” ì •ìƒì ì¸ ìƒí™©
      if (event.error === 'no-speech') {
        console.log('ğŸ¤ ìŒì„± ì—†ìŒ - ì •ìƒì ì¸ ìƒí™©');
        return;
      }

      // ë‹¤ë¥¸ ì˜¤ë¥˜ë“¤ì€ ë¡œê·¸ ì¶œë ¥
      console.error("[STT] recognition error:", event.error);
    };

    // ì•ˆì „í•˜ê²Œ ì‹œì‘
    try {
      recognition.start();
    } catch (error) {
      console.warn('ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨:', error);
    }
  }, []);

  // ë°œí™” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ - ì„±ëŠ¥ ìµœì í™” ì ìš©
  const processSpeechSegment = useCallback(async () => {
    if (isAnalyzing) return;
    setIsAnalyzing(true);
    const text = speechBufferRef.current.trim();
    if (!text) { setIsAnalyzing(false); return; }

    const startTime = performance.now();
    
    try {
      // ë°œí™” ì§€ì†ì‹œê°„ ê³„ì‚°
      const duration = speechStartTimeRef.current && speechEndTimeRef.current 
        ? (speechEndTimeRef.current - speechStartTimeRef.current) / 1000 
        : 0;

      // íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´
      const startTimestamp = speechStartTimeRef.current ? new Date(speechStartTimeRef.current).toLocaleTimeString() : 'ì•Œ ìˆ˜ ì—†ìŒ';
      const endTimestamp = speechEndTimeRef.current ? new Date(speechEndTimeRef.current).toLocaleTimeString() : 'ì•Œ ìˆ˜ ì—†ìŒ';

      // KoELECTRA ëª¨ë¸ ì¶”ë¡  (ìš°ì„ ìˆœìœ„)
      let isStudyRelated = false;
      let koelectraConfidence = 0;
      let analysisMethod = 'í‚¤ì›Œë“œ';

      if (isModelLoaded) {
        try {
          const result = await koelectraInference(text);
          if (result && result.confidence >= 0.6) {
            isStudyRelated = result.logits[1] > result.logits[0]; // ê³µë¶€ ê´€ë ¨ í´ë˜ìŠ¤ê°€ ë” ë†’ì€ ê²½ìš°
            koelectraConfidence = result.confidence;
            analysisMethod = 'KoELECTRA';
          } else {
            // ì‹ ë¢°ë„ê°€ ë‚®ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
            isStudyRelated = analyzeStudyRelatedByKeywords(text);
          }
        } catch (error) {
          console.warn('KoELECTRA ì¶”ë¡  ì‹¤íŒ¨, í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´:', error);
          isStudyRelated = analyzeStudyRelatedByKeywords(text);
        }
      } else {
        // ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜
        isStudyRelated = analyzeStudyRelatedByKeywords(text);
      }

      // ë¬¸ë§¥ ë¶„ì„
      const context = analyzeTextContext(text);
      const contextualWeight = getContextualWeight(context);
      const contextLabel = getContextLabel(context);

      // ìµœì¢… íŒì • (ë¬¸ë§¥ ê°€ì¤‘ì¹˜ ì ìš©)
      const finalJudgment = isStudyRelated && contextualWeight > 0.3;

      const processingTime = performance.now() - startTime;

      // êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
      console.log(`
ğŸ¤ ë°œí™” ë¶„ì„ ê²°ê³¼ (${new Date().toLocaleTimeString()}):
â”œâ”€ ì‹œì‘ ì‹œê°„: ${startTimestamp}
â”œâ”€ ì¢…ë£Œ ì‹œê°„: ${endTimestamp}
â”œâ”€ ì§€ì†ì‹œê°„: ${duration.toFixed(1)}ì´ˆ
â”œâ”€ ì›ë¬¸: "${text}"
â”œâ”€ ë¶„ì„ ë°©ë²•: ${analysisMethod}
â”œâ”€ KoELECTRA ì‹ ë¢°ë„: ${koelectraConfidence.toFixed(3)}
â”œâ”€ ê³µë¶€ ê´€ë ¨: ${isStudyRelated ? 'âœ…' : 'âŒ'}
â”œâ”€ ë¬¸ë§¥: ${contextLabel} (ê°€ì¤‘ì¹˜: ${contextualWeight.toFixed(2)})
â”œâ”€ ìµœì¢… íŒì •: ${finalJudgment ? 'ê³µë¶€ ê´€ë ¨ ë°œí™”' : 'ì¡ë‹´'}
â””â”€ ì²˜ë¦¬ ì‹œê°„: ${processingTime.toFixed(1)}ms
      `);

      // ë²„í¼ ì´ˆê¸°í™”
      speechBufferRef.current = "";
      speechStartTimeRef.current = null;
      speechEndTimeRef.current = null;

    } catch (error) {
      console.error('ë°œí™” ë¶„ì„ ì‹¤íŒ¨:', error);
      speechBufferRef.current = "";
    } finally {
      setIsAnalyzing(false);
      // ë¶„ì„ ì™„ë£Œ í›„ ìŒì„±ì¸ì‹ ì¬ì‹œì‘ ë³´ì¥
      setTimeout(() => {
        if (recognitionRef.current && isInitialized && isFocusSessionRunning && !isFocusSessionPaused) {
          try {
            console.log('ğŸ¤ ë°œí™” ë¶„ì„ ì™„ë£Œ - ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì‹œë„ (ìƒíƒœ:', recognitionRef.current.state, ')');
            if (recognitionRef.current.state === 'inactive') {
              recognitionRef.current.start();
              console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì„±ê³µ');
            } else {
              console.log('ğŸ¤ ìŒì„± ì¸ì‹ì´ ì´ë¯¸ í™œì„± ìƒíƒœì…ë‹ˆë‹¤ (ìƒíƒœ:', recognitionRef.current.state, ')');
            }
          } catch (error) {
            console.warn('ë¶„ì„ í›„ ì¬ì‹œì‘ ì‹¤íŒ¨:', error);
          }
        } else {
          console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì¡°ê±´ ë¶ˆë§Œì¡±:', {
            hasRecognition: !!recognitionRef.current,
            isInitialized,
            isFocusSessionRunning,
            isFocusSessionPaused
          });
          
          // isInitializedê°€ falseì—¬ë„ ì¬ì‹œì‘ ì‹œë„
          if (recognitionRef.current && isFocusSessionRunning && !isFocusSessionPaused) {
            try {
              console.log('ğŸ¤ isInitializedê°€ falseì´ì§€ë§Œ ì¬ì‹œì‘ ì‹œë„');
              // ìƒíƒœê°€ undefinedì¸ ê²½ìš°ì—ë„ ì¬ì‹œì‘ ì‹œë„
              if (!recognitionRef.current.state || recognitionRef.current.state === 'inactive') {
                recognitionRef.current.start();
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ì¬ì‹œì‘ ì„±ê³µ (isInitialized ë¬´ì‹œ)');
              } else {
                console.log('ğŸ¤ ìŒì„± ì¸ì‹ ìƒíƒœ í™•ì¸:', recognitionRef.current.state);
              }
            } catch (error) {
              console.warn('ì¬ì‹œì‘ ì‹¤íŒ¨ (isInitialized ë¬´ì‹œ):', error);
            }
          }
        }
      }, 100); // 100ms ì§€ì—° í›„ ì¬ì‹œì‘
    }
  }, [isModelLoaded, koelectraInference, isInitialized, isAnalyzing]);

  // í…ìŠ¤íŠ¸ ë¬¸ë§¥ì„ ë¶„ì„í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const analyzeTextContext = (text: string):
    | 'discussion'
    | 'class'
    | 'presentation'
    | 'question'
    | 'frustration'
    | 'statement'
    | 'unknown' => {
    const lower = text.toLowerCase();

    // í† ë¡  ìƒí™©
    if (
      lower.includes("í† ë¡ ") ||
      lower.includes("ì°¬ì„±") ||
      lower.includes("ë°˜ëŒ€") ||
      lower.includes("ì˜ê²¬") ||
      lower.includes("ìƒê°ì€")
    ) {
      return 'discussion';
    }
    // ìˆ˜ì—… ìƒí™©
    if (
      lower.includes("ì„ ìƒë‹˜") ||
      lower.includes("êµê³¼ì„œ") ||
      lower.includes("ë¬¸ì œ") ||
      lower.includes("ì„¤ëª…") ||
      lower.includes("ìˆ˜ì—…")
    ) {
      return 'class';
    }
    // ë°œí‘œ ìƒí™©
    if (
      lower.includes("ë°œí‘œ") ||
      lower.includes("ê²°ë¡ ") ||
      lower.includes("ìš”ì•½") ||
      lower.includes("ì •ë¦¬")
    ) {
      return 'presentation';
    }
    // ì§ˆë¬¸
    if (
      lower.includes("ì–´ë–»ê²Œ") ||
      lower.includes("ì™œ") ||
      lower.includes("ë­ì•¼") ||
      lower.includes("ë¬´ì—‡") ||
      lower.includes("ê¶ê¸ˆ") ||
      text.endsWith("?")
    ) {
      return 'question';
    }
    // ì¢Œì ˆ
    if (
      lower.includes("ì•ˆë¼") ||
      lower.includes("ì§œì¦ë‚˜") ||
      lower.includes("ì•„ì˜¤")
    ) {
      return 'frustration';
    }
    // ì§„ìˆ 
    if (text.length > 5) {
      return 'statement';
    }
    return 'unknown';
  };

  // ë¬¸ë§¥ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextualWeight = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): number => {
    switch (context) {
      case 'discussion': return 0.95; // í† ë¡ : ì§‘ì¤‘ë„ ì•½ê°„ ë†’ìŒ
      case 'class': return 1.0; // ìˆ˜ì—…: ì§‘ì¤‘ë„ ìµœìƒ
      case 'presentation': return 0.9; // ë°œí‘œ: ì§‘ì¤‘ë„ ë†’ìŒ
      case 'question': return 0.8; // ì§ˆë¬¸: ë§‰í˜”ì„ ê°€ëŠ¥ì„±, ì§‘ì¤‘ë„ ì•½ê°„ ê°ì†Œ
      case 'frustration': return 0.5; // ì¢Œì ˆ: ëª…í™•í•œ ì§‘ì¤‘ë ¥ ì €í•˜ ì‹ í˜¸, ì§‘ì¤‘ë„ í¬ê²Œ ê°ì†Œ
      case 'statement': return 1.0; // í˜¼ì£ë§/ë‚´ìš© ì½ê¸°: í•™ìŠµ í™œë™ì˜ ì¼ë¶€ë¡œ íŒë‹¨, ê°€ì¤‘ì¹˜ ì—†ìŒ
      default: return 0.9; // ë¶ˆëª…í™•: ì§§ì€ ì¤‘ì–¼ê±°ë¦¼ ë“±, ì•½ê°„ì˜ ë°©í•´ë¡œ ê°„ì£¼
    }
  };

  // ë¬¸ë§¥ ìœ í˜•ì„ í•œê¸€ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
  const getContextLabel = (
    context:
      | 'discussion'
      | 'class'
      | 'presentation'
      | 'question'
      | 'frustration'
      | 'statement'
      | 'unknown'
  ): string => {
    switch (context) {
      case 'discussion': return 'í† ë¡ ';
      case 'class': return 'ìˆ˜ì—…';
      case 'presentation': return 'ë°œí‘œ';
      case 'question': return 'ì§ˆë¬¸';
      case 'frustration': return 'ì¢Œì ˆ';
      case 'statement': return 'ì§„ìˆ ';
      default: return 'ë¶ˆëª…í™•';
    }
  };

  useEffect(() => {
    // Speech Recognition ì„¤ì •ë§Œ ë¨¼ì € ìˆ˜í–‰
    setupSpeechRecognition();

    return () => {
      if (recognitionRef.current) recognitionRef.current.abort()
      if (speechTimeoutRef.current) clearTimeout(speechTimeoutRef.current)
      // AudioContextëŠ” ë‹«ì§€ ì•ŠìŒ (ì˜¤ë””ì˜¤ ë ˆë²¨ ì²´í¬ë¥¼ ìœ„í•´ ìœ ì§€)
      if (workerRef.current) workerRef.current.terminate()
    }
  }, [])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ í† í¬ë‚˜ì´ì € ë° ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
  useEffect(() => {
    const initializeComponents = async () => {
      // 1. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” (ìš°ì„ ìˆœìœ„)
      try {
        await initializeTokenizer();
      } catch (error) {
        console.error('âŒ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      }
      
      // 2. ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (í† í¬ë‚˜ì´ì € ë¡œë“œ í›„)
      setTimeout(() => {
        if (!isInitialized && !isInitializing) {
          initializeAudioPipeline()
        }
      }, 500); // í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ í›„ 500ms ëŒ€ê¸°
    };
    
    initializeComponents();
  }, [isInitialized, isInitializing, initializeAudioPipeline])

  // í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
  const handleTokenizerTest = async () => {
    const testTexts = [
      "ì•ˆë…•í•˜ì„¸ìš”",
      "ê³µë¶€ë¥¼ í•˜ê³  ìˆì–´ìš”",
      "ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ê³  ìˆìŠµë‹ˆë‹¤",
      "ì´ë¡ ì„ ê³µë¶€í•˜ê³  ìˆì–´ìš”",
      "í† ë¡ ì„ í•˜ê³  ìˆì–´ìš”",
      "ì„ ìƒë‹˜ì´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
      "ì–´ë–»ê²Œ í’€ì–´ì•¼ í• ê¹Œìš”?",
      "ì§œì¦ë‚˜ìš” ì´ ë¬¸ì œê°€ ì•ˆ í’€ë ¤ìš”"
    ];
    
    await testTokenizer(testTexts);
  };

  return (
    <div className="p-4 bg-slate-100 rounded-lg">
      <h3 className="font-bold">í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸</h3>
      
      {/* í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ë²„íŠ¼ */}
      <div className="mb-4">
        <button 
          onClick={handleTokenizerTest}
          className="px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600 mr-2"
        >
          í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
        </button>
        <span className="text-sm text-gray-600">ìƒˆë¡œìš´ WordPiece í† í¬ë‚˜ì´ì €ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤</span>
      </div>
      
      {isInitializing && (
        <p className="text-blue-600 mb-4">ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...</p>
      )}
      
      {error && (
        <p className="text-red-600 mb-4">ì˜¤ë¥˜: {error}</p>
      )}
      
      {/* KoELECTRA ëª¨ë¸ ìƒíƒœ */}
      <div className="mb-4 p-3 bg-gray-50 rounded">
        <h4 className="font-semibold mb-2">ğŸ¤– KoELECTRA ëª¨ë¸ ìƒíƒœ</h4>
        <div className="space-y-1 text-sm">
          <p><b>ëª¨ë¸ ë¡œë“œ:</b> 
            {isModelLoading ? "ğŸ”„ ë¡œë”© ì¤‘..." : 
             isModelLoaded ? "âœ… ë¡œë“œë¨" : "âŒ ë¯¸ë¡œë“œ"}
          </p>
          {modelError && (
            <p className="text-red-600"><b>ëª¨ë¸ ì—ëŸ¬:</b> {modelError}</p>
          )}
        </div>
      </div>


      
      {isInitialized && (
        <div className="space-y-3 p-3 bg-white rounded border">
          <h4 className="font-semibold text-sm">ğŸ¤ ì˜¤ë””ì˜¤ íŒŒì´í”„ë¼ì¸ ìƒíƒœ</h4>
          
          {/* ìŒì„± ê°ì§€ ìƒíƒœ */}
          <div className="flex items-center gap-2">
            <div className={`w-3 h-3 rounded-full ${isSpeaking ? 'bg-red-500 animate-pulse' : 'bg-gray-300'}`}></div>
            <span className="text-sm">
              {isSpeaking ? "ğŸ¤ ë§í•˜ëŠ” ì¤‘..." : "ğŸ”‡ ì¡°ìš©í•¨"}
            </span>
          </div>
          
          {/* ì˜¤ë””ì˜¤ ë ˆë²¨ í‘œì‹œ */}
          <div className="space-y-1">
            <div className="flex justify-between text-xs">
              <span>ì˜¤ë””ì˜¤ ë ˆë²¨</span>
              <span>{currentAudioLevel.toFixed(1)}</span>
            </div>
            <div className="w-full bg-gray-200 rounded-full h-2">
              <div 
                className={`h-2 rounded-full transition-all duration-200 ${
                  currentAudioLevel > 25 ? 'bg-red-500' : 
                  currentAudioLevel > 15 ? 'bg-yellow-500' : 'bg-gray-400'
                }`}
                style={{ width: `${Math.min(currentAudioLevel * 2, 100)}%` }}
              ></div>
            </div>
          </div>
          
          {/* ì‹œìŠ¤í…œ ìƒíƒœ */}
          <div className="grid grid-cols-2 gap-2 text-xs">
            <div>
              <span className="text-gray-600">ìŒì„± ì¸ì‹:</span>
              <span className={`ml-1 ${isSpeechRecognitionActive ? 'text-green-600' : 'text-gray-400'}`}>
                {isSpeechRecognitionActive ? 'í™œì„±' : 'ëŒ€ê¸°'}
              </span>
            </div>
            <div>
              <span className="text-gray-600">ì˜¤ë””ì˜¤ ëª¨ë‹ˆí„°ë§:</span>
              <span className={`ml-1 ${isListening ? 'text-green-600' : 'text-gray-400'}`}>
                {isListening ? 'í™œì„±' : 'ë¹„í™œì„±'}
              </span>
            </div>
          </div>
          
          {/* ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ */}
          {liveTranscript && (
            <div className="p-2 bg-blue-50 rounded text-xs">
              <span className="text-gray-600">ì¸ì‹ëœ í…ìŠ¤íŠ¸:</span>
              <p className="mt-1 text-gray-800">{liveTranscript}</p>
            </div>
          )}
        </div>
      )}
    </div>
  )
}
